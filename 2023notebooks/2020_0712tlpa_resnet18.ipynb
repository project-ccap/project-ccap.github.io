{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/2023notebooks/2020_0712tlpa_resnet18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dIarQS5uDMH"
      },
      "source": [
        "# CCAP Project\n",
        "\n",
        "- title: 転移学習による TLPA 画像認識\n",
        "- author: 浅川伸一\n",
        "- filename: `2020-0712tlpa_resnet18.ipynb`\n",
        "- last date: 2020-0713\n",
        "- note:\n",
        "    - 使用モデル: ResNet-18, 論文: https://arxiv.org/abs/1512.03385\n",
        "    - データ: TLPA 図版 大門正太郎 先生より"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "7cihZdGMuDMK"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "%matplotlib inline\n",
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "from PIL import Image as PILImage\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "import skimage.color  # for gray2rgb()\n",
        "from scipy.special import logsumexp, softmax\n",
        "from termcolor import colored\n",
        "\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# PyTorch バージョン確認\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "print(f'device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "JMEWYuNkuDMK"
      },
      "outputs": [],
      "source": [
        "# 各モデルを定義し，訓練済み結合係数をダウンロードする\n",
        "DNNs = {}\n",
        "DNNs['resnet18'] = models.resnet18(weights='DEFAULT', progress=True)\n",
        "#DNNs['resnet18'] = models.resnet18(pretrained=True, progress=True)\n",
        "# DNNs['alexnet'] = models.alexnet(pretrained=True, progress=True)\n",
        "# DNNs['vgg16'] = models.vgg16(pretrained=True, progress=True)\n",
        "# DNNs['squeezenet']= models.squeezenet1_0(pretrained=True, progress=True)\n",
        "# DNNs['densenet'] = models.densenet161(pretrained=True, progress=True)\n",
        "# DNNs['inception'] = models.inception_v3(pretrained=True, progress=True)\n",
        "# DNNs['googlenet'] = models.googlenet(pretrained=True, progress=True)\n",
        "# DNNs['shufflenet'] = models.shufflenet_v2_x1_0(pretrained=True, progress=True)\n",
        "# DNNs['mobilenet'] = models.mobilenet_v2(pretrained=True, progress=True)\n",
        "# DNNs['resnext50_32x4d'] = models.resnext50_32x4d(pretrained=True, progress=True)\n",
        "# DNNs['wide_resnet50_2'] = models.wide_resnet50_2(pretrained=True, progress=True)\n",
        "# DNNs['mnasnet'] = models.mnasnet1_0(pretrained=True, progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "zrKw7iHpuDML"
      },
      "outputs": [],
      "source": [
        "# 上の中から試したいモデルを選んでください。最後のモデルが有効になります。\n",
        "net = DNNs['resnet18']\n",
        "#net = DNNs['squeezenet']\n",
        "#net = DNNs['googlenet']\n",
        "#net = DNNs['shufflenet']\n",
        "#net = DNNs['mobilenet']\n",
        "#net = DNNs['vgg16']\n",
        "#net = DNNs['alexnet']\n",
        "\n",
        "a_parameters = {name:param for name, param in net.named_parameters()}\n",
        "a_modules = {name:param for name, param in net.named_modules()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ZIPm3c_JuDML"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "# RGB 各チャンネルの平均と分散の定義。CNN 唯一の前処理\n",
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]\n",
        "\n",
        "normalize = transforms.Normalize(mean=mean, std=std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "W6dGYwckuDML"
      },
      "outputs": [],
      "source": [
        "# サンプル画像の入手\n",
        "import os\n",
        "if not os.path.exists('test.img'):\n",
        "    !wget -O test.img https://github.com/ShinAsakawa/ShinAsakawa.github.io/raw/master/assets/CIMG0568.JPG\n",
        "\n",
        "image_file = 'test.img'\n",
        "img = PILImage.open(image_file)\n",
        "plt.axis(False); plt.imshow(img); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ADmoRTMhuDML"
      },
      "outputs": [],
      "source": [
        "# 認識するための準備で画像を変形\n",
        "img_ = transform(img)\n",
        "plt.axis(False); plt.imshow(img_.numpy().transpose((1, 2, 0))); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "E-50tH0guDMM"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import ccap\n",
        "except ImportError:\n",
        "    !git clone https://github.com/project-ccap/ccap.git\n",
        "    import ccap\n",
        "from ccap import imagenetDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9ofczeAmuDMM"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "imagenet = imagenetDataset()\n",
        "\n",
        "for _ in range(3):\n",
        "    num = np.random.choice(len(imagenet))\n",
        "    imagenet.sample_and_show(num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "eEhJh4HSuDMM"
      },
      "outputs": [],
      "source": [
        "# 出力結果からラベルを予測する後処理クラス\n",
        "class ImageNetPredictor():\n",
        "    \"\"\"\n",
        "    ImageNet データに対するモデルの出力からラベル出力\n",
        "\n",
        "    Attributes:\n",
        "        class_index : dictionary\n",
        "        クラス index とラベル名 を対応させた辞書型変数\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, class_index):\n",
        "        self.class_index = class_index\n",
        "\n",
        "    def predict_max(self, out):\n",
        "        \"\"\"\n",
        "        最大値を与える ImageNet ラベル名を返す\n",
        "\n",
        "        Parameters:\n",
        "            out : torch.Size([1, 1000])  Net からの出力\n",
        "\n",
        "        Returns:\n",
        "            predicted_labels: [str]\n",
        "            最も予測確率が高いラベルの名前\n",
        "        \"\"\"\n",
        "        outnp = out.detach().numpy()\n",
        "        ids = np.argsort(- outnp)\n",
        "        predicted_labels = [self.class_index[id] for id in ids[0]]\n",
        "\n",
        "        return ids[0], predicted_labels, softmax(outnp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "nhLtBROguDMM"
      },
      "outputs": [],
      "source": [
        "# 下の no は 0 から 999 まで 1000 の ImageNet class に対応\n",
        "no = np.random.choice(len(imagenet))\n",
        "#print(f'no:{no}, ラベル:{imagenet.data[no][\"label\"]}')\n",
        "img_file = imagenet.sample_image(no)\n",
        "img = PILImage.open(img_file)\n",
        "plt.title(f'{no}:{imagenet.data[no][\"label\"]}')\n",
        "plt.axis(False); plt.imshow(img); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "W2QQDkLfuDMM"
      },
      "outputs": [],
      "source": [
        "# 認識の実施\n",
        "inputs = transform(img).unsqueeze_(0)  # torch.Size([1, 3, 224, 224])\n",
        "out = net(inputs)  # torch.Size([1, 1000])\n",
        "\n",
        "outnp = out.detach().numpy()\n",
        "ids = np.argsort( - outnp)\n",
        "\n",
        "n_best = 3\n",
        "print(ids[0][:n_best])\n",
        "for no in ids[0][:n_best]:\n",
        "    print(imagenet(int(no))[1], end=\" \")\n",
        "    print(imagenet.getitem_from_wnid(imagenet(int(no))[1])['label'], end=\" \")\n",
        "    print(imagenet.getitem_from_wnid(imagenet(int(no))[1])['label_ja'], end=\" \")\n",
        "    print(imagenet.getitem_from_wnid(imagenet(int(no))[1])['definition'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "lC4YgAUQuDMM"
      },
      "outputs": [],
      "source": [
        "# 乱数のシードを設定\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4osltHLHuDMM"
      },
      "outputs": [],
      "source": [
        "# 入力画像の前処理をするクラス\n",
        "# 訓練時と推論時で処理が異なる\n",
        "class ImageTransform():\n",
        "    \"\"\"\n",
        "    画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "    画像のサイズをリサイズし、色を標準化する。\n",
        "    訓練時は RandomResizedCrop と RandomHorizontalFlip で データ拡張\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    resize : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    mean : (R, G, B)\n",
        "        各色チャネルの平均値。\n",
        "    std : (R, G, B)\n",
        "        各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, resize, mean, std):\n",
        "        self.data_transform = {\n",
        "            'train': transforms.Compose([\n",
        "                transforms.RandomResizedCrop(\n",
        "                    resize, scale=(0.8, 1.0)),  # データ拡張\n",
        "                transforms.RandomHorizontalFlip(),  # データ拡張\n",
        "                transforms.RandomAffine(degrees=(-20,20), translate=None, scale=[0.9,1.1]),\n",
        "                transforms.ToTensor(),  # テンソルに変換\n",
        "                transforms.Normalize(mean, std)  # 標準化\n",
        "            ]),\n",
        "            'val': transforms.Compose([\n",
        "                transforms.Resize(resize),  # リサイズ\n",
        "                transforms.CenterCrop(resize),  # 画像中央をresize×resizeで切り取り\n",
        "                transforms.ToTensor(),  # テンソルに変換\n",
        "                transforms.Normalize(mean, std)  # 標準化\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, img, phase='train'):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        phase : 'train' or 'val'\n",
        "            前処理のモードを指定。\n",
        "        \"\"\"\n",
        "        return self.data_transform[phase](img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "p7Bk517JuDMM"
      },
      "outputs": [],
      "source": [
        "# 訓練時の画像前処理の動作を確認\n",
        "\n",
        "# 画像読み込み\n",
        "no = 300\n",
        "img_file, label = imagenet(no)\n",
        "img = PILImage.open(img_file)   # [高さ][幅][色RGB]\n",
        "\n",
        "# 元画像の表示\n",
        "plt.axis(False); plt.imshow(img); plt.show()\n",
        "\n",
        "# 画像の前処理と処理済み画像の表示\n",
        "size = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "transform = ImageTransform(size, mean, std)\n",
        "img_transformed = transform(img, phase=\"train\")  # torch.Size([3, 224, 224])\n",
        "\n",
        "# (色、高さ、幅)を (高さ、幅、色)に変換し、0-1に値を制限して表示\n",
        "img_transformed = img_transformed.numpy().transpose((1, 2, 0))\n",
        "img_transformed = np.clip(img_transformed, 0, 1)\n",
        "plt.axis(False); plt.imshow(img_transformed); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4y-MbV10uDMN"
      },
      "outputs": [],
      "source": [
        "from ccap import tlpaDataset\n",
        "\n",
        "tlpa = tlpaDataset()\n",
        "tlpa_img_path = [tlpa.data[k]['img'] for k in tlpa.data.keys()]\n",
        "tlpa.data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W205pGLGuDMN",
        "outputId": "597dd141-9082-414a-c748-c1525721f992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 63: 63, 64: 64, 65: 65, 66: 66, 67: 67, 68: 68, 69: 69, 70: 70, 71: 71, 72: 72, 73: 73, 74: 74, 75: 75, 76: 76, 77: 77, 78: 78, 79: 79, 80: 80, 81: 81, 82: 82, 83: 83, 84: 84, 85: 85, 86: 86, 87: 87, 88: 88, 89: 89, 90: 90, 91: 91, 92: 92, 93: 93, 94: 94, 95: 95, 96: 96, 97: 97, 98: 98, 99: 99, 100: 100, 101: 101, 102: 102, 103: 103, 104: 104, 105: 105, 106: 106, 107: 107, 108: 108, 109: 109, 110: 110, 111: 111, 112: 112, 113: 113, 114: 114, 115: 115, 116: 116, 117: 117, 118: 118, 119: 119, 120: 120, 121: 121, 122: 122, 123: 123, 124: 124, 125: 125, 126: 126, 127: 127, 128: 128, 129: 129, 130: 130, 131: 131, 132: 132, 133: 133, 134: 134, 135: 135, 136: 136, 137: 137, 138: 138, 139: 139, 140: 140, 141: 141, 142: 142, 143: 143, 144: 144, 145: 145, 146: 146, 147: 147, 148: 148, 149: 149, 150: 150, 151: 151, 152: 152, 153: 153, 154: 154, 155: 155, 156: 156, 157: 157, 158: 158, 159: 159, 160: 160, 161: 161, 162: 162, 163: 163, 164: 164, 165: 165, 166: 166, 167: 167, 168: 168, 169: 169, 170: 170, 171: 171, 172: 172, 173: 173, 174: 174, 175: 175, 176: 176, 177: 177, 178: 178, 179: 179}\n"
          ]
        }
      ],
      "source": [
        "# tlpa_name_dict = {i:tlpa.data[k]['Name'] for i, k in enumerate(tlpa.data)}  # N.G. str 型のラベル名では動作しない\n",
        "tlpa_name_dict = {i:k for i, k in enumerate(tlpa.data.keys())}\n",
        "print(tlpa_name_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "83k6vtBauDMN"
      },
      "outputs": [],
      "source": [
        "# Dataset の作成\n",
        "class tlpa_torch_Dataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    TLPA 画像のDatasetクラス。PyTorchのDatasetクラスを継承。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    file_list : リスト\n",
        "        画像のパスを格納したリスト\n",
        "    transform : object\n",
        "        前処理クラスのインスタンス\n",
        "    phase : 'train' or 'test'\n",
        "        学習か訓練かを設定する。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_list, name_dict, transform=None, phase='train'):\n",
        "        self.file_list = file_list  # ファイルパスのリスト\n",
        "        self.transform = transform  # 前処理クラスのインスタンス\n",
        "        self.phase = phase  # train or valの指定\n",
        "        self.namedict = name_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        '''画像の枚数を返す'''\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        前処理をした画像のTensor形式のデータとラベルを取得\n",
        "        '''\n",
        "\n",
        "        # index番目の画像をロード\n",
        "        img_path = self.file_list[index]\n",
        "        img = PILImage.open(img_path)  # [高さ][幅][色RGB]\n",
        "\n",
        "        # 画像の前処理を実施\n",
        "        img_transformed = self.transform(\n",
        "            img, self.phase)  # torch.Size([3, 224, 224])\n",
        "\n",
        "        # 画像のラベルをファイル名から抜き出す\n",
        "        label = self.namedict[index]\n",
        "        return img_transformed, label\n",
        "\n",
        "\n",
        "train_dataset = tlpa_torch_Dataset(file_list=tlpa_img_path,\n",
        "                                   name_dict=tlpa_name_dict,\n",
        "                                   transform=ImageTransform(size, mean, std),\n",
        "                                   phase='train')\n",
        "\n",
        "val_dataset = tlpa_torch_Dataset(file_list=tlpa_img_path,\n",
        "                                 name_dict=tlpa_name_dict,\n",
        "                                 transform=ImageTransform(size, mean, std),\n",
        "                                 phase='val')\n",
        "\n",
        "# 動作確認\n",
        "index = 3\n",
        "print(train_dataset.__getitem__(index)[0].size())\n",
        "print(train_dataset.__getitem__(index)[1])\n",
        "print(train_dataset.__len__())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "JbkGqe3IuDMN"
      },
      "outputs": [],
      "source": [
        "# ミニバッチのサイズを指定\n",
        "batch_size = 32\n",
        "\n",
        "# DataLoaderを作成\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 辞書型変数にまとめる\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
        "\n",
        "# 動作確認\n",
        "batch_iterator = iter(dataloaders_dict[\"train\"])  # イテレータに変換\n",
        "inputs, labels = next(\n",
        "    batch_iterator)  # 1番目の要素を取り出す\n",
        "print(inputs.size())\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "INmMIczPuDMN"
      },
      "outputs": [],
      "source": [
        "# モデルのインスタンスを生成し，事前学習済の結合係数をロード\n",
        "use_pretrained = True  # 学習済みのパラメータを使用\n",
        "net = models.resnet18(weights='DEFAULT')\n",
        "#net = models.resnet18(pretrained=use_pretrained)\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Wnpb_1qyuDMN"
      },
      "outputs": [],
      "source": [
        "# モデルの最終直下層の出力ユニット数を TLPA に合わせて 180 にする\n",
        "net.fc = nn.Linear(in_features=512, out_features=180)\n",
        "\n",
        "# 訓練モードに設定\n",
        "net.train();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4HumsMzluDMN"
      },
      "outputs": [],
      "source": [
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "h-NHo4bjuDMN"
      },
      "outputs": [],
      "source": [
        "# 転移学習で学習させるパラメータを、変数params_to_updateに格納する\n",
        "params_to_update = []\n",
        "\n",
        "# 学習させるパラメータ名\n",
        "update_param_names = [\"fc.weight\", \"fc.bias\"]\n",
        "\n",
        "# 学習させるパラメータ以外は勾配計算をなくし、変化しないように設定\n",
        "for name, param in net.named_parameters():\n",
        "    if name in update_param_names:\n",
        "        param.requires_grad = True\n",
        "        params_to_update.append(param)\n",
        "        print(name)\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# params_to_updateの中身を確認\n",
        "print(\"-----------\")\n",
        "print(params_to_update)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6LhRXVyzuDMN"
      },
      "outputs": [],
      "source": [
        "# 最適化手法の設定\n",
        "#optimizer = optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)\n",
        "#help(optim.Adam)\n",
        "#optimizer = optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(params=params_to_update)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "CmZbh8Y3uDMN"
      },
      "outputs": [],
      "source": [
        "# モデルを学習させる関数\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'エポック {epoch+1}/{num_epochs}')\n",
        "        print('-------------')\n",
        "\n",
        "        # epochごとの学習と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モード\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モード\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "\n",
        "            # 未学習時の検証性能を確かめるため、epoch=0の訓練は省略\n",
        "            if (epoch == 0) and (phase == 'train'):\n",
        "                continue\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            #for inputs, labels in tqdm(dataloaders_dict[phase]):\n",
        "            # tqdm は要らん。冗長な出力になるだけ\n",
        "            for inputs, labels in dataloaders_dict[phase]:\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = net(inputs)\n",
        "                    loss = criterion(outputs, labels)  # 損失を計算\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    # イタレーション結果の計算\n",
        "                    # lossの合計を更新\n",
        "                    epoch_loss += loss.item() * inputs.size(0)\n",
        "                    # 正解数の合計を更新\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率を表示\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "sl-cIK9UuDMN"
      },
      "outputs": [],
      "source": [
        "for inputs, labels in dataloaders_dict['train']:\n",
        "    print(inputs.size(), labels)\n",
        "    output = net(inputs)\n",
        "    loss = criterion(output, labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "metadata": {
        "id": "Jtxw-UXyxNkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "B9-Vhb3guDMN"
      },
      "outputs": [],
      "source": [
        "# 保存してある訓練済モデルを読み込む\n",
        "saved_weight_file = '2020-0712tlpa_resnet18_weights.pth'\n",
        "load_weights = torch.load(saved_weight_file)\n",
        "net.load_state_dict(load_weights)\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcp06DlOuDMN",
        "outputId": "8557f0f8-bdbc-4abd-d862-a71e1d1c2972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 21 ms, sys: 21.7 ms, total: 42.7 ms\n",
            "Wall time: 96.4 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "%%time\n",
        "# 学習・検証を実行する\n",
        "#num_epochs=1\n",
        "#train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)\n",
        "\n",
        "saved_weight_file = '2023-0624lpa_resnet18_weights.pth'\n",
        "saved_weight_file = '2020-0712tlpa_resnet18_weights.pth'\n",
        "#torch.save(net.state_dict(), saved_weight_file)\n",
        "load_weights = torch.load(saved_weight_file)\n",
        "net.load_state_dict(load_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsR4Gql7uDMN",
        "outputId": "3b8f7424-6445-4605-b60f-d1e33981bbdc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "#save_weight_file = '2020-0712tlpa_vgg16_weights.pth'\n",
        "#torch.save(net.state_dict(), save_weight_file)\n",
        "\n",
        "load_weights = torch.load(saved_weight_file)\n",
        "net.load_state_dict(load_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "E6JeBl9yuDMO"
      },
      "outputs": [],
      "source": [
        "# 1. 画像読み込み\n",
        "no = 18\n",
        "img, label = tlpa(no)\n",
        "img = PILImage.open(img)   # [高さ][幅][色RGB]\n",
        "\n",
        "# 2. 元の画像の表示\n",
        "plt.imshow(img); plt.show()\n",
        "\n",
        "# 3. 画像の前処理と処理済み画像の表示\n",
        "size = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "transform = ImageTransform(size, mean, std)\n",
        "img_transformed = transform(img, phase=\"val\")  # torch.Size([3, 224, 224])\n",
        "\n",
        "# (色、高さ、幅)を (高さ、幅、色)に変換し、0-1に値を制限して表示\n",
        "img_transformed_ = img_transformed.numpy().transpose((1, 2, 0))\n",
        "img_transformed_ = np.clip(img_transformed_, 0, 1)\n",
        "plt.imshow(img_transformed_);plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "KhHIOFaPuDMO"
      },
      "outputs": [],
      "source": [
        "#float_formatter = \"{:.3f}\".format\n",
        "#np.set_printoptions(formatter={'float_kind':float_formatter})\n",
        "# see https://note.nkmk.me/python-numpy-set-printoptions-float-formatter/\n",
        "np.set_printoptions(formatter={'int': '{:3d}'.format, 'float_kind':'{:.3f}'.format})\n",
        "\n",
        "def diagnose(no, display=False, n_best=5):\n",
        "    img, label = tlpa(no)\n",
        "    img = PILImage.open(img)   # [高さ][幅][色RGB]\n",
        "\n",
        "    # 元の画像の表示\n",
        "    #if display:\n",
        "    #    plt.imshow(img); plt.show()\n",
        "\n",
        "    # 画像の前処理と処理済み画像の表示\n",
        "    size = 224\n",
        "    mean = (0.485, 0.456, 0.406)\n",
        "    std = (0.229, 0.224, 0.225)\n",
        "\n",
        "    transform = ImageTransform(size, mean, std)\n",
        "    img_transformed = transform(img, phase=\"val\")  # torch.Size([3, 224, 224])\n",
        "\n",
        "    # (色、高さ、幅)を (高さ、幅、色)に変換し、0-1に値を制限して表示\n",
        "    if display:\n",
        "        img_transformed_ = img_transformed.numpy().transpose((1, 2, 0))\n",
        "        img_transformed_ = np.clip(img_transformed_, 0, 1)\n",
        "        plt.imshow(img_transformed_);plt.show()\n",
        "\n",
        "    # 認識の実施\n",
        "    inputs = transform(img, phase='val')\n",
        "    inputs_ = inputs.unsqueeze_(0)\n",
        "    out = net(inputs_)\n",
        "    outnp = out.detach().numpy()\n",
        "    ids = np.argsort( - outnp[0])\n",
        "    sftmx = softmax(-outnp[0])\n",
        "    #print(sftmx[ids[0]], sftmx[ids[1]], sftmx[ids[2]])\n",
        "    #print(np.sort(sftmx)[:5])\n",
        "\n",
        "    if no == ids[0]:\n",
        "        print('Hit ', end=\"\")\n",
        "    else:\n",
        "        print(colored('Miss', 'red'), end=\"\")\n",
        "\n",
        "    print(ids[:n_best], end=\" \")\n",
        "    for no in ids[:n_best]:\n",
        "        print(tlpa.data[no]['Name'], end=\" \")\n",
        "    print(- np.sort(-sftmx)[:n_best])\n",
        "\n",
        "for i in range(tlpa.__len__()):\n",
        "    diagnose(i, display=False, n_best=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "5_f_40jxuDMO"
      },
      "outputs": [],
      "source": [
        "tlpa.show_all_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "NRCXjKSpuDMO"
      },
      "outputs": [],
      "source": [
        "print(tlpa('動物園'))\n",
        "tlpa.show_an_image('動物園')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAaA3rnFuDMO"
      },
      "outputs": [],
      "source": [
        "from ccap import imagenetDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtNBedAAuDMO"
      },
      "outputs": [],
      "source": [
        "imagenet = imagenetDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSGmWO3LuDMO"
      },
      "outputs": [],
      "source": [
        "imagenet.sample_and_show(799)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJOpcRXluDMO"
      },
      "outputs": [],
      "source": [
        "imagenet.labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acASJtMhuDMO"
      },
      "outputs": [],
      "source": [
        "from ccap import salaDataset\n",
        "\n",
        "sala = salaDataset()\n",
        "sala_img_path = [sala.data[k]['img'] for k in sala.data.keys()]\n",
        "sala.data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-Tc6MCxuDMR"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgZl7-f_uDMR"
      },
      "outputs": [],
      "source": [
        "sala.show_all_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdxpHTQ0uDMR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}