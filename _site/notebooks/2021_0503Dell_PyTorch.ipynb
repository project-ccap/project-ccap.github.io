{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "2021_0503Dell_PyTorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2021_0503Dell_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKirH3pbnv3O"
      },
      "source": [
        "# Dell model の PyTorch 実装 Dell モデル零号機と初号機 \n",
        "\n",
        "- CCAP 資料\n",
        "- date: 2021_0503\n",
        "- author: 浅川伸一\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YySkjwGcoPFr"
      },
      "source": [
        "#Colab 上で実行する際には，まず最初に一度だけ，このセルを実行してください\n",
        "!pip3 install Cython\n",
        "!pip3 install japanize_matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfNO4YPGnv3T"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "#import re\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "np.set_printoptions(suppress=False, formatter={'float': '{:6.3f}'.format})\n",
        "torch.set_printoptions(precision=3)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import japanize_matplotlib"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qP_TWlanv3T"
      },
      "source": [
        "#print('#Dell model の項目間 (ニューロン間) 結合を設定: Ws')\n",
        "Ws = np.array([[1,1,1,1,1,1,1,1,1,1, 0,0,0,0,0,0,0,0,0,0, \n",
        "                0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,\n",
        "                0,0,0,0,0,0,0,0,0,0, 0,0,0,0],            # cat 正解\n",
        "               [0,0,0,0,0,0,0,1,1,1, 1,1,1,1,1,1,1,0,0,0, \n",
        "                0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0, \n",
        "                0,0,0,0,0,0,0,0,0,0, 0,0,0,0],            # dog 意味エラー\n",
        "               [0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,1,1,1, \n",
        "                1,1,1,1,1,1,1,0,0,0, 0,0,0,0,0,0,0,0,0,0,\n",
        "                0,0,0,0,0,0,0,0,0,0, 0,0,0,0],            # mat 形態エラー\n",
        "               [0,0,0,0,0,0,0,1,1,1, 0,0,0,0,0,0,0,0,0,0, \n",
        "                0,0,0,0,0,0,0,1,1,1, 1,1,1,1,0,0,0,0,0,0,\n",
        "                0,0,0,0,0,0,0,0,0,0, 0,0,0,0],            # rat 混合エラー\n",
        "               [0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0, \n",
        "                0,0,0,0,0,0,0,0,0,0, 0,0,0,0,1,1,1,1,1,1,\n",
        "                1,1,1,1,0,0,0,0,0,0, 0,0,0,0],            # fog 無関連エラー\n",
        "               [0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0, \n",
        "                0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,\n",
        "                0,0,0,0,1,1,1,1,1,1, 1,1,1,1]             # lat 非単語\n",
        "              ])\n",
        "\n",
        "#print('#語彙層と音韻層とを結ぶ結合係数行列の定義: Wp and phonology')\n",
        "phonology = {'cat': np.array([ 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]),  # correct\n",
        "             'dog': np.array([ 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]),  # semantic error\n",
        "             'mat': np.array([ 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]),  # formal error\n",
        "             'rat': np.array([ 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]),  # mixed error\n",
        "             'fog': np.array([ 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]),  # unrelated error\n",
        "             'lat': np.array([ 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0])}  # nonword\n",
        "# phonology は BOW 形式なんですなー\n",
        "Wp = np.array([phonology[item] for item in phonology])\n",
        "\n",
        "print('#元データ 意味層と語彙層と結合係数 Ws:\\n', Ws)\n",
        "print('#元データ 語彙層と音韻層と結合係数 Wp:\\n', Wp)\n",
        "\n",
        "tags = list(phonology)\n",
        "print('#相関係数行列 意味入力層<->語彙層 Ws:\\n', tags, '\\n', np.corrcoef(Ws))\n",
        "print('#相関係数行列 語彙層<->音韻層 Wp:\\n', tags, '\\n', \n",
        "      np.corrcoef(np.array([phonology[item] for item in phonology])))\n",
        "\n",
        "#Rp = torch.from_numpy(np.corrcoef(Wp)).requires_grad_(False)\n",
        "#print(Rp)\n",
        "Rp = torch.Tensor(np.corrcoef(Wp)).requires_grad_(False)\n",
        "print(Rp)\n",
        "#Rs = torch.from_numpy(np.corrcoef(Ws)).requires_grad_(False)\n",
        "#print(Rs)\n",
        "Rs = torch.Tensor(np.corrcoef(Ws)).requires_grad_(False)\n",
        "print(Rs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNqgpghJnv3U"
      },
      "source": [
        "def draw_dell_graph(A, B, \n",
        "                    width_inches=9, height_inches=4, xcolor='green', ycolor='red', \n",
        "                    alabel='健常統制群データ(Dell,1997)', blabel='beta 調整後のシミュレーション結果',\n",
        "                    title=None,\n",
        "                    fontsize=18):\n",
        "    \"\"\"Dell モデルのグラフ描画\n",
        "    A と B 比較対象の２つのモデル出力について，６種類の反応カテゴリの\n",
        "    棒グラフを描画\n",
        "\n",
        "    引数:\n",
        "    A: np.array((6))\n",
        "    B: np.array((6))\n",
        "    出力値:\n",
        "        なし\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(width_inches,height_inches))\n",
        "    ax = fig.add_axes([0,0,1,1])\n",
        "    X = np.arange(B.shape[0])\n",
        "    \n",
        "    ax.bar(X-0.2, A, color=xcolor, width=0.4, label=alabel)\n",
        "    ax.bar(X+0.2, B, color=ycolor, width=0.4, label=blabel)\n",
        "    plt.legend(fontsize=fontsize)\n",
        "\n",
        "    # https://www.javaer101.com/ja/article/5091810.html\n",
        "    ax.set_xticks(ax.get_xticks().tolist()) \n",
        "    ax.set_ylim(bottom=0, top=1.0)\n",
        "    ax.set_xticklabels(['', '正解', '意味エラー','形態エラー','混合エラー','無関連エラー', '非単語エラー', ''],fontsize=int(fontsize*0.9))\n",
        "    if title != None:\n",
        "        ax.set_title(title)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#print('Dell model の結果図示')\n",
        "# plt.title('Dell(1997) Tab. 4 より作成:')\n",
        "# Dell(1997)による健常者のデータ\n",
        "#Dells_controls = np.array([0.9690, 0.0120, 0.0010, 0.0090, 0.0030, 0.0000])  \n",
        "Dells_controls = torch.tensor([0.9690, 0.0120, 0.0010, 0.0090, 0.0030, 0.0000])  \n",
        "\n",
        "# Dell(1997)による WD モデルのデータ\n",
        "#Dells_WD = np.array([0.9660, 0.0210, 0.0000, 0.0120, 0.0000, 0.0010])  \n",
        "Dells_WD = torch.tensor([0.9660, 0.0210, 0.0000, 0.0120, 0.0000, 0.0010])  \n",
        "#draw_dell_graph(Dells_controls, Dells_WD, \n",
        "#                alabel=\"Dell(1997)健常者のデータ\", \n",
        "#                blabel=\"Dell(1997) WD モデル\")\n",
        "\n",
        "# Foygel & Dell(2000)による SP モデルのデータ, Tab. 3 Foygel and Dell (2000)\n",
        "#Dells_SP = np.array([0.9722, 0.0126, 0.0011, 0.0138, 0.0002, 0.0001])  \n",
        "Dells_SP = torch.tensor([0.9722, 0.0126, 0.0011, 0.0138, 0.0002, 0.0001])  \n",
        "#draw_dell_graph(Dells_controls, Dells_SP, \n",
        "#                alabel=\"Dell(1997)健常者のデータ\", \n",
        "#                blabel=\"Dell(1997) SP モデル\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N75G5wjwnv3U"
      },
      "source": [
        "draw_dell_graph(Dells_controls.detach().numpy(), Dells_WD.detach().numpy(),\n",
        "               alabel=\"Dell 健常\", blabel='Dell weigth-decay',\n",
        "               title=\"実際のグラフ描画。このグラフはシミュレーションではなく，Dell の元論文データを描画したグラフです\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVUtYRQunv3V"
      },
      "source": [
        "def my_CrossEntropy(t, p, epsilon=10 ** -7):\n",
        "    \"\"\"交差エントロピー\"\"\"\n",
        "    ce = t * torch.log(p) + (1-t) * torch.log(1-t)\n",
        "    return -ce.sum()\n",
        "\n",
        "def my_softmax(x, beta=1.):\n",
        "    \"\"\"ソフトマックス関数 に温度パラメータを付加したバージョン\"\"\"\n",
        "    if isinstance(x, list):\n",
        "        x = torch.Tensor(x)\n",
        "    elif isinstance(x, np.ndarray):\n",
        "        x = torch.Tensor(x)\n",
        "    bias = (x * beta).mean()\n",
        "    return torch.exp(x * beta - bias) / torch.exp(x * beta -bias).sum()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KifgFloAnv3V"
      },
      "source": [
        "#初期設定，\n",
        "#文末 の `.requres_grad(False)` はパラメータの学習，(更新) をしないことを意味する\n",
        "#これが Dell モデルの特徴である\n",
        "Sem = torch.zeros(Rs.shape[0]).requires_grad_(False)  #意味層 \n",
        "\n",
        "#意味層の先頭要素に 1 を立てる。すなわち，`cat` 刺激図版が提示されたことを仮定\n",
        "Sem[0] = 1                \n",
        "\n",
        "Lex = torch.zeros(Rs.shape[0]).requires_grad_(False)  #語彙層\n",
        "Pho = torch.zeros(Rp.shape[0]).requires_grad_(False)  #音韻層\n",
        "\n",
        "class Dell_model0(nn.Module):\n",
        "    \"\"\"Dell model 零号機\"\"\"\n",
        "    def __init__(self, Rs, Rp):\n",
        "        super().__init__()\n",
        "        self.Rs = Rs      #意味層と語彙層との間の結合係数\n",
        "        self.Rp = Rp      #語彙層と音韻層との間の結合係数\n",
        "        self.theta_s = nn.Parameter(torch.randn(1))  #Dell の s パラメータ\n",
        "        self.theta_p = nn.Parameter(torch.randn(1))  #Dell の p パラメータ\n",
        "\n",
        "        self.sem = torch.zeros(self.Rs.shape[0]).requires_grad_(False)\n",
        "        self.sem[0] = 1\n",
        "        self.lex = torch.zeros(self.Rs.shape[0]).requires_grad_(False)\n",
        "        self.pho = torch.zeros(self.Rp.shape[0]).requires_grad_(False)\n",
        "        \n",
        "        \n",
        "        #語彙層のバイアス項 Dell のオリジナルモデルには存在しない        \n",
        "        self.bias_sem = nn.Parameter(torch.zeros(self.Rs.shape[0]))\n",
        "        \n",
        "        #語彙層のバイアス項 Dell のオリジナルモデルには存在しない        \n",
        "        self.bias_lex = nn.Parameter(torch.zeros(self.Rs.shape[0]))\n",
        "        \n",
        "        #音韻層のバイアス項 Dell のオリジナルモデルには存在しない        \n",
        "        self.bias_pho = nn.Parameter(torch.zeros(self.Rp.shape[0]))\n",
        "        \n",
        "        self.beta = nn.Parameter(torch.tensor([1.]))  #ソフトマックス関数の温度パラメータ\n",
        "        self.a = nn.Tanh()                            #ハイパータンジェント関数を活性化関数として採用\n",
        "        \n",
        "    #def forward(self, inp):\n",
        "    def forward(self):\n",
        "        \"\"\"前向きの処理\"\"\"\n",
        "        self.sem = self.a(self.lex @ (self.Rs * self.theta_s) + self.bias_sem)\n",
        "        #self.lex = self.a(self.inp @ (self.Rs * self.theta_s) + self.bias_lex)\n",
        "        self.lex = self.a(self.sem @ (self.Rs * self.theta_s) + self.bias_lex)\n",
        "        self.pho = self.a(self.lex @ (self.Rp * self.theta_p) + self.bias_pho)\n",
        "        return my_softmax(self.pho, beta=self.beta)\n",
        "    \n",
        "\n",
        "def train_dell(target, model, \n",
        "               iter_max=10 ** 3, lr=0.5, loss_f=my_CrossEntropy, \n",
        "               Rp=Rp, Rs=Rs, \n",
        "               verbose=False):\n",
        "    interval = iter_max * 10 ** -1\n",
        "    model = model(Rs,Rp)                      #モデルの初期化\n",
        "    for tau in range(iter_max):               #iter_max 回の繰り返し \n",
        "        #y_hat = model(Sem)                    #入力層へ Sem を与えて，出力値の予測値 y_hat を得る\n",
        "        y_hat = model()                    #入力層へ Sem を与えて，出力値の予測値 y_hat を得る\n",
        "        loss = loss_f(target, y_hat)          #直上行で得られた y_hat と予測すべき値 (患者から得れた値など) とから損失値を得る\n",
        "        #loss.backward()                       #誤差逆伝播を行う\n",
        "        loss.backward(retain_graph=True)\n",
        "        with torch.no_grad():           \n",
        "            for param in model.parameters():  #モデルへのフィッティング = 学習\n",
        "                param -= lr * param.grad\n",
        "            model.zero_grad()\n",
        "        if tau % interval == 0 and verbose:       #途中経過の印字\n",
        "            beta = model.beta.detach().numpy()[0]\n",
        "            s = model.theta_s.detach().numpy()[0]\n",
        "            p = model.theta_p.detach().numpy()[0]\n",
        "            print(f'{tau:05d} loss:{loss:.3f} beta:{1./beta:5.3f} s:{s:5.3f} p:{p:5.3f}')\n",
        "\n",
        "    #得られた結果の出力\n",
        "    beta = model.beta.detach().numpy()[0]\n",
        "    s = model.theta_s.detach().numpy()[0]\n",
        "    p = model.theta_p.detach().numpy()[0]\n",
        "    params = {'beta':beta, 's':s, 'p':p}\n",
        "    return y_hat.detach().numpy(), [beta, s, p]\n",
        "\n",
        "\n",
        "y_hat, [beta, s, p] = train_dell(Dells_controls, Dell_model0)\n",
        "print(f'target:{Dells_controls.detach().numpy()}')\n",
        "print(f'y_hat: {y_hat} beta:{1./beta:.3f} s:{s:.3f} p:{p:.3f}')\n",
        "\n",
        "y_hat, [beta, s, p] = train_dell(Dells_SP, Dell_model0)\n",
        "print(f'target:{Dells_SP.detach().numpy()}')\n",
        "print(f'y_hat: {y_hat} beta:{1./beta:.3f} s:{s:.3f} p:{p:.3f}')\n",
        "\n",
        "y_hat, [beta, s, p] = train_dell(Dells_WD, Dell_model0)\n",
        "print(f'target:{Dells_WD.detach().numpy()}')\n",
        "print(f'y_hat: {y_hat} beta:{1./beta:.3f} s:{s:.3f} p:{p:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJdQVSLHnv3W"
      },
      "source": [
        "for teach in [Dells_controls, Dells_SP, Dells_WD]:\n",
        "    y_hat, [beta, s, p] = train_dell(teach, Dell_model0, verbose=False)\n",
        "    print(f'target:{teach.detach().numpy()}')\n",
        "    print(f'y_hat: {y_hat} beta:{1./beta:.3f} s:{s:.3f} p:{p:.3f}')    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUrjVbm3nv3W"
      },
      "source": [
        "print('#Dell モデルの再現実験, 原著論文 Foygell and Dell (2000)を参照のこと')\n",
        "FoygellDell_tab1={\n",
        "    #Table 1 のデータ\n",
        "    0:{'patient':'LH', 'data':[0.69,0.03,0.07,0.15,0.01,0.02]},\n",
        "    1:{'patient':'IG', 'data':[0.69,0.09,0.05,0.02,0.03,0.01]}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfuGCyglnv3W"
      },
      "source": [
        "for d in FoygellDell_tab1:\n",
        "    patient = FoygellDell_tab1[d]['patient']\n",
        "    teach = torch.tensor(FoygellDell_tab1[d]['data']).requires_grad_(False)\n",
        "    y_hat, [beta, s, p] = train_dell(teach, Dell_model0)\n",
        "    print('-' * 73)\n",
        "    print(f'患者名:{patient}')\n",
        "    print('                ', list(phonology))\n",
        "    print('実データ:        ', teach.detach().numpy())\n",
        "    print(f'target:{teach.detach().numpy()}')\n",
        "    print(f'y_hat: {y_hat} \\nbeta:{1./beta:.3f} s:{s:.3f} p:{p:.3f}')\n",
        "    draw_dell_graph(teach, y_hat, width_inches=7,height_inches=3, fontsize=14, \n",
        "                    alabel='患者イニシャル:{0}'.format(patient), blabel='シミュレーション結果',\n",
        "                    title='Dell モデルの再現実験 (Foygell and Dell,2000)')    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LY21h_Fnv3W"
      },
      "source": [
        "FoygellDell_tab2={\n",
        "    #Table 2 に記載のデータ。小数点以下４桁目が必要なのか疑問。理由は PNT の検査図版は 175 枚しかない。\n",
        "    #どんなに努力しても一回の検査で得られるデータは 175 しかないのだから，小数点以下 2 桁で十分ではないかなー\n",
        "    #各行のデータ，最後の 4 列は weight, decay と s, f パラメータの推定値を表す. \n",
        "    #ただし s, f は table 4 より引用\n",
        "    'W.B':  [0.9400,0.0200,0.0100,0.0100,0.0100,0.0000,0.0200,0.5600, 0.0352, 0.0274],\n",
        "    'T.T.': [0.9300,0.0100,0.0100,0.0000,0.0200,0.0000,0.0200,0.5600, 0.0340, 0.0866],\n",
        "    'J.Fr.':[0.9200,0.0100,0.0100,0.0200,0.0200,0.0000,0.0200,0.5600, 0.0316, 0.0305],\n",
        "    'V.C.': [0.8700,0.0200,0.0100,0.0300,0.0100,0.0000,0.0200,0.5700, 0.0407, 0.0229],\n",
        "    'L.B.': [0.8200,0.0400,0.0200,0.0900,0.0100,0.0100,0.0070,0.5000, 0.0274, 0.0221],\n",
        "    'J.B.': [0.7600,0.0600,0.0100,0.0500,0.0200,0.0100,0.0065,0.5000, 0.0264, 0.0246],\n",
        "    'J.L.': [0.7600,0.0300,0.0100,0.0600,0.0300,0.0100,0.0250,0.6000, 0.0255, 0.0221],\n",
        "    'G.S.': [0.7000,0.0200,0.0600,0.1500,0.0100,0.0200,0.0057,0.5000, 0.0246, 0.0191],\n",
        "    'L.H.': [0.6900,0.0300,0.0700,0.1500,0.0100,0.0200,0.0057,0.5000, 0.0237, 0.0178],\n",
        "    'J.G.': [0.5500,0.0600,0.0800,0.1800,0.0400,0.0300,0.0450,0.7000, 0.0191, 0.0172],\n",
        "    'E.G.': [0.9300,0.0300,0.0000,0.0100,0.0200,0.0000,0.1000,0.6000, 0.0316, 0.0305],\n",
        "    'B.Me.':[0.8400,0.0300,0.0100,0.0000,0.0500,0.0100,0.1000,0.8200, 0.0165, 0.0866],\n",
        "    'B.Mi.':[0.8300,0.0500,0.0100,0.0100,0.0200,0.0100,0.0550,0.7000, 0.0255, 0.0328],\n",
        "    'J.A.': [0.7800,0.0400,0.0000,0.0200,0.0300,0.0100,0.0580,0.7000, 0.0246, 0.0294],\n",
        "    'A.F.': [0.7500,0.0200,0.0300,0.0700,0.0600,0.0400,0.1000,0.8500, 0.0205, 0.0229],\n",
        "    'N.C.': [0.7500,0.0300,0.0700,0.0800,0.0100,0.0000,0.1000,0.8500, 0.0237, 0.0221],\n",
        "    'I.G.': [0.6900,0.0900,0.0500,0.0200,0.0300,0.0100,0.1000,0.8600, 0.0198, 0.0340],\n",
        "    'H.B.': [0.6100,0.0600,0.1300,0.1800,0.0200,0.0100,0.0500,0.7130, 0.0191, 0.0172],\n",
        "    'J.F.': [0.5600,0.1400,0.0100,0.0200,0.1100,0.0100,0.1000,0.8600, 0.0107, 0.0365],\n",
        "    'G.L.': [0.2800,0.0400,0.2100,0.3000,0.0300,0.0900,0.0790,0.8500, 0.0093, 0.0154],\n",
        "    'W.R.': [0.0800,0.0600,0.1500,0.2800,0.0500,0.3300,0.1000,0.9400, 0.0010, 0.0178]\n",
        "}\n",
        "\n",
        "\n",
        "for patient, teacher in FoygellDell_tab2.items():\n",
        "    teacher = np.array(teacher[:6])\n",
        "    teacher = torch.tensor(teacher)\n",
        "    pred, [beta, s, p] = train_dell(teacher, Dell_model0)\n",
        "    print('-' * 73)\n",
        "    print(f'患者名:{patient}')\n",
        "    print('                ', list(phonology))\n",
        "    print('実データ:        ', teacher.detach().numpy())\n",
        "    print('シミュレーション: ', pred)\n",
        "    print(f'ベータ:{1/beta:.3f} s:{s:.3f} p:{p:.3f}')\n",
        "    draw_dell_graph(teacher,pred, width_inches=7,height_inches=3, fontsize=14, \n",
        "                    alabel='患者イニシャル:{0}'.format(patient), blabel='シミュレーション結果'\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N9SnKVMnv3X"
      },
      "source": [
        "print('上図で赤いバーが緑のバーよりも高くなっていることについての考察')\n",
        "print('特に一番確率の高い反応カテゴリーで，赤いバーが高くなっています。')\n",
        "print('これは，Dell のオリジナルデータがそもそも足し合わせて １ になっていないからでしょう。')\n",
        "print('以下に原著論文の表から反応をすべて足し合わせた結果を示します:')\n",
        "for patient in FoygellDell_tab2:\n",
        "    print(patient, FoygellDell_tab2[patient][:7],end=\".sum() :\")\n",
        "    print(f'{np.array(FoygellDell_tab2[patient][:7]).sum():.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyl_Dyb4nv3X"
      },
      "source": [
        "#import gensim.downloader as api\n",
        "#glove_en = api.load('word2vec-google-news-300', return_path=True)\n",
        "\n",
        "#DellX = np.zeros((len(tags),len(glove_en['cat'])))\n",
        "#for i, word in enumerate(tags):\n",
        "#    DellX[i] = np.copy(glove_en[word])\n",
        "\n",
        "#Dell_wordR = np.corrcoef(DellX)\n",
        "#np.savetxt('Dell_wordR.txt',Dell_wordR)\n",
        "# Dell の Lexicon 間の相関係数行列。存在しない場合は，このセル上部のコメントをはずして実行\n",
        "\n",
        "ccap_stim_dict = {\n",
        "    #寺尾先生から教えていただいた刺激語リスト\n",
        "    '2morae':{'words':['サル', 'リス', 'ざる', 'サイ'],\n",
        "              'R':np.array([[1.   , 0.607, 0.458, 0.504],\n",
        "                            [0.607, 1.   , 0.453, 0.449],\n",
        "                            [0.458, 0.453, 1.   , 0.677],\n",
        "                            [0.504, 0.449, 0.677, 1.   ]],dtype=np.float32)},\n",
        "    '3morae':{'words':['ゴリラ', 'ヒグマ', 'コレラ', 'クジラ'],\n",
        "              'R':np.array([[1.   , 0.537, 0.46 , 0.572],\n",
        "                            [0.537, 1.   , 0.364, 0.58 ],\n",
        "                            [0.46 , 0.364, 1.   , 0.561],\n",
        "                            [0.572, 0.58 , 0.561, 1.   ]])},\n",
        "    '4morae':{'words':['シマウマ', 'トナカイ', 'トラウマ', 'シロクマ'],\n",
        "              'R':np.array([[1.   , 0.481, 0.397, 0.534],\n",
        "                            [0.481, 1.   , 0.293, 0.352],\n",
        "                            [0.397, 0.293, 1.   , 0.36 ],\n",
        "                            [0.534, 0.352, 0.36 , 1.   ]])},\n",
        "    '5morae':{'words':['オットセイ', 'カンガルー', 'カットソー', 'オポッサム'],\n",
        "              'R':np.array([[1.   , 0.402, 0.386, 0.629],\n",
        "                            [0.402, 1.   , 0.378, 0.525],\n",
        "                            [0.386, 0.378, 1.   , 0.403],\n",
        "                            [0.629, 0.525, 0.403, 1.   ]])},\n",
        "    '5morae': {'words':['オットセイ', 'カンガルー', 'カットソー', 'ネットカフェ'],\n",
        "               'R':np.array([[1.   , 0.402, 0.386, 0.28 ],\n",
        "                             [0.402, 1.   , 0.378, 0.281],\n",
        "                             [0.386, 0.378, 1.   , 0.265],\n",
        "                             [0.28 , 0.281, 0.265, 1.   ]])}}\n",
        "\n",
        "for morae in ccap_stim_dict:\n",
        "    print(morae, end=\": \")\n",
        "    for x in ccap_stim_dict[morae]:\n",
        "        print(ccap_stim_dict[morae][x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzNi4-5YqnQU"
      },
      "source": [
        "!pip3 install Levenshtein\n",
        "!pip3 install jaconv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVQTBUUInv3X"
      },
      "source": [
        "print('音韻類似度の近似としてレーベンシュタイン距離を用いる')\n",
        "import Levenshtein\n",
        "import jaconv\n",
        "def levenshtein_distance(a, b):\n",
        "    return Levenshtein.distance(a, b)\n",
        "\n",
        "l = [jaconv.hira2kata(w) for w in ccap_stim_dict['2morae']['words']]\n",
        "\n",
        "def make_phon_R(words):\n",
        "    ret = np.zeros((len(l),len(l)))\n",
        "    for i, w1 in enumerate(l):\n",
        "        for j, w2 in enumerate(l):\n",
        "            ret[i][j] = levenshtein_distance(jaconv.hira2kata(w1),jaconv.hira2kata(w2))\n",
        "    return ret\n",
        "\n",
        "for vocab_set in ccap_stim_dict:\n",
        "    word_list = ccap_stim_dict[vocab_set]['words']\n",
        "    vocab_set_R = make_phon_R(word_list)\n",
        "    print(word_list, '\\n', vocab_set_R, '\\n最大距離: ', vocab_set_R.max(), '\\n', vocab_set_R/vocab_set_R.max())\n",
        "    print('---\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmhH5dM7nv3X"
      },
      "source": [
        "stim_words = ccap_stim_dict['2morae']\n",
        "Rs = ccap_stim_dict['2morae']['R']\n",
        "Rp = make_phon_R(stim_words)\n",
        "print(stim_words)\n",
        "print(Rs)\n",
        "print(Rp/Rp.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDtvC4Lonv3Y"
      },
      "source": [
        "class Dell_model1(nn.Module):\n",
        "    \"\"\"Dell model 初号機\"\"\"\n",
        "    def __init__(self, Rs, Rp):\n",
        "        super().__init__()\n",
        "        if isinstance(Rs, np.ndarray):    #意味層と語彙層との間の結合係数\n",
        "            self.Rs = torch.from_numpy(Rs.astype(np.float32)).requires_grad_(False)\n",
        "        if isinstance(Rp, np.ndarray):    #語彙層と音韻層との間の結合係数\n",
        "            self.Rp = torch.from_numpy(Rp.astype(np.float32)).requires_grad_(False)\n",
        "        self.theta_s = nn.Parameter(torch.randn(1))  #Dell の s パラメータ\n",
        "        self.theta_p = nn.Parameter(torch.randn(1))  #Dell の p パラメータ\n",
        "\n",
        "        self.sem = torch.zeros(self.Rs.shape[0]).requires_grad_(False)\n",
        "        self.sem[0] = 1\n",
        "        self.lex = torch.zeros(self.Rs.shape[0]).requires_grad_(False)\n",
        "        self.pho = torch.zeros(self.Rp.shape[0]).requires_grad_(False)\n",
        "        \n",
        "        \n",
        "        #語彙層のバイアス項 Dell のオリジナルモデルには存在しない        \n",
        "        self.bias_sem = nn.Parameter(torch.zeros(self.Rs.shape[0]))\n",
        "        \n",
        "        #語彙層のバイアス項 Dell のオリジナルモデルには存在しない        \n",
        "        self.bias_lex = nn.Parameter(torch.zeros(self.Rs.shape[0]))\n",
        "        \n",
        "        #音韻層のバイアス項 Dell のオリジナルモデルには存在しない        \n",
        "        self.bias_pho = nn.Parameter(torch.zeros(self.Rp.shape[0]))\n",
        "        \n",
        "        self.beta = nn.Parameter(torch.tensor([1.]))  #ソフトマックス関数の温度パラメータ\n",
        "        self.a = nn.Tanh()                            #ハイパータンジェント関数を活性化関数として採用\n",
        "        \n",
        "    #def forward(self, inp):\n",
        "    def forward(self):\n",
        "        \"\"\"前向きの処理\"\"\"\n",
        "        self.sem = self.a(self.lex @ (self.Rs * self.theta_s) + self.bias_sem)\n",
        "        #self.lex = self.a(self.inp @ (self.Rs * self.theta_s) + self.bias_lex)\n",
        "        self.lex = self.a(self.sem @ (self.Rs * self.theta_s) + self.bias_lex)\n",
        "        self.pho = self.a(self.lex @ (self.Rp * self.theta_p) + self.bias_pho)\n",
        "        return my_softmax(self.pho, beta=self.beta)\n",
        "    \n",
        "\n",
        "def train_dell1(target, model, \n",
        "               iter_max=10 ** 3, lr=0.5, loss_f=my_CrossEntropy, \n",
        "               Rp=Rp, Rs=Rs, \n",
        "               verbose=False):\n",
        "    interval = iter_max * 10 ** -1\n",
        "    model = model(Rs,Rp)                      #モデルの初期化\n",
        "    for tau in range(iter_max):               #iter_max 回の繰り返し \n",
        "        #y_hat = model(Sem)                    #入力層へ Sem を与えて，出力値の予測値 y_hat を得る\n",
        "        y_hat = model()                    #入力層へ Sem を与えて，出力値の予測値 y_hat を得る\n",
        "        loss = loss_f(target, y_hat)          #直上行で得られた y_hat と予測すべき値 (患者から得れた値など) とから損失値を得る\n",
        "        #loss.backward()                       #誤差逆伝播を行う\n",
        "        loss.backward(retain_graph=True)\n",
        "        with torch.no_grad():           \n",
        "            for param in model.parameters():  #モデルへのフィッティング = 学習\n",
        "                param -= lr * param.grad\n",
        "            model.zero_grad()\n",
        "        if tau % interval == 0 and verbose:       #途中経過の印字\n",
        "            beta = model.beta.detach().numpy()[0]\n",
        "            s = model.theta_s.detach().numpy()[0]\n",
        "            p = model.theta_p.detach().numpy()[0]\n",
        "            print(f'{tau:05d} loss:{loss:.3f} beta:{1./beta:5.3f} s:{s:5.3f} p:{p:5.3f}')\n",
        "\n",
        "    #得られた結果の出力\n",
        "    beta = model.beta.detach().numpy()[0]\n",
        "    s = model.theta_s.detach().numpy()[0]\n",
        "    p = model.theta_p.detach().numpy()[0]\n",
        "    params = {'beta':beta, 's':s, 'p':p}\n",
        "    return y_hat.detach().numpy(), [beta, s, p]\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saKke_R_nv3Y"
      },
      "source": [
        "#適当な数値を入れて実験\n",
        "patient_resp = torch.tensor([0.7, 0.1, 0.5, 0.15])  \n",
        "y_hat, [beta, s, p] = train_dell1(patient_resp, Dell_model1)\n",
        "print(f'target:{patient_resp.detach().numpy()}')\n",
        "print(f'y_hat: {y_hat} beta:{1./beta:.3f} s:{s:.3f} p:{p:.3f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S28aNncZrE4f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}