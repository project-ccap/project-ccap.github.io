{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "2021-0412Dell_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2021_0412Dell_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81a960ad"
      },
      "source": [
        "# Simulations and explanation for the weight-decay and sp models\n",
        "- date: 2021-0412\n",
        "- author: Shin Asakawa\n",
        "- Origin: Foygel and Dell (2000) Models of Impaired Lexical Access in Speech Production, \n",
        "- Journal of Memory and Language 43, 182–216 (2000) doi:10.1006/jmla.2000.2716\n",
        "\n",
        "<!--\n",
        "## 蘊蓄\n",
        "\n",
        "ボックスアンドアローモデル，日本名，箱と矢印モデルの起源は，言うまでもなく，情報処理分野からの借用であろう。\n",
        "情報処理モデルを取り入れた，認知心理学関係者のモデルが認知心理学的症状を説明するために用いられたことが起源であろう。\n",
        "[Wikipedia の記述](https://en.wikipedia.org/wiki/Cognitive_model)でもそのように書かれている。\n",
        "ところで，ボックスアンドアローモデルを，\n",
        "２つに分けることができると思う。\n",
        "1. フローチャート型 flowchart\n",
        "2. 力学系型 dyanmical system\n",
        "\n",
        "慣例として，いずれのモデルでも，コンピュータコードとして実装が存在すると，計算モデルと呼ばれるようである。\n",
        "Marr の意味での 計算論モデル は数学的に記述可能なモデルを指す。一方，数学的な記述とは無関係に計算モデルと呼ばれることもある。\n",
        "\n",
        "神経心理学との関連で言えば，\n",
        "二重経路連接モデル (Coltheart ら,2000) はフローチャート型モデルである。\n",
        "一方，相互活性化モデル(McClelland and Rumelhart, 1981) は力学系型のモデルといえる。\n",
        "\n",
        "両者の相違を簡単に記述すると，以下のようになる:\n",
        "1. フローチャート型モデル: 箱の中で記述されている内容が実行されてから，次に矢印に示されている次の箱に処理を移し，次の箱によって指示されている処理が実行されると考える\n",
        "2. 力学系モデル: 箱の状態は時々刻々変化する。その変化が矢印を通じて他の箱に伝達される。矢印で指示された次の箱の処理は，前の箱の処理が終了することを仮定しないで自律的に動作する。言い換えれば，力学系モデルでは，各箱に状態が存在し，その状態が変動することを仮定する。\n",
        "\n",
        "一方で，フローチャートモデルでは，状態に重きを置くのではなく，手順として記述される。\n",
        "フローチャートで状態変化を記述することも可能であるし，力学系モデルで手順を記述することも可能である。\n",
        "このため両者は，しばしば同一視して用いられることもある。\n",
        "ある段階 (ステージ) が終了してから，次の段階が駆動されるような逐次段階を仮定する初期の，二重経路連接モデルと，各段階の状態が時間変動する相互活性化モデルとの相違である。\n",
        "\n",
        "いずれのモデルが優れている，劣っているということではない。\n",
        "そうではなく，両モデルの相違を念頭に置いて議論した方がモデルを評価する際に便利であろうと思う。\n",
        "\n",
        "ニューラルネットワーク用語で言えば，フローチャート型モデルの代表は，階層型ニューラルネットワークであり，力学系モデルの代表はリカレントニューラルネットワークである。\n",
        "階層型とリカレント型との両ニューラルネットワークの相違は，自己結合を持つか否かである。\n",
        "次式は，それぞれのニューラルネットワークの動作式を表している:\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "y_i &= a\\left(\\sum_i w_ix_i + b_i\\right), \\hspace{3em}\\text{階層型の場合}\\\\\n",
        "y_{i,t+1} &= a\\left(y_{i,t}+\\sum_i w_ix_{i,t-1} + b_i\\right),\\hspace{3em}\\text{リカレント型}\n",
        "\\end{cases}\n",
        "$$ \n",
        "\n",
        "ここで，$y$:ニューロンの出力，$a$:出力関数, $w$:シナプス結合強度, $x$:ニューロンへ入力信号 を表す。\n",
        "-->"
      ],
      "id": "81a960ad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be31f5cb"
      },
      "source": [
        "from IPython import display\n",
        "\n",
        "print('Foygell and Dell (2000) Fig. 1. オリジナルは Dell (1997) 3 つの層の結合は双方向。\\n上から意味層, 語彙層, 音素層。最上層の意味層で 暗く塗りつぶされているニューロンは `cat`, `dog`, `rat` で共有されていることを示す。')\n",
        "\n",
        "display.Image(url='https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/master/figures/2000Foygel_Dell_fig1.png', width=480)\n"
      ],
      "id": "be31f5cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "711a1c83"
      },
      "source": [
        "print('必要となるライブラリの読み込み...', end=\" \")\n",
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from numpy.random import Generator, PCG64\n",
        "rng = Generator(PCG64())\n",
        "#rng.standard_normal()\n",
        "\n",
        "# 表示精度桁数の設定\n",
        "#np.set_printoptions(suppress=False, formatter={'float': '{:7.4f}'.format})\n",
        "np.set_printoptions(suppress=False, formatter={'float': '{:6.3f}'.format})\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#事前に japanize_matplotlib のインストールが必要です。次行，行頭の #を削除してく，このセルを実行してください\n",
        "!pip install japanize_matplotlib\n",
        "import japanize_matplotlib\n",
        "print('done')\n",
        "\n",
        "rng = Generator(PCG64())  # 乱数の定義"
      ],
      "id": "711a1c83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b86b5c2"
      },
      "source": [
        "print('#Dell モデルの再現実験, 原著論文 Foygell and Dell (2000)を参照のこと')\n",
        "FoygellDell_tab1={\n",
        "    #Table 1 のデータ\n",
        "    'LH':[0.69,0.03,0.07,0.15,0.01,0.02],\n",
        "    'IG':[0.69,0.09,0.05,0.02,0.03,0.01]\n",
        "}\n",
        "\n",
        "FoygellDell_tab2_and_4={\n",
        "    #Table 2 に記載のデータ。小数点以下４桁目が必要なのか疑問。理由は PNT の検査図版は 175 枚しかない。\n",
        "    #どんなに努力しても一回の検査で得られるデータは 175 しかないのだから，小数点以下 2 桁で十分ではないかなー\n",
        "    #各行のデータ， 最初の 6 列は，それぞれののデータ，\n",
        "    #2行目最後の 4 列は weight, decay パラメータの推定値を表す. Foygell and Dell (2000)table 4 より引用\n",
        "    #3行目は Tab.4 の sp モデル\n",
        "    'W.B.': [[0.9400,0.0200,0.0100,0.0100,0.0100,0.0000,0.0200,0.5600, 0.0352, 0.0274],\n",
        "             [0.9416,0.0309,0.0063,0.0137,0.0054,0.0021,0.0255,0.5781,0.0054,2.282],\n",
        "             [0.9419,0.0282,0.0076,0.0172,0.0047,0.0004,0.0352,0.0274,0.0051, 2.194]],\n",
        "    'T.T.': [[0.9300,0.0100,0.0100,0.0000,0.0200,0.0000,0.0200,0.5600, 0.0340, 0.0866],\n",
        "             [0.9247,0.0496,0.0063,0.0055,0.0131,0.0008,0.0542,0.6758,0.0168,7.657],\n",
        "            [0.9689,0.0077,0.0109,0.0000,0.0125,0.0000,0.0340,0.0866,0.0162,1.194]],\n",
        "    'J.Fr.':[[0.9200,0.0100,0.0100,0.0200,0.0200,0.0000,0.0200,0.5600, 0.0316, 0.0305],\n",
        "             [0.9061,0.0536,0.0111,0.0160,0.0116,0.0016,0.0470,0.6602,0.0191,7.782],\n",
        "             [0.9332,0.0366,0.0074,0.0140,0.0070,0.0018,0.0316,0.0305,0.0135,8.566]],\n",
        "    'V.C.':[[0.8700,0.0200,0.0100,0.0300,0.0100,0.0000,0.0200,0.5700, 0.0407, 0.0229],\n",
        "            [0.9145,0.0390,0.0112,0.0265,0.0056,0.0032,0.0213,0.5703,0.0199,3.267],\n",
        "            [0.9317,0.0246,0.0079,0.0305,0.0053,0.0000,0.0407,0.0229,0.0253,1.694]],\n",
        "    'L.B.':[[0.8200,0.0400,0.0200,0.0900,0.0100,0.0100,0.0070,0.5000, 0.0274, 0.0221],\n",
        "            [0.8248,0.0528,0.0300,0.0708,0.0066,0.0150,0.0087,0.5156,0.0107,2.641],\n",
        "            [0.8213,0.0534,0.0273,0.0828,0.0075,0.0077,0.0274,0.0221,0.0070,1.306]],\n",
        "    'J.B.':[[0.7600,0.0600,0.0100,0.0500,0.0200,0.0100,0.0065,0.5000, 0.0264, 0.0246],\n",
        "            [0.8288,0.0798,0.0266,0.0374,0.0183,0.0091,0.0523,0.6953,0.0305,4.458],\n",
        "            [0.8419,0.0564,0.0259,0.0561,0.0084,0.0113,0.0264,0.0246,0.0345,6.088]],\n",
        "    'J.L.':[[0.7600,0.0300,0.0100,0.0600,0.0300,0.0100,0.0250,0.6000, 0.0255, 0.0221],\n",
        "            [0.8288,0.0798,0.0266,0.0374,0.0183,0.0091,0.0523,0.6953,0.0368,11.966],\n",
        "            [0.7921,0.0601,0.0375,0.0867,0.0107,0.0129,0.0255,0.0221,0.0251,14.040]],\n",
        "    'G.S.': [[0.7000,0.0200,0.0600,0.1500,0.0100,0.0200,0.0057,0.5000, 0.0246, 0.0191],\n",
        "             [0.6894,0.0673,0.0615,0.1404,0.0087,0.0327,0.0070,0.5156,0.0208,6.865],\n",
        "             [0.7182,0.0555,0.0505,0.1511,0.0080,0.0167,0.0246,0.0191,0.0168,4.570]],\n",
        "    'L.H.':[[0.6900,0.0300,0.0700,0.1500,0.0100,0.0200,0.0057,0.5000,0.0237,0.0178],\n",
        "            [0.6767,0.0700,0.0671,0.1466,0.0088,0.0308,0.0065,0.5117,0.0179,4.773],\n",
        "            [0.6712,0.0573,0.0661,0.1768,0.0085,0.0201,0.0237,0.0178,0.0175,3.166]],\n",
        "    'J.G.':[[0.5500,0.0600,0.0800,0.1800,0.0400,0.0300,0.0450,0.7000,0.0191,0.0172],    \n",
        "            [0.5543,0.1121,0.1091,0.1542,0.0246,0.0457,0.0514,0.7227,0.0281,8.988],\n",
        "            [0.5667,0.0746,0.0882,0.2170,0.0123,0.0412,0.0191,0.0172,0.0217,13.273]],\n",
        "    'E.G.':[[0.9300,0.0300,0.0000,0.0100,0.0200,0.0000,0.1000,0.6000,0.0316,0.0305],\n",
        "            [0.9222,0.0498,0.0017,0.0062,0.0199,0.0002,0.0698,0.7148,0.0089,2.129],\n",
        "            [0.9332,0.0336,0.0074,0.0140,0.0070,0.0018,0.0316,0.0305,0.0070,6.245]],\n",
        "    'B.Me.':[[0.8400,0.0300,0.0100,0.0000,0.0500,0.0100,0.1000,0.8200,0.0165,0.0866],\n",
        "             [0.8424,0.0925,0.0147,0.0216,0.0258,0.0030,0.0750,0.7617,0.0290,18.265],\n",
        "             [0.8468,0.0313,0.0938,0.0000,0.0177,0.0104,0.0165,0.0866,0.0368,23.438]],\n",
        "    'B.Mi.':[[0.8300,0.0500,0.0100,0.0100,0.0200,0.0100,0.0550,0.7000,0.0255,0.0328],\n",
        "             [0.8712,0.0620,0.0202,0.0280,0.0120,0.0066,0.0407,0.6484,0.0198,4.914],\n",
        "            [0.8973,0.0538,0.0181,0.0128,0.0078,0.0102,0.0255,0.0328,0.0282,5.012]],\n",
        "    'J.A.':[[0.7800,0.0400,0.0000,0.0200,0.0300,0.0100,0.0580,0.7000,0.0246,0.0294],\n",
        "            [0.8455,0.0824,0.0191,0.0259,0.0234,0.0037,0.0626,0.7227,0.0331,10.487],\n",
        "            [0.8627,0.0637,0.0263,0.0236,0.0093,0.0144,0.0246,0.0294,0.0378,15.927]],\n",
        "    'A.F.':[[0.7500,0.0200,0.0300,0.0700,0.0600,0.0400,0.1000,0.8500,0.0205,0.0229],\n",
        "            [0.7318,0.1004,0.0565,0.0679,0.0258,0.0176,0.0573,0.7246,0.0391,26.456],\n",
        "            [0.7113,0.0841,0.0644,0.0935,0.0123,0.0344,0.0205,0.0229,0.0401,45.699]],\n",
        "    'N.C.':[[0.7500,0.0300,0.0700,0.0800,0.0100,0.0000,0.1000,0.8500,0.0237,0.0221],\n",
        "            [0.7683,0.0621,0.0488,0.0929,0.0079,0.0200,0.0078,0.5156,0.0199,8.503],\n",
        "            [0.7693,0.0638,0.0479,0.0907,0.0082,0.0201,0.0237,0.0221,0.0205,8.810]],\n",
        "    'I.G.':[[0.6900,0.0900,0.0500,0.0200,0.0300,0.0100,0.1000,0.8600,0.0198,0.0340],\n",
        "            [0.7594,0.1116,0.0475,0.0457,0.0255,0.0103,0.0661,0.7500,0.0315,4.534],\n",
        "            [0.7948,0.0863,0.0544,0.0131,0.0138,0.0376,0.0198,0.0340,0.0449,10.018]],\n",
        "    'H.B.':[[0.6100,0.0600,0.1300,0.1800,0.0200,0.0100,0.0500,0.7130,0.0191,0.0172],\n",
        "            [0.5526,0.1002,0.1120,0.1697,0.0170,0.0485,0.0400,0.6816,0.0337,9.922],\n",
        "            [0.5667,0.0746,0.0882,0.2170,0.0123,0.0412,0.0191,0.0172,0.0322,10.628]],\n",
        "    'J.F.':[[0.5600,0.1400,0.0100,0.0200,0.1100,0.0100,0.1000,0.8600,0.0107,0.0365],\n",
        "            [0.7129,0.1359,0.0452,0.0587,0.0391,0.0082,0.0982,0.8594,0.0721,37.590],\n",
        "            [0.5237,0.1369,0.1662,0.0136,0.0178,0.1418,0.0107,0.0365,0.0928,131.684]],\n",
        "    'G.L.':[[0.2800,0.0400,0.2100,0.3000,0.0300,0.0900,0.0790,0.8500,0.0093,0.0154],\n",
        "            [0.2500,0.0985,0.2036,0.3121,0.0282,0.1076,0.0806,0.8594,0.0284,7.351],\n",
        "            [0.2716,0.1020,0.1565,0.3230,0.0137,0.1332,0.0093,0.0154,0.0397,15.974]],\n",
        "    'W.R.':[[0.0800,0.0600,0.1500,0.2800,0.0500,0.3300,0.1000,0.9400,0.0010,0.0178],\n",
        "            [0.1598,0.0859,0.2027,0.4039,0.0242,0.1235,0.0965,0.9336,0.1068,82.627],\n",
        "            [0.1343,0.1179,0.2319,0.2594,0.0132,0.2433,0.0010,0.0178,0.0610,37.527]]}\n",
        "\n",
        "\n",
        "def draw_dell_graph(A, B, \n",
        "                    title=None,\n",
        "                    width_inches=7, height_inches=4,\n",
        "                    alabel='健常統制群データ(Dell,1997)', acolor='red',\n",
        "                    blabel='beta 調整後のシミュレーション結果', bcolor='green',\n",
        "                    fontsize=16):\n",
        "    \"\"\"Dell モデルのグラフ描画\n",
        "    A と B 比較対象の２つのモデル出力について，６種類の反応カテゴリの\n",
        "    棒グラフを描画\n",
        "\n",
        "    引数:\n",
        "    A: np.array((6))\n",
        "    B: np.array((6))\n",
        "    出力値:\n",
        "        なし\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(width_inches,height_inches))\n",
        "    ax = fig.add_axes([0,0,1,1])\n",
        "    X = np.arange(B.shape[0])\n",
        "    \n",
        "    ax.bar(X-0.2, A, color=acolor, width=0.4, label=alabel)\n",
        "    ax.bar(X+0.2, B, color=bcolor, width=0.4, label=blabel)\n",
        "    plt.legend(fontsize=fontsize)\n",
        "    if isinstance(title, str):\n",
        "        plt.title(title,fontsize=fontsize)\n",
        "    # https://www.javaer101.com/ja/article/5091810.html\n",
        "    ax.set_xticks(ax.get_xticks().tolist()) \n",
        "    ax.set_ylim(bottom=0, top=1.0)\n",
        "    ax.set_xticklabels(['', '正解', '意味エラー','形態エラー','混合エラー','無関連エラー', '非単語エラー', ''],fontsize=int(fontsize*0.8))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#plt.title('Dell(1997) Tab. 4 より作成:')\n",
        "# Dell(1997)による健常者のデータ\n",
        "Dells_controls = np.array([0.9690, 0.0120, 0.0010, 0.0090, 0.0030, 0.0000])  \n",
        "\n",
        "# Dell(1997)による WD モデルのデータ\n",
        "Dells_WD = np.array([0.9660, 0.0210, 0.0000, 0.0120, 0.0000, 0.0010])  \n",
        "draw_dell_graph(Dells_controls, Dells_WD, \n",
        "                title='Dell model の結果図示',\n",
        "                alabel=\"Dell(1997)健常者のデータ\", \n",
        "                blabel=\"Dell(1997) WD モデル\")\n",
        "\n",
        "# Foygel & Dell(2000)による SP モデルのデータ, Tab. 3 Foygel and Dell (2000)\n",
        "Dells_SP = np.array([0.9722, 0.0126, 0.0011, 0.0138, 0.0002, 0.0001])  \n",
        "draw_dell_graph(Dells_controls, Dells_SP, \n",
        "                alabel=\"Dell(1997)健常者のデータ\", \n",
        "                blabel=\"Dell(1997) SP モデル\")\n"
      ],
      "id": "3b86b5c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d09fbca4-5433-44de-bcf2-aea3b8660afd"
      },
      "source": [
        "def draw_dell_graph3(A, B, C, name='noname',\n",
        "                     title = None,\n",
        "                     acolor='green', bcolor='red', ccolor='blue',\n",
        "                     alabel='健常統制群データ(Dell,1997)', \n",
        "                     blabel='Weight decay',\n",
        "                     clabel='s-p model',\n",
        "                     width_inches=4, height_inches=3,\n",
        "                     fontsize=12):\n",
        "    \"\"\"Dell モデルのグラフ描画\n",
        "    A, B, C 比較対象の 3 つのモデル出力について，６種類の反応カテゴリの棒グラフを描画\n",
        "\n",
        "    引数:\n",
        "    A: np.array((6))\n",
        "    B: np.array((6))\n",
        "    C: np.array((6))\n",
        "    出力値:\n",
        "        なし\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(width_inches,height_inches))\n",
        "    ax = fig.add_axes([0,0,1,1])  #  [left, bottom, width, height] of the new axes\n",
        "    A, B, C = np.array(A[:6]), np.array(B[:6]), np.array(C[:6])\n",
        "    X = np.arange(B.shape[0])\n",
        "    \n",
        "    ax.bar(X-0.2, A, color=acolor, width=0.2, label=alabel)\n",
        "    ax.bar(X    , B, color=bcolor, width=0.2, label=blabel)\n",
        "    ax.bar(X+0.2, C, color=ccolor, width=0.2, label=clabel)\n",
        "    plt.legend(fontsize=fontsize)\n",
        "    if isinstance(title, str):\n",
        "        plt.ttile(title)\n",
        "\n",
        "    # https://www.javaer101.com/ja/article/5091810.html\n",
        "    ax.set_xticks(ax.get_xticks().tolist()) \n",
        "    ax.set_ylim(bottom=0, top=1.0)\n",
        "    ax.set_title('Patient {0}'.format(name))\n",
        "    ax.set_xticklabels(['', '正解', '意味エラー','形態エラー','混合エラー','無関連エラー', '非単語エラー', ''],fontsize=int(fontsize*0.8))\n",
        "    plt.show()\n",
        "\n"
      ],
      "id": "d09fbca4-5433-44de-bcf2-aea3b8660afd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05ea4483-dcb5-4618-82ac-c6fbc8221764"
      },
      "source": [
        "np.set_printoptions(suppress=False, formatter={'float': '{:4.2f}'.format})\n",
        "\n",
        "print('#Dell のデータ, 実データ, wd model, sp model, Foygell and Dell (2000) Table 2 and 4 を参照のこと')\n",
        "for patient in FoygellDell_tab2_and_4:\n",
        "    data_n = 0\n",
        "    data_n_wd = 1\n",
        "    data_n_sp = 2\n",
        "    data = np.array(FoygellDell_tab2_and_4[patient][data_n][:6])\n",
        "    wd_data = np.array(FoygellDell_tab2_and_4[patient][data_n_wd][:6])\n",
        "    sp_data = np.array(FoygellDell_tab2_and_4[patient][data_n_sp][:6])\n",
        "    print('{0:5s} {1} {2} {3}'.format(patient, data, wd_data, sp_data))\n",
        "    draw_dell_graph3(data, wd_data, sp_data, name=patient, \n",
        "                     alabel=patient, blabel='WD モデル推定値', clabel='SP モデル推定値',\n",
        "                     width_inches=6, height_inches=3, \n",
        "                     fontsize=12, \n",
        "                     ccolor='black', acolor='gray', bcolor='pink')\n"
      ],
      "id": "05ea4483-dcb5-4618-82ac-c6fbc8221764",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48d9cf0b",
        "outputId": "51a6892d-bbed-4cff-e4b8-63bf3208d505"
      },
      "source": [
        "print('#ニューラルネットワーク業界で頻用される出力関数の一行野郎 one liner 定義...', end='')\n",
        "sigmoid = lambda x: 1/(1+np.exp(-x)) # ロジスティックシグモイド関数\n",
        "d_sigmoid = lambda x: x * (1-x)\n",
        "\n",
        "ReLU = lambda x: np.maximum(0, x)\n",
        "d_ReLU = lambda x:[1 if x_ > 0 else 0 for x_ in x]\n",
        "\n",
        "tanh = lambda x: np.tanh(x)\n",
        "d_tanh = lambda x: 1 - x**2\n",
        "print('done')\n",
        "\n",
        "print('#ソフトマックス関数の定義...', end='')\n",
        "def softmax(x, beta=1.):\n",
        "    \"\"\"softmax 関数の定義\n",
        "    引数\n",
        "    x: np.array\n",
        "        softmax を行う変数群，ベクトルを仮定\n",
        "    beta: float\n",
        "        逆温度，この値の逆数が温度\n",
        "        \n",
        "    戻り値\n",
        "    x.shape[0] 次元の多項確率密度\n",
        "    \"\"\"\n",
        "    if not isinstance(x, np.ndarray):\n",
        "        return None\n",
        "\n",
        "    if x.ndim == 1:\n",
        "        return np.exp(x * beta)/np.exp(x * beta).sum()\n",
        "    else:\n",
        "        return np.array([np.exp(x_ * beta)/np.exp(x_ * beta).sum() for x_ in x])\n",
        "\n",
        "print('done')\n",
        "\n",
        "print('#交差エントロピー誤差関数との定義...', end='')\n",
        "def CE_loss(teach, pred):\n",
        "    \"\"\"\n",
        "    交差エントロピー誤差の計算\n",
        "    - 引数\n",
        "    teach: np.array\n",
        "        教師信号ベクトル\n",
        "    pred: np.array\n",
        "        予測値ベクトル\n",
        "            \n",
        "    - 戻り値\n",
        "    loss: float\n",
        "        損失関数の値\n",
        "    \"\"\"\n",
        "    epsilon = np.finfo(np.float).eps\n",
        "    #epsilon = 0.001\n",
        "    #loss = - (teach * np.log(pred) + (1-teach) * np.log(1-pred)).sum()\n",
        "    loss = 0.\n",
        "    for t, p in zip(teach, pred):\n",
        "        if t > epsilon and p > epsilon:\n",
        "            loss += t * np.log(p)\n",
        "            \n",
        "    return -1. * loss\n",
        "\n",
        "print('done')\n"
      ],
      "id": "48d9cf0b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#ニューラルネットワーク業界で頻用される出力関数の一行野郎 one liner 定義...done\n",
            "#ソフトマックス関数の定義...done\n",
            "#交差エントロピー誤差関数との定義...done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18cb6650"
      },
      "source": [
        "# メモ\n",
        "<!--\n",
        "上の２つの結合係数行列は Dell らが用いたものである。ところがこの結合係数行列を\n",
        "相関係数行列で置き換えることができるだろう。そう考えて，相関係数行列を計算してみたのが上のセルの出力中では下２つの行列である。\n",
        "\n",
        "* **相関係数で考えておく理由**は次の通り:<br/>\n",
        "Dell らは入力層のニューロン数を 54 個としている。ところが実際に動物を表現しているニューロン数は誰も数えたことが無いし，しかも，数えることが現時点では不可能である。\n",
        "また，人によっても異なるであろう。\n",
        "そのため 54 個というニューロン数の決定について恣意的であると言わざるを得ない。\n",
        "ニューロン個数の恣意性を排除するために，各ニューロンを抽象化かつ簡便化すると，各概念について一つだけのニューロンを用意することが最小構成となる。\n",
        "\n",
        "そのために，以下では，\n",
        "1. Dell モデルのニューロン個数を用いた再現実験と，その後，\n",
        "2. 相関係数行列を用いた改変モデルを示す。さらに，\n",
        "3. Dell モデルでは説明の困難なパラメータ推定問題について，勾配降下法を用いて解決する方法を示す。さらに，その後，\n",
        "4. 任意の単語に拡張し，日本語版も示すことにする。\n",
        "-->"
      ],
      "id": "18cb6650"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e1ff189"
      },
      "source": [
        "\n",
        "合成関数の微分則から，損失関数をパラメータ $\\mathbf{\\theta}$ の各成分 $\\beta,\\theta_d,\\theta_s,\\theta_p$ で微分することで勾配降下法による学習を定義できる。\n",
        "Dell の動作方程式を若干書き換える\n",
        "\n",
        "$$\n",
        "x_{t+1} = (1-d)x_t + \\theta_{s,p} \\sum_i w_i x_i +\\mathcal{N}(0,a_1^2+a_2^2)\\hspace{1em}\\text{(Dell's original)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_{i,t+1} = a\\left((1-\\theta_d)x_{i,t} + \\theta_{s,p} \\sum_j w_{j,t-1} x_{j,t}+b_i\\right) \\hspace{1em}\\text{(modified)}\n",
        "$$\n",
        "\n",
        "ここで， $a$ は活性化関数である。$a=\\mathbb{1}$ のとき Dell モデルに一致する。\n",
        "$y$ は出力値， $x$ は内部状態とする。元来，Dell モデルは RNN とみなせるで，内部状態と外部出力とを区別することが妥当であると思われる。 \n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\frac{\\partial l}{\\partial \\theta}&=\\frac{\\partial l}{\\partial p}\\frac{\\partial p}{\\partial{y}}\\frac{\\partial y}{\\partial \\theta}\\\\\n",
        "&\\sum_i\\frac{t_i-p_i}{p_i\\left(1-p_i\\right)}\\sum_j p_j\\left(\\delta_{ij}-p_j\\right)\\frac{\\partial y_j}{\\partial\\theta}\\\\\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "ここで $\\delta_{ij}$ は Dirac の delta であるとする。\n",
        "\n",
        "$a$ が Dell の 式(1) であるとすると $\\partial l/\\partial\\theta$ は次式のようになる:\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\frac{\\partial l}{\\partial\\theta}&=\\sum_i\\frac{t_i-p_i}{p_i\\left(1-p_i\\right)}\\sum_j p_i\\left(\\delta_{ij}-p_j\\right) \\frac{d}{da_j}\\frac{\\partial a_j}{\\partial\\theta}\\hspace{5cm}\\text{ここで， $\\frac{d}{da_j}\\frac{\\partial a_j}{\\partial\\theta}=c_j$ とおくと}\\\\\n",
        "&=\\sum_i\\frac{t_i-p_i}{p_i\\left(1-p_i\\right)}p_i\\left(c_i-\\sum_jp_jc_j\\right)\\\\\n",
        "&=\\sum_i\\frac{t_i-p_i}{1-p_i}\\left(c_i-\\bar{c}\\right),\\\\\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "ただし，$\\mu_{x_{t-1}}$ は下位層のニューロンの出力に確率 $p_i$ を掛けて総和したもの，すなわち平均である。\n",
        "\n",
        "\n"
      ],
      "id": "5e1ff189"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db8ab63f"
      },
      "source": [
        "以下では， \n",
        "- $t_i$: `teacher`\n",
        "- $p_i$: `pred`\n",
        "- $t_i-p_i$: `delta`"
      ],
      "id": "db8ab63f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b000dd43"
      },
      "source": [
        "class Dell2021():\n",
        "    \"\"\"Optimization for the Foygell and Dell model's parameter vector.\n",
        "    ソフトマックス関数により多項分布の確率密度に変換した値から\n",
        "    損失関数を算出し，その損失間数値に基づいて勾配降下法に従って各パラメータ最適化を行う\n",
        "    ただし beta は逆温度である。1/beta が実際の温度\n",
        "    \n",
        "    $\\beta$ に加えて，$\\theta=(w,d,s,p)$ についても $\\partial l/\\partial\\theta$ を用いて\n",
        "    更新する\n",
        "    \"\"\"\n",
        "    \n",
        "#    def __init__(self, w=0.1, d=0.5, s=0.0698, p=0.1,\n",
        "    def __init__(self, d=0.5, param=None, # w=0.1, d=0.5, s=1., p=1.,\n",
        "                 beta_init=1., iter_max=10**5, lr=0.5, \n",
        "                 time_n = 16,\n",
        "                 teacher=None,\n",
        "                 interval=None, verbose=True, nonstop=False,\n",
        "                 IFG_beta=15.,  # 頭頂葉の関与を表す\n",
        "                 f=tanh, Weights='R', model='WD', data=None):\n",
        "        \"\"\"\n",
        "        - 引数\n",
        "        beta_init: float\n",
        "            ソフトマックス関数を計算する際に用いる温度パラメータの初期値\n",
        "        iter_max: int\n",
        "            繰り返し回数の上限値。Dell のオリジナルモデルは iter_max = 16 に固定されていた\n",
        "        lr: float\n",
        "            学習係数\n",
        "        interval: int\n",
        "            途中結果を出力する際の間隔。\n",
        "            デフォルトでは iter_max の 1/10 毎に途中結果を出力する\n",
        "        verbose: bool\n",
        "            True なら途中結果の冗長な出力を印字する\n",
        "            デフォルト: True\n",
        "        nonstop: bool\n",
        "            途中で学習を打ち切るフラグ\n",
        "            デフォルト: False\n",
        "        f: function\n",
        "            出力関数\n",
        "        W: str\n",
        "            'R': 相関係数行列を用いる\n",
        "            'softmax': 相関係数をソフトマックスに変換して用いる\n",
        "            else: 相関係数行列のソフトマックス変換を用いる\n",
        "        \"\"\"\n",
        "        self.sem_jolt = 10 # 元論文 (Foygell and Dell, 2000) page 185 footnote\n",
        "        self.lex_jolt = 100\n",
        "        self.jolt_t = 7  # カウントが 0 ベースなので t=7 (8-1) が 8 回目\n",
        "        \n",
        "        if param == None:\n",
        "            self.param = {'WD':{'Weight':0.1,  # for normal\n",
        "                                'Decay':0.5    # for normal\n",
        "                               },\n",
        "                          'SP':{'Decay': 0.6,\n",
        "                                'S_Weight':0.0698,\n",
        "                                'P_Weight':0.1000\n",
        "                               }}\n",
        "        else:\n",
        "            self.param = param\n",
        "\n",
        "        if model=='WD':\n",
        "            self.s, self.p = self.param['WD']['Weight'], self.param['WD']['Weight']\n",
        "        else:\n",
        "            self.s, self.p = self.param['SP']['S_Weight'], self.param['SP']['P_Weight']\n",
        "\n",
        "\n",
        "        # 音素の定義\n",
        "        self.phonemes = {'onset':['f', 'r', 'd', 'k', 'm'],\n",
        "                         'vowel':['ae', 'o'],\n",
        "                         'coda':['t','g']}\n",
        "        self.Onset, self.Vowel, self.Coda = slice(0,5), slice(5,7), slice(7,9)\n",
        "\n",
        "        self.teacher = teacher\n",
        "        self.data = None\n",
        "        self.nonstop = nonstop\n",
        "        self.beta = beta_init\n",
        "        self.iter_max = iter_max\n",
        "        self.lr = lr\n",
        "        if interval == None:\n",
        "            # interval が定義されていなければ設定する\n",
        "            self.interval = iter_max * 10**-1\n",
        "        else:\n",
        "            self.interval = interval\n",
        "        if verbose == None:\n",
        "            self.verbose = False\n",
        "        else:\n",
        "            self.verbose = True\n",
        "        #self.epsilon = 10 ** -5\n",
        "        self.epsilon = np.finfo(np.float).eps\n",
        "\n",
        "        self.max_t = 16  # Dell の 1 試行あたりの繰り返し数 相互活性化モデルでの繰り返し更新回数\n",
        "\n",
        "        self.Weights = Weights\n",
        "        if self.Weights == 'R':              #相関係数\n",
        "            _, self.Ws, _ = self.init_Ws()\n",
        "            _, self.Wp, _ = self.init_Wp()\n",
        "        elif self.Weights == 'softmax':      #ソフトマックス\n",
        "            _, _, self.Ws = self.init_Ws()\n",
        "            _, _, self.Wp = self.init_Wp()\n",
        "        else:                                #オリジナル\n",
        "            self.Ws, _, _ = self.init_Ws()\n",
        "            self.Wp, _, _ = self.init_Wp()\n",
        "        self.lexicon_candidates = list(self.phonology.keys())\n",
        "\n",
        "        # nubmer of neurons of each layer\n",
        "        #self.n_sem, self.n_lex, self.n_pho, self.n_phon = 6, 6, 6, 9\n",
        "        self.n_sem = self.Ws.shape[1]\n",
        "        self.n_lex = self.Ws.shape[0]\n",
        "        if self.n_lex != self.Wp.shape[0]:  #整合性チェックのため\n",
        "            assert('self.n_lex != self.Wp.shape[0]:')\n",
        "        self.n_pho = self.Wp.shape[1]\n",
        "\n",
        "        self.sem_slice = slice(0,                    self.n_sem)\n",
        "        self.lex_slice = slice(self.n_sem,           self.n_sem+self.n_lex)\n",
        "        self.pho_slice = slice(self.n_sem+self.n_lex,self.n_sem+self.n_lex+self.n_pho)\n",
        "\n",
        "        self.time_n = time_n\n",
        "        self.IFG_beta = IFG_beta\n",
        "        self.decay = d\n",
        "        #self.a1, self.a2 = 0.01, 0.16\n",
        "        self.a1, self.a2 = 0., 0.\n",
        "        self.f = f\n",
        "\n",
        "        self.W = self.make_W()\n",
        "        if data == None:\n",
        "            self.teacher = np.array([0.97, 0.01, 0.00, 0.01, 0.00, 0.00])\n",
        "            self.one_trial(verbose=verbose)\n",
        "        else:\n",
        "            init_neurons()\n",
        "            self.data = data\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def init_Ws(self):\n",
        "        Ws = np.array([[1,1,1,1,1,1,1,1,1,1, 0,0,0,0,0,0,0,0,0,0, \n",
        "                        0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,\n",
        "                        0,0,0,0,0,0,0,0,0,0, 0,0,0,0],             # cat 正解\n",
        "                       [0,0,0,0,0,0,0,1,1,1, 1,1,1,1,1,1,1,0,0,0, \n",
        "                        0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0, \n",
        "                        0,0,0,0,0,0,0,0,0,0, 0,0,0,0],             # dog 意味エラー\n",
        "                       [0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,1,1,1, \n",
        "                        1,1,1,1,1,1,1,0,0,0, 0,0,0,0,0,0,0,0,0,0,\n",
        "                        0,0,0,0,0,0,0,0,0,0, 0,0,0,0],             # mat 形態エラー\n",
        "                       [0,0,0,0,0,0,0,1,1,1, 0,0,0,0,0,0,0,0,0,0, \n",
        "                        0,0,0,0,0,0,0,1,1,1, 1,1,1,1,0,0,0,0,0,0,\n",
        "                        0,0,0,0,0,0,0,0,0,0, 0,0,0,0],             # rat 混合エラー\n",
        "                       [0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0, \n",
        "                        0,0,0,0,0,0,0,0,0,0, 0,0,0,0,1,1,1,1,1,1,\n",
        "                        1,1,1,1,0,0,0,0,0,0, 0,0,0,0],             # fog 無関連エラー\n",
        "                       [0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0, \n",
        "                        0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,\n",
        "                        0,0,0,0,1,1,1,1,1,1, 1,1,1,1]              # lat 非単語\n",
        "                      ], dtype=np.float)\n",
        "        Ws *= self.s\n",
        "        R_ws = np.corrcoef(Ws)\n",
        "        return Ws, R_ws, softmax(R_ws,beta=1)\n",
        "\n",
        "\n",
        "    def init_Wp(self):\n",
        "        # 語彙層と音韻層とを結ぶ結合係数行列の定義\n",
        "        #                                     f.   r.   d.   k.   m.   ae.  o.   t.   g\n",
        "        self.phonology = {'cat': np.array([ 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]),  # correct\n",
        "                          'dog': np.array([ 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]),  # semantic error\n",
        "                          'mat': np.array([ 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]),  # formal error\n",
        "                          'rat': np.array([ 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]),  # mixed error\n",
        "                          'fog': np.array([ 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]),  # unrelated error\n",
        "                          'lat': np.array([ 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0])}  # nonword\n",
        "        # phonology は BOW 形式なんですなー\n",
        "        Wp = np.zeros((len(self.phonology), len(self.phonology['cat'])), dtype=np.float)\n",
        "        for i, x in enumerate(self.phonology.values()):\n",
        "            Wp[i] = np.copy(x)\n",
        "\n",
        "        Wp *= self.p\n",
        "        R_wp = np.corrcoef(np.array([self.phonology[item] for item in self.phonology]))\n",
        "        return Wp, R_wp, softmax(R_wp, beta=1)\n",
        "\n",
        "\n",
        "    def make_W(self):\n",
        "        \"\"\"大きな遷移行列の作成\n",
        "            意味，語彙，音韻の3層を一列にならべてベクトル表現した場合\n",
        "    \n",
        "        戻り値:\n",
        "        - W: np.array\n",
        "            単一結合係数行列\n",
        "        \"\"\"\n",
        "        if self.Weights == 'R':              #相関係数\n",
        "            _, self.Ws, _ = self.init_Ws()\n",
        "            _, self.Wp, _ = self.init_Wp()\n",
        "        elif self.Weights == 'softmax':      #ソフトマックス\n",
        "            _, _, self.Ws = self.init_Ws()\n",
        "            _, _, self.Wp = self.init_Wp()\n",
        "        else:                                #オリジナル\n",
        "            self.Ws, _, _ = self.init_Ws()\n",
        "            self.Wp, _, _ = self.init_Wp()\n",
        "\n",
        "        nL, nS, nP = self.n_lex, self.n_sem, self.n_pho\n",
        "        \n",
        "        Isem, Ilex, Ipho = np.eye(nS), np.eye(nL), np.eye(nP)\n",
        "        Dsem, Dlex, Dpho = np.eye(nS) * self.decay, np.eye(nL) * self.decay, np.eye(nP) * self.decay\n",
        "        \n",
        "        Z_sl = np.zeros((nS, nP))\n",
        "        c1 = np.concatenate((Isem-Dsem, self.Ws.T, Z_sl), axis=1)\n",
        "        c2 = np.concatenate((self.Ws, Ilex-Dlex, self.Wp), axis=1)\n",
        "        c3 = np.concatenate((Z_sl.T, self.Wp.T, Ipho-Dpho), axis=1)\n",
        "        W = np.concatenate((c1,c2,c3))\n",
        "        return W\n",
        "\n",
        "    \n",
        "    def init_neurons(self):\n",
        "        \"\"\"各層のニューロンを初期化\"\"\"\n",
        "        Sem = np.zeros([self.n_sem,], dtype=np.float)  # 意味層\n",
        "        Lex = np.zeros([self.n_lex,], dtype=np.float)  # 語彙層\n",
        "        Pho = np.zeros([self.n_pho,], dtype=np.float)  # 音韻層\n",
        "        \n",
        "        # Sem[0] すなわち cat 画像だけ 1 に設定する。すなわちネコ画像が入力されたことを意味\n",
        "        #Sem[0] = 1.                   \n",
        "        Sem[:10] = np.copy(np.ones(10)) * self.sem_jolt\n",
        "\n",
        "        ret = np.concatenate((Sem,Lex,Pho))  # 一行のベクトルになるように改変\n",
        "        return ret\n",
        "\n",
        "    \n",
        "    def split(self, X=None):\n",
        "        if not isinstance(X, np.ndarray):\n",
        "            X = np.copy(self.y)\n",
        "        #s,l,p = np.copy(self.y[self.sem_slice]), np.copy(self.y[self.lex_slice]), np.copy(self.y[self.pho_slice])\n",
        "        s,l,p = np.copy(X[self.sem_slice]), np.copy(X[self.lex_slice]), np.copy(X[self.pho_slice])\n",
        "        \n",
        "        return s, l, p\n",
        "\n",
        "    \n",
        "    def merge(self, s, l, p):\n",
        "        ret = np.concatenate((s,l,p))\n",
        "        return ret\n",
        "\n",
        "    \n",
        "    #def update(self, Sem, Lex, Pho, f=None):\n",
        "    def update(self, f=None):\n",
        "        \"\"\"Dell の 3 層相互活性化モデルに従って各層のニューロンの活性値を更新\n",
        "    \n",
        "        引数:\n",
        "        - f: function\n",
        "            出力関数。指定しなければ変換せずに値を出力\n",
        "            Dell らのモデルは恒等写像。そのため各ニューロンの活性値が指数関数的増加，減衰を示す。\n",
        "            それを回避するためには何らかの正規化，正則化が必要になると考えられる。\n",
        "            Sigmoid, tanh, ReLu, \n",
        "    \n",
        "        戻り値:\n",
        "        #- Sem_next: np.array\n",
        "        #    次刻の意味層ベクトル\n",
        "        #- Lex_next: np.array\n",
        "        #    次刻の語彙層ベクトル\n",
        "        #- Pho_next: np.array\n",
        "        #    次刻の音韻層ベクトル\n",
        "        \"\"\"\n",
        "        \n",
        "        self.Sem, self.Lex, self.Pho = self.split(self.y)\n",
        "        Sem_next = self.Sem * (1. - self.decay) + self.s * np.matmul(self.Ws.T, self.Lex)\n",
        "        Lex_next = self.Lex * (1. - self.decay) + self.s * np.matmul(self.Ws, self.Sem) + self.p * np.matmul(self.Wp, self.Pho)\n",
        "        Pho_next = self.Pho * (1. - self.decay) + self.p * np.matmul(self.Wp.T, self.Lex)\n",
        "    \n",
        "        # ノイズの付加\n",
        "        for x in [Sem_next, Lex_next, Pho_next]:\n",
        "            for i, xx in enumerate(x):\n",
        "                xx += rng.standard_normal() * (self.a1 + self.a2 * xx)\n",
        "                #x[i] = max(0, xx)  # ReLU\n",
        "\n",
        "        # 非線形変換 or not\n",
        "        if f == None:\n",
        "            self.Sem, self.Lex, self.Pho = Sem_next, Lex_next, Pho_next \n",
        "        else:\n",
        "            self.Sem, self.Lex, self.Pho = f(Sem_next), f(Lex_next), f(Pho_next)\n",
        "            \n",
        "        #return self.Sem, self.Lex, self.Pho\n",
        "        return self.merge(self.Sem, self.Lex, self.Pho)\n",
        "\n",
        "\n",
        "    def grad(self, Teach, X):\n",
        "        # 予測値\n",
        "        Prob = softmax(X, beta=self.beta)  \n",
        "        \n",
        "        # 教師信号と出力値との差分ベクトル\n",
        "        Delta = Teach - Prob\n",
        "        \n",
        "        # 誤差の微分\n",
        "        g_Delta = Delta / (Prob * (1-Prob)) \n",
        "        \n",
        "        return Prob, Delta, g_Delta\n",
        "\n",
        "    \n",
        "    def IFG_process(self):\n",
        "        \"\"\"IFg: Inferior Frontal gyrus から／へ の相互作用を実現する\n",
        "        \"\"\"\n",
        "        ##self.Sem = softmax(self.Sem, beta=beta_fixed)\n",
        "        #Lex = self.y[self.n_sem:self.n_sem+self.n_lex]\n",
        "        #Lex = softmax(Lex, beta=self.IFG_beta)\n",
        "        \n",
        "        #Pho = self.y[self.n_sem+self.n_lex:]\n",
        "        #Pho = softmax(Pho, beta=self.IFG_beta)\n",
        "        \n",
        "        #self.y[self.n_sem:self.n_sem+self.n_lex] = np.copy(Lex)\n",
        "        #self.y[self.n_sem+self.n_lex:] = np.copy(Pho)\n",
        "        #self.Lex = softmax(self.Lex, beta=self.IFG_beta)\n",
        "        #self.Pho = softmax(self.Pho, beta=self.IFG_beta)\n",
        "\n",
        "        Sem, Lex, Pho = self.split()\n",
        "        self.y[self.lex_slice] = np.copy(softmax(Lex))\n",
        "        \n",
        "        \n",
        "    def one_trial(self, verbose=False):\n",
        "        \"\"\"Dell モデルの一試行を実施\n",
        "        言い換えれば一枚のネコ画像を提示して反応を収集\n",
        "        \n",
        "        argments:\n",
        "        max_t: int\n",
        "            相互活性化モデルでの最終時刻，相互活性を繰り返す回数の上限値\n",
        "            デフォルトでは 16 回。\n",
        "            原著論文 (Foygell and Dell, 2000) では n と記載されている\n",
        "        beta_fixed: float\n",
        "            前頭葉 へ／から の相互作用の影響を制御するパラメータ\n",
        "            実際にはソフトマックス関数の温度パラメータ\n",
        "            \n",
        "        returns:\n",
        "        self.y: np.array\n",
        "            \n",
        "        sefl.seriesY: np.array((self.y, max_t))\n",
        "            各層の履歴\n",
        "        \"\"\"\n",
        "        time_n = self.time_n\n",
        "        self.y = self.init_neurons()       #各層のニューロンの初期値の設定\n",
        "\n",
        "        # BPTT を用いて, w, d, s, p の各パラメータを更新するため各ニューロンの履歴を保存領域の確保\n",
        "        self.seriesY = np.zeros((time_n, len(self.y)), dtype=np.float)\n",
        "        \n",
        "        for t in range(self.max_t):\n",
        "            if t == self.jolt_t:\n",
        "                s,l,p = self.split()\n",
        "                l[np.argmax(l)] = self.lex_jolt\n",
        "                self.y[self.lex_slice] = np.copy(l)\n",
        "\n",
        "            if verbose:\n",
        "                if t == 0:\n",
        "                    print('         {0}'.format(self.lexicon_candidates))\n",
        "                    \n",
        "                lex = self.convert_onset_vowel_coda_2_lexicon(self.y)\n",
        "                pred = softmax(lex, beta=self.beta)\n",
        "                loss = CE_loss(self.teacher, pred)\n",
        "                print('t={0:02d} lex:{1}'.format(t+1, self.y[self.lex_slice]), end=' ')\n",
        "                print('pred:{0} loss:{1:.3f} beta:{2:.3f}'.format(pred, loss, self.beta))\n",
        "\n",
        "            self.seriesY[t] = np.copy(self.y)\n",
        "            self.y = np.dot(self.W, self.y)   #各層のニューロンの値を更新 \n",
        "            \n",
        "        return self.y, self.seriesY\n",
        "\n",
        "\n",
        "    #def convert_onset_vowel_coda_2_lexicon_invert(self, X, verbose=False):\n",
        "    def convert_onset_vowel_coda_2_lexicon(self, X, verbose=False):\n",
        "        \"\"\"phonlogy 表現からもっともらしい語彙表現を検索し，その当てはまり度合いから\n",
        "        語彙表象の値を返すことを考える。\n",
        "        今一つのアイデアとしては，\n",
        "        phonolgy から lexicon への逆変換を考えて，そこでの値をソフトマックスする。\n",
        "        1. phonolgy 層を onset, vowel, coda の 3 部分に分解\n",
        "        2. 分解した各音で softmax を実施し確率密度に変換\n",
        "        3. 3 つの部分を合成して phoneme 表現とする\n",
        "        4. phoneme 表現と phonology の 6 単語間の交差エントロピー誤差を計算する\n",
        "        5. 6 単語との交差エントロピー誤差の値から，最小値を引き， (最大値-最小値) で割ることで 0-1 に変換\n",
        "        6. マイナス 1 を掛けて 上限反転させて +1 することで， 0 から 1 までの値にする\n",
        "        \"\"\"\n",
        "        val = np.finfo(np.float).max     #np で定義済の安全な最大値を代入\n",
        "        #val = 0.\n",
        "        name = 'NG'                      #len(words) + 1 語彙名のダミー\n",
        "        s, lexeme, phoneme = self.split(X)\n",
        "        lex = np.dot(self.Wp, phoneme)\n",
        "        #lex = sigmoid(lex)\n",
        "        return lex\n",
        "        \n",
        "        for i, word in enumerate(self.phonology):\n",
        "            loss = CE_loss(phonology[word], phoneme)\n",
        "            lex[i] = loss\n",
        "            if loss < val:\n",
        "                name = word\n",
        "                val = loss\n",
        "        lex_diff = lex.max() - lex.min()\n",
        "        lex = - ((lex - lex.min()) / lex_diff) + 1.\n",
        "        return lex\n",
        "\n",
        "    \n",
        "    \n",
        "    def convert_onset_vowel_coda_2_lexicon_ok(self, X, verbose=False):\n",
        "        \"\"\"phonlogy 表現からもっともらしい語彙表現を検索し，その当てはまり度合いから\n",
        "        語彙表象の値を返すことを考える。\n",
        "        1. phonolgy 層を onset, vowel, coda の 3 部分に分解\n",
        "        2. 分解した各音で softmax を実施し確率密度に変換\n",
        "        3. 3 つの部分を合成して phoneme 表現とする\n",
        "        4. phoneme 表現と phonology の 6 単語間の交差エントロピー誤差を計算する\n",
        "        5. 6 単語との交差エントロピー誤差の値から，最小値を引き， (最大値-最小値) で割ることで 0-1 に変換\n",
        "        6. マイナス 1 を掛けて 上限反転させて +1 することで， 0 から 1 までの値にする\n",
        "        \"\"\"\n",
        "        val = np.finfo(np.float).max     #np で定義済の安全な最大値を代入\n",
        "        #val = 0.\n",
        "        name = 'NG'                      #len(words) + 1 語彙名のダミー\n",
        "        s, lexeme, phoneme = self.split(X)\n",
        "        onset, vowel, coda = softmax(phoneme[:5]), softmax(phoneme[5:7]), softmax(phoneme[7:])\n",
        "        phoneme = np.concatenate((onset,vowel,coda))\n",
        "        lex = np.zeros_like(lexeme)\n",
        "        \n",
        "        for i, word in enumerate(self.phonology):\n",
        "            loss = CE_loss(phonology[word], phoneme)\n",
        "            lex[i] = loss\n",
        "            if loss < val:\n",
        "                name = word\n",
        "                val = loss\n",
        "        lex_diff = lex.max() - lex.min()\n",
        "        #lex = - (lex - lex.min() / lex_diff) + 1.\n",
        "        lex = - ((lex - lex.min()) / lex_diff) + 1.\n",
        "        return lex\n",
        "\n",
        "\n",
        "    def convert_onset_vowel_coda_2_lexicon_sigmoid(self, X, verbose=False):\n",
        "        \"\"\"phonlogy 表現からもっともらしい語彙表現を検索し，その当てはまり度合いから\n",
        "        語彙表象の値を返すことを考える。\n",
        "        1. phonolgy 層を sigmoid 変換して phoneme とする\n",
        "        2. phoneme 表現 と phonology の 6 単語間の交差エントロピー誤差を計算する\n",
        "        5. 6 単語との交差エントロピー誤差の値から，最小値を引き， (最大値-最小値) で割ることで 0-1 に変換\n",
        "        6. マイナス 1 を掛けて 上限反転させて +1 することで， 0 から 1 までの値にする\n",
        "        \"\"\"\n",
        "        val = np.finfo(np.float).max     #np で定義済の安全な最大値を代入\n",
        "        #val = 0.\n",
        "        name = 'NG'                      #len(words) + 1 語彙名のダミー\n",
        "        \n",
        "        s, lexeme, phoneme = self.split(X)\n",
        "        lex = np.zeros_like(lexeme)\n",
        "        phoneme = sigmoid(phoneme)\n",
        "        \n",
        "        for i, word in enumerate(self.phonology):\n",
        "            loss = CE_loss(phonology[word], phoneme)\n",
        "            lex[i] = loss\n",
        "            if loss < val:\n",
        "                name = word\n",
        "                val = loss\n",
        "        lex_diff = lex.max() - lex.min()\n",
        "        lex = - (lex - lex.min() / lex_diff) + 1.\n",
        "        return lex\n",
        "    \n",
        "\n",
        "    def fit(self, teacher=None, data=None, verbose=False, nonstop=False):\n",
        "        \"\"\"[]\n",
        "        fit は学習させることを意味するから命名\n",
        "\n",
        "        引数:\n",
        "        teacher: np.array\n",
        "            教師信号。\n",
        "        data: np.array\n",
        "            データ。教師信号に近づくように学習させる\n",
        "            \n",
        "        戻り値:\n",
        "        beta: float\n",
        "            温度パラメータ\n",
        "        [loss, pred, mse]\n",
        "        loss: float\n",
        "            損失関数の値\n",
        "        pred: np.array\n",
        "            予測値\n",
        "        mse: float\n",
        "            最小自乗誤差\n",
        "        \"\"\"\n",
        "        if isinstance(teacher, np.ndarray):\n",
        "            self.teacher = teacher\n",
        "        elif self.teacher == None:\n",
        "            print('Set the teacher signal.')\n",
        "            sys.exit()\n",
        "\n",
        "        if isinstance(data, np.ndarray):\n",
        "            self.data = data\n",
        "        else:\n",
        "            self.y, self.seriesY = self.one_trial(verbose=verbose)\n",
        "            #self.data = self.y\n",
        "\n",
        "        lex_est = self.convert_onset_vowel_coda_2_lexicon(self.y)\n",
        "        pred = softmax(lex_est)\n",
        "        loss0 = CE_loss(self.teacher, pred)  #損失関数の初期値\n",
        "        nonstopped = False\n",
        "        \n",
        "        satu_flag = False\n",
        "        \n",
        "        for i in range(self.iter_max):\n",
        "            \n",
        "            self.y, self.seriesY = self.one_trial(verbose=False)\n",
        "            lex_est = self.convert_onset_vowel_coda_2_lexicon(self.y)\n",
        "            pred = softmax(lex_est, beta=self.beta)\n",
        "            \n",
        "            loss = CE_loss(self.teacher, pred)\n",
        "            if loss0 < (loss + self.epsilon) and (not satu_flag) and (i>0):\n",
        "                print('iteration saturated at {0:05d} loss:{1:.3f}'.format(i,loss))\n",
        "                satu_flag = True\n",
        "            if loss0 > loss:\n",
        "                loss0 = loss\n",
        "                \n",
        "            if (i % self.interval) == 0:  # 途中結果の出力\n",
        "                if self.verbose:\n",
        "                    delta = self.teacher - pred                #教師信号と出力値との差分ベクトル\n",
        "                    mse = (delta**2).sum()/delta.shape[0]\n",
        "                    print('t:{0:05d} '.format(i),end='')\n",
        "                    print('beta:{0:6.3f} decay:{1:.3f} s:{2:.3f} p:{3:.3f}, loss:{4:.3f} pred:{5}'.format(\n",
        "                        1/self.beta, self.decay, self.s, self.p, loss, pred))\n",
        "                if (loss0 - loss) < self.epsilon and (i > 0) and (not nonstopped):\n",
        "                    print('Iteration terminated at {} times'.format(i))\n",
        "                    return [self.beta, self.decay, self.p, self.s], [loss, pred]\n",
        "            \n",
        "            self.y, self.seriesY = self.one_trial(verbose=verbose)\n",
        "            lex = self.convert_onset_vowel_coda_2_lexicon(self.y)\n",
        "            pred = softmax(lex, beta=self.beta)\n",
        "            delta = self.teacher - pred                #教師信号と出力値との差分ベクトル\n",
        "            g_delta = delta / (1-pred)\n",
        "\n",
        "            #beta の更新\n",
        "            ybar = lex.mean()\n",
        "            ybar2 = np.dot(pred, lex)\n",
        "            #Z = np.exp(lex).sum()\n",
        "            #ydiff = lex - ybar/(Z + self.epsilon)\n",
        "            ydiff = lex - ybar\n",
        "            #ydiff = lex - ybar2\n",
        "            self.beta += self.lr * (g_delta * ydiff).sum()\n",
        "\n",
        "            s, l, p = self.split(self.seriesY[-1])\n",
        "            W_pp = self.W[self.pho_slice, self.pho_slice]\n",
        "            W_lp = self.W[self.lex_slice, self.pho_slice]\n",
        "            W_sl = self.W[self.sem_slice, self.lex_slice]\n",
        "            \n",
        "            d_delta = np.dot(W_lp.T, g_delta)\n",
        "            y = np.dot(W_pp,p)\n",
        "            ybar = y.mean()\n",
        "            ydiff = y - ybar\n",
        "            self.decay += self.lr * (d_delta * ydiff).sum()\n",
        "\n",
        "            l_delta = np.dot(W_lp, d_delta)\n",
        "            y = np.dot(W_lp,p)\n",
        "            ybar = y.mean()\n",
        "            ydiff = y - ybar\n",
        "            self.p += self.lr * (l_delta * ydiff).sum()\n",
        "            \n",
        "            s_delta = np.dot(W_sl, l_delta)\n",
        "            y = np.dot(W_lp,p)\n",
        "            y2 = np.dot(W_sl,y)\n",
        "            ybar = y2.mean()\n",
        "            ydiff = y2 - ybar\n",
        "            #print('546 ydiff.sahpe:', ydiff.shape)\n",
        "            #print('547 s_delta.shape:', s_delta.shape)\n",
        "            #print('584 ', s_delta * ydiff)\n",
        "            #sys.exit()\n",
        "            self.s += self.lr * (s_delta * ydiff).sum()\n",
        "                \n",
        "            #self.decay -= self.lr * d_decay\n",
        "            #if self.decay < self.epsilon:\n",
        "            #    self.decay = self.epsilon\n",
        "            #for x, y  in zip([self.s, self.p],[d_s, d_p]):\n",
        "            #    x += self.lr * y\n",
        "\n",
        "\n",
        "        # 平均自乗誤差の計算\n",
        "        #mse = (delta**2).sum()/delta.shape[0]\n",
        "        return [self.beta, self.decay, self.p, self.s], [loss, pred]\n",
        "    \n",
        "    \n",
        "#表示桁数の設定\n",
        "np.set_printoptions(suppress=False, formatter={'float': '{:4.2f}'.format})\n",
        "#np.set_printoptions(suppress=False, formatter={'float': '{:4.2f}'.format})\n",
        "\n",
        "teacher = np.copy(Dells_controls)\n",
        "#teacher = np.array([0.82, 0.04, 0.02, 0.09, 0.01, 0.01])  # L.B\n",
        "#f = Dell2021(Weights='raw', teacher=teacher, d=0.5, iter_max=10 ** 4, lr=25, IFG_beta=8, f=tanh, verbose=True) # f=ReLU)\n",
        "model = Dell2021(Weights='raw', teacher=teacher, d=0.5, iter_max=10 ** 4, lr=1, IFG_beta=8, f=tanh, verbose=False) # f=ReLU)\n",
        "\n",
        "#data = np.array([8.359,2.218,3.285,5.258,0.193,3.504])\n",
        "#data = np.array([20.840,9.077,6.209,14.118,0.833,6.792])\n",
        "[beta, decay, s, p], [loss, pred] = model.fit(teacher=teacher, verbose=False)\n",
        "\n",
        "draw_dell_graph(teacher, pred, \n",
        "                alabel=\"Dell(1997)健常者のデータ\", acolor='blue',\n",
        "                blabel=\"2021 年型 Dell 最新モデル\", bcolor='green')\n",
        "\n",
        "print('          ', list(model.phonology.keys()))\n",
        "print('teacher:  ', teacher)\n",
        "print('pred:     ', pred)\n",
        "print('beta:{0:.3f} decay:{1:.3f} s:{2:.3f} p:{3:.3f}'.format(1/beta, decay, s, p))\n"
      ],
      "id": "b000dd43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af8ccc15-ea55-4308-b566-10bfe5f26a48"
      },
      "source": [
        "print('#Dell のデータ, 実データ, wd model, sp model, Foygell and Dell (2000) Table 2 and 4 を参照のこと')\n",
        "for patient in FoygellDell_tab2_and_4:\n",
        "    data_n, data_n_wd, data_n_sp = 0, 1, 2\n",
        "    data = np.array(FoygellDell_tab2_and_4[patient][data_n][:6])\n",
        "    #model = Dell2021(Weights='raw', teacher=data, d=0.5, iter_max=10 ** 4, lr=1, IFG_beta=8, f=tanh, verbose=False)\n",
        "    model = Dell2021(Weights='raw', teacher=data, d=0.5, iter_max=10 ** 4, lr=0.5, IFG_beta=8, f=tanh, verbose=False)\n",
        "    [beta, decay, s, p], [loss, pred] = model.fit(teacher=data, verbose=False)\n",
        "\n",
        "    #alabel=\n",
        "    draw_dell_graph(data, pred, title='Foygell and Dell (2000)', \n",
        "                    alabel='patient:{0} From Foygell & Dell(2000) Tab. 2'.format(patient), acolor='black',\n",
        "                    blabel='2021年型 新型Dellモデルによる推定値', bcolor='gray',\n",
        "                    width_inches=6, height_inches=3, fontsize=12)\n"
      ],
      "id": "af8ccc15-ea55-4308-b566-10bfe5f26a48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6958c470"
      },
      "source": [
        "#import gensim.downloader as api\n",
        "#glove_en = api.load('word2vec-google-news-300', return_path=True)\n",
        "\n",
        "#DellX = np.zeros((len(lexicons),len(glove_en['cat'])))\n",
        "#for i, word in enumerate(lexicon):\n",
        "#    DellX[i] = np.copy(glove_en[word])\n",
        "\n",
        "#Dell_wordR = np.corrcoef(DellX)\n",
        "#np.savetxt('Dell_wordR.txt',Dell_wordR)\n",
        "\n",
        "# Dell の Lexicon 間の相関係数行列。存在しない場合は，このセル上部のコメントをはずして実行\n",
        "Dell_LexR = np.array([\n",
        "    [1.000000000000000000e+00, 7.607610875238556281e-01, 1.526896477666510876e-01, \n",
        "     5.327596034338319964e-01, 7.083070865057317089e-02, -1.112107376916258127e-02],\n",
        "    [7.607610875238557391e-01, 1.000000000000000000e+00, 1.842294356793724996e-01, \n",
        "     4.393862362725595161e-01, 6.237472001379058828e-02, 3.078664862324366047e-03],\n",
        "    [1.526896477666510876e-01, 1.842294356793724719e-01, 1.000000000000000000e+00, \n",
        "     7.657631000394304888e-02, 8.018105209643067166e-02, 1.477240578677296579e-01],\n",
        "    [5.327596034338321074e-01, 4.393862362725595716e-01, 7.657631000394306275e-02, \n",
        "     9.999999999999998890e-01, 6.513470828101816656e-02, -6.153752081466464657e-02],\n",
        "    [7.083070865057317089e-02, 6.237472001379059522e-02, 8.018105209643067166e-02, \n",
        "     6.513470828101815269e-02, 1.000000000000000000e+00, 1.396624608082916268e-02],\n",
        "    [-1.112107376916258127e-02, 3.078664862324366480e-03, 1.477240578677296579e-01, \n",
        "     -6.153752081466464657e-02, 1.396624608082916268e-02, 9.999999999999997780e-01]])\n",
        "print(lexicon)\n",
        "print(Dell_LexR)"
      ],
      "id": "6958c470",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7338dcf"
      },
      "source": [
        ""
      ],
      "id": "c7338dcf",
      "execution_count": null,
      "outputs": []
    }
  ]
}