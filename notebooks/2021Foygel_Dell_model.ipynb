{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "2021Foygel_Dell_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2021Foygel_Dell_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39cjpqlKxQCA"
      },
      "source": [
        "# Simulationf for the weight-decay and sp models\n",
        "- date: 2020-1105\n",
        "- author: Shin Asakawa\n",
        "- Origin: Foygel and Dell (2000) Models of Impaired Lexical Access in Speech Production, \n",
        "- Journal of Memory and Language 43, 182–216 (2000) doi:10.1006/jmla.2000.2716\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/project-ccap/project-ccap.github.io/blob/master/figures/2000Foygel_Dell_fig1.png?raw=truen\" style=\"width:33%\"><br/>\n",
        "    <div align=\"left\" style=\"width:49%\">\n",
        "    Foygell and Dell (2000) Fig. 1. オリジナルは Dell (1997) 3 つの層の結合は双方向。\n",
        "    上から意味層, 語彙層, 音素層。最上層の意味層で 暗く塗りつぶされているニューロンは `cat`, `dog`, `rat` で共有されていることを示す。\n",
        "    </div>\n",
        "</center>\n",
        "\n",
        "Dell らのモデルには解析解が存在するのではないかという疑惑がある。\n",
        "Dell らと Roelofs, Levelt らも基本的な動作方程式は上式の通りである。\n",
        "\n",
        "$$\n",
        "x_{i,t+1} = (1-d) x_{i,t} + \\sum_j w x_{j,t} +\\mathcal{N}(x_i),\n",
        "$$\n",
        "\n",
        "$x_{i,t}$ は時刻 $t$ における $i$ 番目のニューロンの活性値，$d$ は崩壊率，$w$ は重み係数である。\n",
        "正規乱数であり $a_1=0.01, a_2=0.16$ をパラメータとする次式 $x_i\\sim \\mathcal{N}(0, a_1^2+a_2^2x_{i,t})$ \n",
        "で与えられる。\n",
        "Dell の初期の weight decay モデルでは $w$ と $d$ が系全体の挙動を定めるパラメータであった。\n",
        "\n",
        "<!--\n",
        "> ただし，Levelt, Roelofs らの WEAVER, WEAVER++, WEAVER++/ARC は決定論的動作方程式であるため，右辺最終稿の $\\text{noise}$ が存在しない。\n",
        "$1\\le d\\le0$ であるから，ノイズを無視すれば，\n",
        "$$\n",
        "x_{i,t} = e^{-dt},\n",
        "$$\n",
        "である。ノイズ項は乱数であるから，各乱数が i.i.d であると考えられるので，よく知られているとおり 2 つの確率変数 $x$ と $y$ の和の期待値と分散は\n",
        "$$\n",
        "E(X+Y) = E(X) + E(Y)\n",
        "$$\n",
        "$$\n",
        "V(X+Y) = V(X) + V(Y) + \\text{Cov}(X,Y)\n",
        "$$\n",
        "である。\n",
        "-->\n",
        "\n",
        "また $t=8$ のとき $x_{i,t=8}=10$ という，正解へのブースト (原著論文では `jolt`)がある。\n",
        "$n=8$ と $n=16$ で jolt (boost) する。\n",
        "\n",
        "- **レンマアクセス** \n",
        "猫の絵が提示されたとする。モデルの外にある視覚的プロセスが画像を識別し、猫の概念に対応する10個の意味的ノードにそれぞれ活性化の衝撃が与えられる。このときの衝撃の大きさは任意に100とし、10個のノードで分割すると、猫の意味ノードに10の衝撃が与えられることになる。\n",
        "ネットワーク内にフィードバックが存在するということは、レンマアクセスの間に、`mat`, `sat`, `can` などの `cat` の音韻的隣接単語ノードが活性化されることを意味する。\n",
        "これは `dog` のような意味的隣接語に加えて、共有された意味的なノードを介して活性化される。\n",
        "したがって、このステップで最も活性化される単語ノードは、ターゲット語とその意味的・形式的に関連する隣接語人である。\n",
        "レンマ・アクセスは、選択プロセスによって終了する。単語は、その中で最も活性化している単語ノードが選択される。\n",
        "\n",
        "- **音素学的アクセス**\n",
        "語彙検索の第二段階は、選択された単語のノードである `cat` に大きな活性化のブーストが与えられて開始される。\n",
        "ブースト値は意味レベルへの最初のブーストと同様100単位である。\n",
        "選択された単語へのこの大きなブーストは、非線形性が介入し、単語ノードが有用な隠れ層として機能することを可能にする。\n",
        "これにより、意味層から単語形式への写像が実現される。\n",
        "このブーストは、音韻的アクセスの開始時に、単語ノード `cat` をその競合単語のどれよりもはるかに活発にさせる。\n",
        "`cat` へのブースト後、活性化はさらに $n$ 時間ステップ拡散する。\n",
        "レンマアクセスの場合と同様、活性化は上にも下にも広がり `cat` に接続されているノード以外のノードが活性化することもある。\n",
        "しかし、音素アクセス時の拡散処理の目的は `cat` の音素を取得することである。\n",
        "$n$ 時間ステップを経て、最も活性化度の高い音素ノードが選択され、音韻フレームのスロットにリンクされる。\n",
        "音素フレームは単語の構造を表しており、その音節の数とストレスパターン、各音節内の子音と母音の系列を表している。\n",
        "本実装モデルにおける単純化では、単音節単語である，ｌ子音-母音-子音 (cvc) 単語のみを用いている。\n",
        "各音素ノードは，オンセット子音と，母音と，コーダ子音 でラベル付けされる。\n",
        "最も活性化度の高いオンセット，母音，コーダが選択され，フレーム内の対応するスロットに関連付けることで行われる。\n",
        "`cat` の場合、/k/-オンセット, /ae/-母音, /t/-コーダのノードが選択されやすい。\n",
        "モデルには 5 つのエラーカテゴリが存在する。\n",
        "このエラーカテゴリは、コード化されることがあるいくつかの影響を反映してはいない。\n",
        "例えば ターゲット単語の音素が誤って並べられる **言い間違いエラー**や、以前に話された単語やその音が対象語に持続的に入ってくる **保続エラー**である。\n",
        "このモデルでは、スロット位置への音素の関連性を仮定しているため、音節内の音の並び替えを許容していない。\n",
        "\n",
        "- **実装**\n",
        "モデルを 非失語症患者の絵画命名検査結果に適合させ、その後、失語症患者データに再適合させる。\n",
        "この実装のために、ネットワーク構造と拡散活性化パラメータの両方を指定しなければならない。\n",
        "実装戦略は 3 つのフェーズからなる。\n",
        "    - 第一段階: ネットワーク構造を特定し、モデルのネットワークが適用されるドメインである英語辞書の本質的な特徴を保持していることを確認する。\n",
        "    - 第2段階: フィラデルフィア命名テスト（PNT）で英語の非形容詞話者をテストし、結果として得られる誤差データに適合するようにモデルをパラメータ化する。\n",
        "    - 最終段階: パラメータ化されたモデルを患者データに適用する。\n",
        "誤りの頻度を推定するため，非単語の頻度，すなわち生成された音列が非単語である可能性を決定した。\n",
        "Dell and Reich (1981) と Best (1996) は、絵画命名研究と音声エラーから単語のセットを取り出し、各単語に含まれる 1 つの音素を別の音素で置き換えて、合法的な文字列を作成した。\n",
        "これらセットに含まれる非単語の割合は 約 0.55 から 0.80 の範囲であった。\n",
        "この場合、単語性とは大学レベルの辞書に掲載されていることを意味する。\n",
        "この手法は、実際の単語の音素学的に合致した隣接非単語である確率を決定する。\n",
        "しかし、この手法は実在する単語の隣接語を見ることになり、また、単語は音韻空間周辺に存在する傾向がある。\n",
        "このため，非単語の結果のエラーの可能性を過小評価する可能性がある。\n",
        "このため、非単語の誤差頻度の推定値として，最も保守的な値である $0.80$ を選択した。\n",
        "また PNT の対象語に対して置換法を用いたところ、辞書基準を用いて $0.74$ の非単語が推定された。\n",
        "このことから $0.80$ の値が妥当であることがわかった。\n",
        "\n",
        "- **健常データへのフィット**\n",
        "前述の近傍構造を用いて、統制群データに適合するモデルのパラメータ空間を探索した。\n",
        "選択したパラメータは，データに近い誤差確率をもたらした。表は、これらのパラメータと 100,000 回の試行に基づいてシミュレートされた誤差確率を示している。\n",
        "データとモデルの両方において、命名は非常に正確で、意味エラーが優勢であり、形式エラーは意味的な関係に関連してのみ存在していた。\n",
        "\n",
        "<center>\n",
        "Table 4: Nammg Data From 60 Control Participants and Simulated Probabilities\n",
        "\n",
        "|Data source| correct | semantic | Formal | Non word | Mixed | Unrelated |\n",
        "|-----------|---------|----------|--------|----------|-------|-----------|\n",
        "|Controls   | .969    |.012      | .001   | .000     | .009  |  .003     |\n",
        "|Simulated probabilities | .966 | .021 | .000 | .001 | .012  |  .000     |\n",
        "\n",
        "Chosen parameters: $w=0.01$, $d=0.5$, $\\text{SD1}=O.01$, $\\text{SD2}=0.16$; $n=8$.\n",
        "</center>\n",
        "\n",
        "\n",
        "<center>\n",
        "Foygel Dell (2000) table 1.<br/>\n",
        "\n",
        "|patient|correct|semantic|formal|nonword|mixed|unrelated|\n",
        "|:---|:---|:---|:---|:---|:---|:---|\n",
        "|L.H. |0.69|0.03|0.07|0.15|0.01|0.02|\n",
        "|DSMSG(weight=0.0057,decay=0.5)|0.69|0.07|0.06|0.14|0.01|0.03|\n",
        "|I.G. |0.69|0.09|0.05|0.02|0.03|0.01|\n",
        "|DSMSG(weight=0.1,decay=0.86)|0.73|0.13|0.04|0.05|0.04|0.01|\n",
        "</center>\n",
        "\n",
        "<center>\n",
        "Foygel Dell (2000) table 3.<br/>\n",
        "\n",
        "|patient|correct|semantic|formal|nonword|mixed|unrelated|\n",
        "|:---|:---|:---|:---|:---|:---|:---|\n",
        "|control |0.9690|0.0120|0.0010|0.0000|0.0090|0.0030|\n",
        "|sp model(s=0.0698, p=0.1000)|0.9722|0.0126|0.0011|0.0001|0.0138|0.0002|\n",
        "</center>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxGRqpvTxQCC"
      },
      "source": [
        "<!--\n",
        "> **Lemma access**. Assume that a picture of a cat is presented. Visual processes that are outside of the model identify the picture\n",
        "and the 10 semantic nodes corresponding to the cat concept are each given a jolt of activation. \n",
        "The size of the jolt was arbitrarily set at 100, which, when divided among 10 nodes, gives a jolt of 10 to each semantic node for cat.\n",
        "\n",
        "> The existence of feedback in the network means that during lemma access the word nodes of phonological neighbors of _cat_, such as _mat_, _sat_, or _can_, will become activated. \n",
        "This is in addition to semantic neighbors such as _dog_, which obtain activation from shared semantic nodes. \n",
        "Thus, the most activated word nodes at this step are the target and its semantic and formally related neighbors.\n",
        "\n",
        "> Lemma access is concluded by a selection process. \n",
        "The most highly activated word node of the proper syntactic category is selected. \n",
        "During the production of a sentence, selection entails the linkage of a word to a slot in a syntactic frame. \n",
        "Frame and slot approaches to grammatical encoding in production have ample empirical support (Bock & Loebell, 1990; Garrett, 1975; Levelt, 1989; see Bock & Levelt, 1994, for a review). \n",
        "In the case of object picture naming, we assume a degenerate frame consisting of a slot for a single noun. Therefore, in our implementation of the naming task, the most highly activated noun is selected.\n",
        "\n",
        "> **Phonological access**. The second step of lexical retrieval begins when the selected word node. _cat_, is given a large jolt of activation. \n",
        "This is also 100 units' worth, the same as the initial jolt to the semantic level. \n",
        "When a sentence is being produced, the jolt to a word occurs when the syntactic frame says that it should occur. \n",
        "In a single-word naming task. \n",
        "it occurs immediately on selection because there is only a single noun slot in the frame.[3] \n",
        "\n",
        "> This large jolt to the selected word is important because it introduces a nonlinearity and hence allows the word nodes to act as a useful hidden layer. \n",
        "This, in turn, allows the meaning-to-form mapping to be achieved. \n",
        "The jolt makes the word node _cat_ much more active than any of its competitors at the beginning of phonological access. \n",
        "This function of enhancing the \"winner\" is often carried out by lateral inhibition among competitors (e.g., Feldman & Ballard, 1982; Grossberg, 1982; Harley, 1990; McClelland & Rumelhart, 1981) or by an absolute threshold that, when crossed, boosts the activation of a node (e.g., MacKay, 1987). \n",
        "The jolt to the selected word in our model is similar to these mechanisms. \n",
        "However, we are specifically tying it to syntactic processes. \n",
        "The source of the jolt is the syntactic slot that the selected word is linked to (e.g., Berg, 1988; Dell & O'Seaghdlha, 1991; Eikmeyer & Schade, 1991; MacKay, 1982, 1987; Stemberger, 1985).\n",
        "\n",
        "> After the jolt to _cat_, activation spreads for $n$ more time steps. \n",
        "As was true for lemma access, adivation spreads both upward and downward, and nodes other than those connected to _cat_ can become activated. \n",
        "The goal of the spreading proccess during phonological access, however, is to retrieve the phonemes of _cat_. \n",
        "After the $n$ time steps, the most highly activated phoneme nodes are selected and linked to slots in a phonological frame, a process analogous to the linking of the selected word to a syntactic slot in lemma access. \n",
        "A phonological frame represents the structure of a word --- its number of syllables and their stress pattern and the sequence of consonants and vowels within each syllable. \n",
        "Most current theories of production hypothesize that phonological access consists of the retrieval of phoneme-size units and their insertion into frame slots, although there are differences among theorists with respect to the nature or the frame (for reviews, see Levelt. 1992; Meyer & Bock, 1992; Shattack-Hufnagel, 1992). \n",
        "The evidence for phonological frames comes from speech errors (e.g., Shattuck-Hufnagel, 1979; Sternberger, 1990) and experimental studies showing that frame structures can be primed (Meijer, 1994; Romani, 1992; Sevald, Dell, & Cole, 1995).\n",
        "\n",
        "> One simplification of the implemented model is that it only has a frame for single-syllable consonant-vowel-consonant (cvc) words. \n",
        "Each phoneme node is labeled according to whether it is an onset consonant, a vowel, or a coda consonant.\n",
        "Selection consists of picking the most highly activated onset, vowel, and coda and associating them with the corresponding slots in the frame. \n",
        "In the case of _cat_, the nodes for /k/-onset, /ae/-vowel, and /t/-coda would likely be seleted. \n",
        "Thus, phonological selection is guided by categoreies, such as onset or vowel, in the same way that lemma selection is guided by syntactic categories. \n",
        "Categorical selection in phonological access is suggested by patterns of sound substitutions in phonological speech\n",
        "errors (e.g., MacKay, 1970, 1972; Shattuck-Hufnagel, 1979).\n",
        "\n",
        "\n",
        "> Although the model's five error categories can code most of the error responses reported in the literature, the categories do not reflect some influences that are sometimes coded. \n",
        "These influences are observed in errors in which the phonemes of the target word are misordered and errors in which previously spoken  words or their sounds perseverate into the target. \n",
        "Because of the model's assumptions about the association of phonemes to slot positions, it does not allow for the misordering of sounds within a syllable, such as cat spoken as _act_ or _tack_. \n",
        "This is probably correct for normal speech errors (e.g., Dell, 1986), but not for aphasic speakers, who, as noted earlier, do exhibit misordering within the syllable or word. \n",
        "In addition, because the model assumes that each naming attempt is independent of other attempts, perseverative effects do not occur.\n",
        "In principle, a spreading activation model can account for such effects through persistence of activation or connection weight changes (Plaul & Shallice, 1993a), but we did not implement such a mechanism.\n",
        "\n",
        "保続についても言及はしているな\n",
        "\n",
        "> **Implementation**. \n",
        "Our goal is to fit the model to nonaphasic picture-naming data and then lesion it to fit patient data. \n",
        "For such an implementation, both the network structures and the spreading activation parameters have to be specified. \n",
        "Our strategy for implementation has three phases. \n",
        "In the first phase, we specify the network structure, making sure that the model's network preserves essential features of the domain to which it is applied, the English lexicon. \n",
        "The second phase consists of testing nonaphasic speakers of English on the Philadelphia Naming Test (PNT) and then parameterizing the model so that it fits the resulting error data. \n",
        "The chosen parameters also will have to be consistent with facts about the time course of picture naming, specifically that the activation pattern initially includes semantic, but not phonological, neighbors of the target and then later includes phonological, but not semantic, neighbors (Peterson & Savoy, in press; Schriefers et al., 1990). \n",
        "The final phase of modeling involves applying the parameterized model to patient data. \n",
        "\n",
        "一段落省略\n",
        "\n",
        "> To estimate the error opportunities, we first determined the opportunities for non words (i.e., the likelihood that a legal string is a nonword). \n",
        "Dell and Reich (1981) and Best (1996) took sets of words from picture-naming studies and speech error collections and replaced a single phoneme in each word with another phoneme creating a legal string. \n",
        "The proportion of nonwords in these sets ranged from around .55 to .80, in which wordhood meant being listed in a college-level dictionary. \n",
        "This technique essentially determines the chance that phonologically legal neighbors of real words are nonwords. \n",
        "However, because it involves looking at the neighbors of real words, and words tend to clump in phonological space, it may underestimate the error opportunities for nonword outcomes. \n",
        "For this reason, we chose the most conservative value, $0.80$, as the estimated error opportunity for nonwords. \n",
        "We also used the substitution technique on the target words for the PNT and found an estimate of $0.74$ nonwords using the dictionary criterion, which made us feel reasonably comfortable with the chosen $0.80$ value.\n",
        "\n",
        "> #### Model Fit to Control Data \n",
        "> Using the neighborhood structure described earlier, we explored the parameter space of the model to fit the control data. \n",
        "The chosen parameters led to error probabilities reasonably close to those in the data. \n",
        "Table 4 shows these parameters and the simulated error probabilities, which were based on 100,000 trials. \n",
        "In both the data and the model, naming was highly accurate, semantically related errors predominated, and formal relations were present only in conjunction with a semantic relation. \n",
        "\n",
        "Table 4: Nammg Data From 60 Control Participants and Simulated Probabilities\n",
        "\n",
        "|Data source| correct | semantic | Formal | Non word | Mixed | Unrelated |\n",
        "|-----------|---------|----------|--------|----------|-------|-----------|\n",
        "|Controls   | .969    |.012      | .001   | .000     | .009  |  .003     |\n",
        "|Simulated probabilities | .966 | .021 | .000 | .001 | .012  |  .000     |\n",
        "\n",
        "Chosen parameters: $w=0.01$, $d=0.5$, $SD1=O.01$, $SD2=0.16$; $n=8$.\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-mQVrbjxQCF"
      },
      "source": [
        "# 0. 初期設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tosc-DqRxXe5"
      },
      "source": [
        "!pip install japanize_matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YphA_C8-xQCI"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from numpy.random import Generator, PCG64\n",
        "#rng = Generator(PCG64())\n",
        "#rng.standard_normal()\n",
        "\n",
        "# 表示精度桁数の設定\n",
        "np.set_printoptions(suppress=False, formatter={'float': '{:7.4f}'.format})\n",
        "np.set_printoptions(suppress=False, formatter={'float': '{:6.3f}'.format})\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# 事前に `!pip install japanize_matplotlib` が必要\n",
        "import japanize_matplotlib"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuQdJaIhxQCI"
      },
      "source": [
        "fig = plt.figure(figsize=(8,5))  # 横と縦のサイズ，単位はインチ\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "X = np.arange(6)\n",
        "\n",
        "plt.title('Dell(1997) Tab. 4 より作成:')\n",
        "# Dell(1997)による健常者のデータ\n",
        "Dells_controls = np.array([0.9690, 0.0120, 0.0010, 0.0090, 0.0030, 0.0000])  \n",
        "\n",
        "# Dell(1997)による WD モデルのデータ\n",
        "Dells_WD = np.array([0.9660, 0.0210, 0.0000, 0.0120, 0.0000, 0.0010])  \n",
        "\n",
        "# Foygel & Dell(2000)による SP モデルのデータ, Tab. 3 Foygel and Dell (2000)\n",
        "Dells_SP = np.array([0.9722, 0.0126, 0.0011, 0.0138, 0.0002, 0.0001])  \n",
        "\n",
        "# 横軸のラベル名\n",
        "ax.set_xticklabels(['', 'correct', 'semantic','formal','mixed','unrelated', 'nonword'])\n",
        "\n",
        "ax.bar(X-0.2, Dells_controls, color='g', width=0.4, label='健常統制群')\n",
        "ax.bar(X+0.2, Dells_WD, color='r', width=0.4, label='Dell(1997) WD モデル')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m1xNYXqxQCI"
      },
      "source": [
        "# 1. 必要となるパラメータの宣言"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bFyAufMxQCI"
      },
      "source": [
        "# Weight-Decay モデルのパラメータ, 値は Dell らの原著論文に記載されている値を採用\n",
        "Weight = 0.1  # for normal\n",
        "Decay  = 0.5  # for normal\n",
        "\n",
        "# S-P モデルのパラメータ\n",
        "S_weight = 0.0698\n",
        "P_weight = 0.1000\n",
        "SP_decay = 0.6\n",
        "\n",
        "Model = 'WD'\n",
        "# Model = 'SP'\n",
        "\n",
        "# ノイズパラメータ。SD1 は定常雑音，SD2 は x の大きさに依存した雑音 x ~ N(0,SD1^2+SD2^2 x)\n",
        "SD1 = 0.01\n",
        "SD2 = 0.16\n",
        "\n",
        "# 外部入力を表す値，ブーストを起こす時刻\n",
        "Sem_jolt = 10\n",
        "Lex_jolt = 100\n",
        "\n",
        "# Dell らのモデルは 3 層の相互活性化モデルを踏襲している\n",
        "# 意味層 <---> 語彙層 <---> 音韻層\n",
        "# 3 層の 2 ステップ相互活性化モデルのニューロン数\n",
        "n_sem = 54 # nubmer of semantic neurons\n",
        "n_lex = 6  # number of lexical neurons\n",
        "n_phon = 9 # number of phonological neurons\n",
        "\n",
        "# 3 つの層を定義\n",
        "Sem = np.zeros([n_sem,], dtype=np.float)   # 意味層\n",
        "Lex = np.zeros([n_lex,], dtype=np.float)   # 語彙層\n",
        "Pho = np.zeros([n_phon,], dtype=np.float)  # 音韻層\n",
        "\n",
        "Ws = np.zeros((n_sem, n_lex))\n",
        "Ws= np.array([[1,1,1,1,1,1,1,1,1,1, \n",
        "               0,0,0,0,0,0,0,0,0,0, \n",
        "               0,0,0,0,0,0,0,0,0,0, \n",
        "               0,0,0,0,0,0,0,0,0,0,\n",
        "               0,0,0,0,0,0,0,0,0,0, 0,0,0,0],  # cat 正解\n",
        "              [0,0,0,0,0,0,0,1,1,1, \n",
        "               1,1,1,1,1,1,1,0,0,0, \n",
        "               0,0,0,0,0,0,0,0,0,0, \n",
        "               0,0,0,0,0,0,0,0,0,0, \n",
        "               0,0,0,0,0,0,0,0,0,0, 0,0,0,0],  # dog 意味エラー\n",
        "              [0,0,0,0,0,0,0,0,0,0, \n",
        "               0,0,0,0,0,0,0,1,1,1, \n",
        "               1,1,1,1,1,1,1,0,0,0, \n",
        "               0,0,0,0,0,0,0,0,0,0,\n",
        "               0,0,0,0,0,0,0,0,0,0, 0,0,0,0],  # mat 形態エラー\n",
        "              [0,0,0,0,0,0,0,1,1,1, \n",
        "               0,0,0,0,0,0,0,0,0,0, \n",
        "               0,0,0,0,0,0,0,1,1,1, \n",
        "               1,1,1,1,0,0,0,0,0,0,\n",
        "               0,0,0,0,0,0,0,0,0,0, 0,0,0,0],  # rat 混合エラー\n",
        "              [0,0,0,0,0,0,0,0,0,0, \n",
        "               0,0,0,0,0,0,0,0,0,0, \n",
        "               0,0,0,0,0,0,0,0,0,0, \n",
        "               0,0,0,0,1,1,1,1,1,1,\n",
        "               1,1,1,1,0,0,0,0,0,0, 0,0,0,0],  # fog 無関連エラー\n",
        "              [0,0,0,0,0,0,0,0,0,0, \n",
        "               0,0,0,0,0,0,0,0,0,0, \n",
        "               0,0,0,0,0,0,0,0,0,0, \n",
        "               0,0,0,0,0,0,0,0,0,0,\n",
        "               0,0,0,0,1,1,1,1,1,1, 1,1,1,1]  # lat 非単語\n",
        "             ])\n",
        "#Ws = Ws.T\n",
        "\n",
        "# 語彙の定義\n",
        "# cat(correct), dog(semantic), mat(formal), rat(mixed), log(unrelated), lat(nonword)\n",
        "words = ['cat', 'dog', 'mat', 'rat', 'fog', 'lat']\n",
        "\n",
        "# 音素の定義\n",
        "phonemes = {'onset':['f', 'r', 'd', 'k', 'm'],\n",
        "            'vowel':['ae', 'o'],\n",
        "            'coda':['t','g']}\n",
        "Onset, Vowel, Coda = slice(0,5), slice(5,7), slice(7,9)\n",
        "\n",
        "# 語彙層と音韻層とを結ぶ結合係数行列の定義\n",
        "#                               f.   r.   d.   k.   m.   ae.  o.   t.   g\n",
        "phonology = {'cat': np.array([ 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]),  # correct\n",
        "             'dog': np.array([ 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]),  # semantic error\n",
        "             'mat': np.array([ 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]),  # formal error\n",
        "             'rat': np.array([ 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]),  # mixed error\n",
        "             'fog': np.array([ 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]),  # unrelated error\n",
        "             'lat': np.array([ 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0])}  # nonword\n",
        "# phonology は BOW 形式なんですなー"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeJRoo1HxQCJ"
      },
      "source": [
        "class Dell_model(object):\n",
        "    def __init__(self, Model='WD', Ws=None, Wp=None, param=None, seed=None):\n",
        "        \n",
        "        if not seed:\n",
        "            self.seed = int(time.time())\n",
        "        else:\n",
        "            self.seed = seed\n",
        "        self.rng = Generator(PCG64(self.seed))\n",
        "        \n",
        "        self.sem_jolt = 10\n",
        "        self.lex_jolt = 100\n",
        "        self.jolt_t = 7  # カウントが 0 ベースなので t=7 (8-1) が 8 回目\n",
        "        \n",
        "        # ノイズパラメータ。SD1 は定常雑音，SD2 は x の大きさに依存した雑音 x ~ N(0,SD1^2+SD2^2 x)\n",
        "        self.SD1 = 0.01\n",
        "        self.SD2 = 0.16\n",
        "        \n",
        "        if param == None:\n",
        "            self.param = {'WD':{'Weight':0.1,  # for normal\n",
        "                                'Decay':0.5    # for normal\n",
        "                               },\n",
        "                          'SP':{'Decay': 0.6,\n",
        "                                'S_Weight':0.0698,\n",
        "                                'P_Weight':0.1000\n",
        "                               }}\n",
        "        else:\n",
        "            self.param = param\n",
        "\n",
        "        \n",
        "        self.n_sem = 54 # nubmer of semantic neurons\n",
        "        self. n_lex = 6  # number of lexical neurons\n",
        "        self.n_phon = 9 # number of phonological neurons\n",
        "\n",
        "        # 3 つの層を定義\n",
        "        self.Sem = np.zeros([n_sem,], dtype=np.float)   # 意味層\n",
        "        self.Lex = np.zeros([n_lex,], dtype=np.float)   # 語彙層\n",
        "        self.Pho = np.zeros([n_phon,], dtype=np.float)  # 音韻層\n",
        "\n",
        "        if Ws == None:\n",
        "            self.Ws = self.init_Ws()\n",
        "        else:\n",
        "            self.Ws = Ws\n",
        "            \n",
        "        if Wp == None:\n",
        "            self.Wp = self.init_Wp()\n",
        "        else:\n",
        "            self.Wp = Wp\n",
        "\n",
        "        if Model == 'WD':\n",
        "            self.s_weight = self.param['WD']['Weight']\n",
        "            self.p_weight = self.param['WD']['Weight']\n",
        "            self.decay = self.param['WD']['Decay']\n",
        "        else:\n",
        "            self.s_weight = self.param['SP']['Weight']\n",
        "            self.p_weight = self.param['SP']['Weight']\n",
        "            self.decay = self.param['SP']['Decay']\n",
        "        self.Ws *= self.s_weight\n",
        "        self.Wp *= self.p_weight\n",
        "        return\n",
        "\n",
        "            \n",
        "    def init_Ws(self):\n",
        "        return np.array([[1,1,1,1,1,1,1,1,1,1, 0,0,0,0,0,0,0,0,0,0, \n",
        "                          0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,\n",
        "                          0,0,0,0,0,0,0,0,0,0, 0,0,0,0],             # cat 正解\n",
        "                         [0,0,0,0,0,0,0,1,1,1, 1,1,1,1,1,1,1,0,0,0, \n",
        "                          0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0, \n",
        "                          0,0,0,0,0,0,0,0,0,0, 0,0,0,0],             # dog 意味エラー\n",
        "                         [0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,1,1,1, \n",
        "                          1,1,1,1,1,1,1,0,0,0, 0,0,0,0,0,0,0,0,0,0,\n",
        "                          0,0,0,0,0,0,0,0,0,0, 0,0,0,0],             # mat 形態エラー\n",
        "                         [0,0,0,0,0,0,0,1,1,1, 0,0,0,0,0,0,0,0,0,0, \n",
        "                          0,0,0,0,0,0,0,1,1,1, 1,1,1,1,0,0,0,0,0,0,\n",
        "                          0,0,0,0,0,0,0,0,0,0, 0,0,0,0],             # rat 混合エラー\n",
        "                         [0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0, \n",
        "                          0,0,0,0,0,0,0,0,0,0, 0,0,0,0,1,1,1,1,1,1,\n",
        "                          1,1,1,1,0,0,0,0,0,0, 0,0,0,0],             # fog 無関連エラー\n",
        "                         [0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0, \n",
        "                          0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,\n",
        "                          0,0,0,0,1,1,1,1,1,1, 1,1,1,1]              # lat 非単語\n",
        "                        ], dtype=np.float)\n",
        "\n",
        "    def init_Wp(self):\n",
        "        # 語彙層と音韻層とを結ぶ結合係数行列の定義\n",
        "        #                                     f.   r.   d.   k.   m.   ae.  o.   t.   g\n",
        "        self.phonology = {'cat': np.array([ 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]),  # correct\n",
        "                          'dog': np.array([ 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]),  # semantic error\n",
        "                          'mat': np.array([ 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]),  # formal error\n",
        "                          'rat': np.array([ 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]),  # mixed error\n",
        "                          'fog': np.array([ 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]),  # unrelated error\n",
        "                          'lat': np.array([ 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0])}  # nonword\n",
        "        # phonology は BOW 形式なんですなー\n",
        "        Wp = np.zeros((n_lex, n_phon), dtype=np.float)\n",
        "        for i, x in enumerate(phonology.values()):\n",
        "            Wp[i] = np.copy(x)\n",
        "        return Wp\n",
        "\n",
        "        \n",
        "    def initialize(self):\n",
        "        return init_Wp(), init_Ws()\n",
        "\n",
        "        \n",
        "    def update(self, Sem, Lex, Pho):\n",
        "        \"\"\"3 層を更新式を用いて更新する\"\"\"\n",
        "        Sem_next = Sem * (1. - self.decay) + np.matmul(self.Ws.T, Lex)\n",
        "        Lex_next = Lex * (1. - self.decay) + np.matmul(self.Ws, Sem) + np.matmul(self.Wp, Pho)\n",
        "        Pho_next = Pho * (1. - self.decay) + np.matmul(self.Wp.T, Lex)\n",
        "    \n",
        "        # ノイズの付加\n",
        "        for x in [Sem_next, Lex_next, Pho_next]:\n",
        "            # SD1 ノイズの付加\n",
        "            x +=  self.rng.standard_normal(size=len(x)) * self.SD1**2\n",
        "            for i, xx in enumerate(x):\n",
        "                # SD2 ノイズの付加と ReLU\n",
        "                xx += xx * self.rng.standard_normal() * self.SD2**2\n",
        "                x[i] = max(0, xx)\n",
        "        return Sem_next, Lex_next, Pho_next\n",
        "\n",
        "    \n",
        "    def one_epoch(self, verbose=False):\n",
        "        n=16\n",
        "        # sem_jolt=Sem_jolt, lex_jolt=Lex_jolt, jolt_t=7, verbose=False):\n",
        "    \n",
        "        # 各層を 0 で初期化\n",
        "        self.Sem.fill(0); self.Lex.fill(0); self.Pho.fill(0)\n",
        "\n",
        "        # 意味層での語彙表現は 10 ニューロンが 1, 他が全て 0 であることを表す全要素が 1 のベクトル\n",
        "        input_vec = np.ones((1,10), dtype=np.float)\n",
        "    \n",
        "        self.Sem[:10] = np.copy(input_vec) * self.sem_jolt  # ターゲットである `cat` を意味層にセット\n",
        "        #Sem[7:17] = np.copy(input_vec) * sem_jolt # dog にするならこの行を生かして直上行をコメントアウト\n",
        "\n",
        "        # 更新は 16 回。原著論文では n と記載されている\n",
        "        # t=8 のときブースト (jolt) を起こす\n",
        "        self.Ws, self.Wp = self.init_Ws() * self.s_weight, self.init_Wp() * self.p_weight\n",
        "        for tau in range(n):\n",
        "            if tau == self.jolt_t:\n",
        "                self.Lex[np.argmax(Lex)] = self.lex_jolt\n",
        "            if verbose:\n",
        "                print('tau=', tau+1, self.Lex)\n",
        "            self.Sem, self.Lex, self.Pho = self.update(self.Sem, self.Lex, self.Pho)\n",
        "\n",
        "        # 最も良く当てはまっている出力を検索\n",
        "        min_v = np.finfo(self.Pho.dtype).max\n",
        "        min_n = 'NG' # len(words) + 1\n",
        "        dum = np.zeros_like(self.Pho)\n",
        "        dum[np.argmax(Pho[Onset])] = 1.\n",
        "        dum[np.argmax(Pho[Vowel])+5] = 1.\n",
        "        dum[np.argmax(Pho[Coda])+5+2] = 1.\n",
        "        if verbose:\n",
        "            print('dum', dum)\n",
        "            print('Pho', self.Pho)\n",
        "        for k, v in self.phonology.items():\n",
        "            _x = np.sum((v - self.Pho)**2)\n",
        "            #_x = np.sum((v - dum)**2)\n",
        "            if _x < min_v:\n",
        "                min_n = k\n",
        "                min_v = _x\n",
        "        if min_n != 'cat' and verbose:\n",
        "            print('{2:03d} output:{0}, Least Mean Square:{1:8.5f}'.format(min_n, min_v, i), end=\", \")\n",
        "            print('Pho', Pho)\n",
        "\n",
        "        return min_n, min_v\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iYPNwRUxQCJ"
      },
      "source": [
        "# 確認用\n",
        "# フィラデルフィア絵画命名検査 PNT の図版総数が 175 枚であるため，この回数だけ繰り返す\n",
        "nPNT = 175\n",
        "#nPNT = 2000\n",
        "#nPNT = 1  # テスト用\n",
        "\n",
        "# 実際のシミュレーションの実施\n",
        "model = Dell_model()\n",
        "results = np.zeros((nPNT, len(words)), dtype=np.float)\n",
        "for i in range(nPNT):\n",
        "    word, val = model.one_epoch(verbose=False)\n",
        "    results[i, words.index(word)] = 1."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncBjcXxaxQCJ"
      },
      "source": [
        "# シミュレーション結果の印字\n",
        "print('Response         : [ Corr.  Sem.   For.   Mixed  Unrel. Nonword]')\n",
        "print('Examples         :', words)\n",
        "print('This result      :', np.mean(results, axis=0))\n",
        "print('Dell1997 Tab4(WD):', Dells_WD)\n",
        "print('F&D2000 Tab3(SP) :', Dells_SP)\n",
        "print('Control          :', Dells_controls)\n",
        "print('Parameters       : weight={0}, Decay={1}'.format(Weight, Decay))\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQEFgHKmxQCK"
      },
      "source": [
        "# 結果の表示\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "fig = plt.figure(figsize=(8,5))\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "X = np.arange(6)\n",
        "\n",
        "#ax.set_xticklabels(['', 'correct', 'semantic','formal','mixed','unrelated', 'nonword'])\n",
        "ax.set_xticklabels(['', '正解', '意味エラー','形態エラー','混合エラー','無関連エラー', '非単語エラー'])\n",
        "ax.bar(X-0.2, Dells_controls, color='g', width=0.4, label='健常統制群データ(Dell,1997)')\n",
        "ax.bar(X+0.2, np.mean(results,axis=0), color='r', width=0.4, label='シミュレーション結果')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mnJt_XfxQCK"
      },
      "source": [
        "#### The Semantic-Phonological Map\n",
        "The semantic-phonological model associates patients with different values of $s$ and $f$. \n",
        "As a preliminary to constructing the map of the error patterns associated with variation in $s$ and $f$, we first sought model parameters that would fit the error data from DSMSG’s normal control speakers, allowing ourselves the freedom to examine different values of $s$ and $f$ in the context of other parameters. \n",
        "Table 3 shows the control data, the model’s fit, and the chosen values of $s$, $f$, and other parameters. \n",
        "The model, like the normal speakers, produces correct responses on 97% of the trials, and the few errors that occur are largely confined to the semantic and mixed categories.\n",
        "Notice that we use a slightly higher decay rate ($q=0.6$) than DSMSG did ($q=0.5$). \n",
        "The semantic-phonological map was constructed using the same algorithm as in the previous section (see the Appendix). \n",
        "To enable the closest possible comparison with the weight-decay model, $s$ and $f$ were varied between $0.001$ and $0.1$, and the map was increased to similar size (3662 mapped points).\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "Table 3. Error distribution $s=0.0698$, $f=0.1000$<br/>\n",
        "\n",
        "|         |Correct |Semantic| Formal| Mixed |Unrelated|Nonword|\n",
        "|:-------:|-------:|-------:|------:|------:|--------:|------:|\n",
        "|  Control| 0.9690 |0.0120  | 0.0010|0.0090 |0.0030   |0.0000 |\n",
        "|  S-F    | 0.9722 |0.0126  | 0.0011|0.0138 |0.0002   |0.0001 |   \n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kifpKo28xQCK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}