{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/2022notebooks/2022_0219tlpa_enc_dec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WwW0sXCSugA"
      },
      "source": [
        "# TLPA 絵画命名検査をデータとした enc-dec モデル CCAP バージョン\n",
        "\n",
        "- date: 2022-0219\n",
        "- filename: 2022_0219tlpa_enc_dec.ipynb\n",
        "- file required: 呼称(TLPA)語彙属性.xlsx\n",
        "- author: 浅川伸一\n",
        "- license: MIT\n",
        "\n",
        "## 手続き\n",
        "\n",
        "\n",
        "### 材料\n",
        "\n",
        "TLPA の絵画命名課題に用いられる図版 200 枚を単語にして刺激語として用いた。\n",
        "TLPA の本課題は，入力が白黒画像であるが，これを単語として表現した場合，複数の表記が考えられる。\n",
        "たとえば，「海苔巻き」と「ノリ巻き」とであれば，どちらでも刺激図版を表す単語として用いることができる。\n",
        "今回は，日本語ウィキペディア 2021 年 5 月版に記載のあった単語を用いることとした。\n",
        "このため，200 枚の図版に対して，237 語の刺激語を用いた。\n",
        "一方，NTT 日本語語彙特性 (天野，近藤, 1999) の頻度に基づいて，最頻語を 10,000 語用いて訓練データとした。\n",
        "NTT 日本語語彙特性に基づく単語頻度のうち，記号やアルファベットを除き，かつ，上述の TLPA 単語 237 語を除外した\n",
        "上位 10,000 語を訓練データとした。\n",
        "\n",
        "訓練においては，各入力単語は，文字別に付番した番号を文字トークン ID とした。\n",
        "この文字トークン ID を ワンホットベクトルとみなし，入力層に与えた。\n",
        "出力の音素表現については，単語の読みを mecab (Kudo, 2007) によって取得した。\n",
        "得られた単語の読みを julius 表記によってアルファベット文字列に変換した情報をワンホットベクトルとみなして出力表現とした。\n",
        "このようにして得られた，単語の音表現は 41 種 と特殊トークン 4 種の計 45 種 であった。\n",
        "特殊トークン 4 種とは，入力表現でも用いられた，次の 4 種である。\n",
        "\n",
        "* '\\<EOW\\>': 単語の終端を表す特殊トークン\n",
        "* '\\<SOW\\>': 単語の先端を表す特殊トークン\n",
        "* '\\<UNK\\>': 未知記号を表す特殊トークン\n",
        "* '\\<PAD\\>': 埋め草用特殊トークン，ミニバッチ学習の際に入出力表現ベクトルの次元を揃えるために用いられる\n",
        "    \n",
        "このようにして得られた入力用文字トークンは，1903 種，音トークンは先述のとおり 45 種とした。\n",
        "入力文字トークン系列の最大系列長は 12, 出力音トークン系列の最大系列長は，28 であった。\n",
        "\n",
        "たとえば，単語「バス」についての入出力表現は，以下の通りである:\n",
        "\n",
        "```\n",
        "{0: {'orig': 'バス', 'ortho': ['バ', 'ス'], 'phone': ['b', 'a', 's', 'u'], 'ortho_ids': [696, 519], 'phone_ids': [25, 7, 19, 12]\n",
        "```\n",
        "\n",
        "### 訓練手続き\n",
        "\n",
        "材料の項で述べた単語の書記素表現系列から音表現系列への変換を，符号化器=復号化器モデル，あるいは seq2seq モデル，(Sutskever et.al, 2014, arxiv:1409.3215) \n",
        "を用いて学習させた。\n",
        "中間層のニューロン数は 64 または 256 とした。\n",
        "符号化器=復号化器モデルに用いた理関連とニューラルネットワークとして GRU (Cho, et.al, 2014) を用いた。\n",
        "学習の評価には，負の対数尤度を用いた。\n",
        "学習は Adam (Kingma and Welling, 2015, arxiv:1412.6980) を用いて訓練した。\n",
        "このとき学習係数は 0.01，alpha, beta の値は PyTorch の既定値を用いた。\n",
        "各学習エポックでは，10,000 語のデータが用いれ，TLPA 語は学習には用いず，検証データして用いた。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 補足\n",
        "- 用いた TLPA 単語 (237 語)\n",
        "```\n",
        "'バス', '緑', '桜', 'のり巻き', '海苔巻', '五重塔', 'コップ', 'ごぼう', '土踏まず', '風呂', 'ヒトデ', 'ハム', 'うさぎ', '兎', 'ウサギ', 'ロープウェイ', '学校', 'ちりとり', '縁側', '歯', 'ねぎ', 'あじさい', '紫陽花', '灰色', '天井', '鍵', '肌色', 'ワニ', '鰐', '電車', '顔', '松', 'ガードレール', '柿', 'ちまき', '信号', 'ススキ', '薄', 'じょうろ', 'ジョウロ', 'コンセント', '天ぷら', 'てんぷら', '中指', 'ヨット', 'ピンク', 'フクロウ', 'ふくろう', 'みかん', '蜜柑', 'ミカン', '柱', '角砂糖', '犬', 'かご', '駕籠', 'バラ', '薔薇', '鍋', 'まぶた', 'くるみ', '黒', 'デパート', 'カーネーション', '城', 'アリ', '豆腐', 'ドライバー', '紺', '階段', '戦車', '人参', '背中', '鏡餅', 'スプーン', '朝顔', '金色', '足', 'ふすま', 'へび', '蛇', 'ヘビ', 'レモン', '公園', '乳母車', '床', '藤', 'ピンセット', 'トラック', 'いちご', '苺', 'イチゴ', '黄土色', '銭湯', 'ナマズ', 'ソバ', '蕎麦', 'おなか', 'お腹', 'オレンジ', 'バター', '工場', 'ハト', '鳩', '電卓', 'のど仏', '喉仏', 'チューリップ', '白菜', 'トラクター', '廊下', 'パトカー', '押し入れ', '鉛筆', '目尻', '芋', '吊橋', '赤', 'かき氷', '豹', 'サボテン', 'ピラミッド', 'サイ', '目', 'ひまわり', 'はたき', 'さしみ', '刺身', '玄関', 'トマト', '黄緑', '三輪車', 'にわとり', '鶏', 'つむじ', 'アスパラガス', 'ドア', '銀色', 'ウイスキー', '梅', 'タクシー', '動物園', '床の間', 'こげ茶', 'ぶどう', '葡萄', 'ブドウ', '飴', '毛虫', 'アイロン', '寺', 'そり', 'ひょうたん', '首', '消しゴム', '頬', 'イチョウ', 'いちょう', '駅', '餃子', '牛', 'びわ', '枇杷', '飛行機', '畳', '白', '竹', 'ペリカン', '紫', '手すり', '口', '大根', '風車', '鋏', 'ハサミ', '潜水艦', 'ステーキ', 'マッチ', '二階', '落花生', 'ごはん', 'ご飯', '自転車', '歩道橋', 'クジラ', '鯨', '茶色', 'あやめ', 'ふくらはぎ', 'もも', '桃', '鯛焼き', '道路', '靴べら', '水色', '壁', 'タンポポ', 'たんぽぽ', 'いかだ', 'ヤギ', '山羊', '鼻', 'エビ', '海老', '台所', 'オートバイ', 'かぶ', '蕪', '柳', 'しゃもじ', 'まんじゅう', '饅頭', 'かかと', '薄紫', '家', 'おせち料理', '青', '傘', 'つくし', 'リンゴ', '林檎', '馬車', '線路', 'タツノオトシゴ', '耳', '便所', 'レンコン', '蓮根', '猫', '黄色', 'へそ', '街灯', '障子', '酒', '船', '安全ピン', 'もみじ'\n",
        "```\n",
        "\n",
        "- 上記のローマ字表記を音表現として用いた\n",
        "\n",
        "```\n",
        "'basu', 'midori', 'sakura', 'norimaki', 'norimaki', 'goju:noto:', 'koqpu', 'gobo:', 'tsuchifumazu', 'furo', 'hitode', 'hamu', 'usagi', 'usagi', 'usagi', 'ro:puwei', 'gaqko:', 'chiritori', 'eNgawa', 'ha', 'negi', 'ajisai', 'ajisai', 'hai:ro', 'teNjo:', 'kagi', 'hadairo', 'wani', 'wani', 'deNsha', 'kao', 'matsu', 'ga:dore:ru', 'kaki', 'chimaki', 'shiNgo:', 'susuki', 'usu', 'jo:ro', 'jo:ro', 'koNseNto', 'teNpura', 'teNpura', 'nakayubi', 'yoqto', 'piNku', 'fukuro:', 'fukuro:', 'mikaN', 'mikaN', 'mikaN', 'hashira', 'kakuzato:', 'inu', 'kago', 'gakago', 'bara', 'bara', 'nabe', 'mabuta', 'kurumi', 'kuro', 'depa:to', 'ka:ne:shoN', 'shiro', 'ari', 'to:fu', 'doraiba:', 'koN', 'kaidaN', 'seNsha', 'niNjiN', 'senaka', 'kagamimochi', 'supu:N', 'asagao', 'kiNiro', 'ashi', 'fusuma', 'hebi', 'hebi', 'hebi', 'remoN', 'ko:eN', 'ubaguruma', 'yuka', 'fuji', 'piNseqto', 'toraqku', 'ichigo', 'ichigo', 'ichigo', 'o:doiro', 'seNto:', 'namazu', 'soba', 'soba', 'onaka', 'onaka', 'oreNji', 'bata:', 'ko:ba', 'hato', 'hato', 'deNtaku', 'nodobotoke', 'nodobotoke', 'chu:riqpu', 'hakusai', 'torakuta:', 'ro:ka', 'patoka:', 'oshi:re', 'eNpitsu', 'mejiri', 'imo', 'tsuribashi', 'aka', 'kakigo:ri', 'hyo:', 'saboteN', 'piramiqdo', 'sai', 'me', 'himawari', 'hataki', 'sashimi', 'sashimi', 'geNkaN', 'tomato', 'kimidori', 'saNriNsha', 'niwatori', 'niwatori', 'tsumuji', 'asuparagasu', 'doa', 'giNiro', 'uisuki:', 'ume', 'takushi:', 'do:butsueN', 'tokonoma', 'kogecha', 'budo:', 'budo:', 'budo:', 'ame', 'kemushi', 'airoN', 'tera', 'sori', 'hyo:taN', 'kubi', 'keshigomu', 'ho:', 'icho:', 'icho:', 'eki', 'gyo:za', 'ushi', 'biwa', 'biwa', 'hiko:ki', 'tatami', 'shiro', 'take', 'perikaN', 'murasaki', 'tesuri', 'kuchi', 'daikoN', 'fu:sha', 'hasami', 'hasami', 'seNsuikaN', 'sute:ki', 'maqchi', 'nikai', 'raqkasei', 'gohaN', 'gohaN', 'jiteNsha', 'hodo:kyo:', 'kujira', 'kujira', 'chairo', 'ayame', 'fukurahagi', 'momo', 'momo', 'taiyaki', 'do:ro', 'kutsubera', 'mizuiro', 'kabe', 'taNpopo', 'taNpopo', 'ikada', 'yagi', 'yagi', 'hana', 'ebi', 'ebi', 'daidokoro', 'o:tobai', 'kabu', 'kabu', 'yanagi', 'shamoji', 'maNju:', 'maNju:', 'kakato', 'usumurasaki', 'ie', 'osechiryo:ri', 'ao', 'kasa', 'tsukushi', 'riNgo', 'riNgo', 'basha', 'seNro', 'tatsuno:toshigo', 'mimi', 'beNjo', 'reNkoN', 'reNkoN', 'neko', 'ki:ro', 'heso', 'gaito:', 'sho:ji', 'sake', 'fune', 'aNzeNpiN', 'momiji'\n",
        "```\n",
        "\n",
        "- 用いた音表現\n",
        "\n",
        "```\n",
        "['<EOW>', '<PAD>', '<SOW>', '<UNK>', 'N', 'a', 'a:', 'a::', 'b', 'by', 'ch', 'd', 'dy', 'e', 'e:', 'f', 'g', 'gy', 'h', 'hy', 'i', 'i:', 'j', 'k', 'ky', 'm', 'my', 'n', 'ny', 'o', 'o:', 'o::', 'p', 'py', 'q', 'r', 'ry', 's', 'sh', 't', 'ts', 'u', 'u:', 'u::', 'w', 'y', 'z']\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x25t5qBSugG"
      },
      "source": [
        "# 0. ハイパーパラメータの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v0ZcWZINSugG",
        "outputId": "d40b5b09-b12b-4531-c706-9219e75ae53e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[34mtraindata_size\u001b[0m \u001b[30m: 10000\u001b[0m\n",
            "\u001b[1m\u001b[34mepochs\u001b[0m \u001b[30m: 20\u001b[0m\n",
            "\u001b[1m\u001b[34mhidden_size\u001b[0m \u001b[30m: 256\u001b[0m\n",
            "\u001b[1m\u001b[34mloss_func\u001b[0m \u001b[30m: CrossEntropyLoss()\u001b[0m\n",
            "\u001b[1m\u001b[34moptim_func\u001b[0m \u001b[30m: <class 'torch.optim.adam.Adam'>\u001b[0m\n",
            "\u001b[1m\u001b[34mlr\u001b[0m \u001b[30m: 0.001\u001b[0m\n",
            "\u001b[1m\u001b[34mdropout_p\u001b[0m \u001b[30m: 0.0\u001b[0m\n",
            "\u001b[1m\u001b[34msource\u001b[0m \u001b[30m: orthography\u001b[0m\n",
            "\u001b[1m\u001b[34mtarget\u001b[0m \u001b[30m: orthography\u001b[0m\n",
            "\u001b[1m\u001b[34mpath_saved\u001b[0m \u001b[30m: 2022_0219tlpa_o2o.pt\u001b[0m\n",
            "\u001b[1m\u001b[34mpath_graph\u001b[0m \u001b[30m: 2022_0219tlpa_o2o.pdf\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from termcolor import colored\n",
        "import torch.nn\n",
        "import torch.optim\n",
        "\n",
        "params = {\n",
        "    'traindata_size': 10000,                  # 訓練データ数，NTT 日本語語彙特性の高頻度語を上位から抽出\n",
        "    'epochs': 20,                             # 学習のためのエポック数\n",
        "    'hidden_size': 256,                       # 中間層のニューロン数\n",
        "\n",
        "    'loss_func' :torch.nn.CrossEntropyLoss(), # 交差エントロピー損失 ['torch.nn.NLLLoss()', or 'torch.nn.CrossEntropyLoss()']\n",
        "    'optim_func': torch.optim.Adam,           # 最適化アルゴリズム ['torch.optim.Adam', 'torch.optim.SGD', 'torch.optim.AdamW']\n",
        "\n",
        "    'lr': 0.001,                              # 学習率\n",
        "    'dropout_p': 0.0,                         # ドロップアウト率\n",
        "    \n",
        "    'source': 'orthography',                  # ['orthography' or 'phonology']\n",
        "    'target': 'orthography',                  # ['orthography' or 'phonology']\n",
        "    'path_saved': '2022_0219tlpa_o2o.pt',     # 学習済のモデルパラメータを保存するファイル名\n",
        "    'path_graph': '2022_0219tlpa_o2o.pdf',    # 結果の散布図を保存するファイル名\n",
        "}\n",
        "\n",
        "for param in params:\n",
        "    print(colored(f'{param}','blue',attrs=['bold']), colored(f': {params[param]}','grey'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqlm_U4xSugI"
      },
      "source": [
        "## 0.1 準備作業"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4P-cpqpdSugI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import typing\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import gzip\n",
        "from termcolor import colored\n",
        "\n",
        "# from tqdm import tqdm         #commandline で実行時\n",
        "from tqdm.notebook import tqdm  #jupyter で実行時\n",
        "\n",
        "isColab = 'google.colab' in str(get_ipython())\n",
        "if isColab:\n",
        "\n",
        "    # MeCab, ipadic のインストール\n",
        "    !apt install aptitude swig > /dev/null 2>&1\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null 2>&1\n",
        "    !pip install mecab-python3 > /dev/null 2>&1\n",
        "    !git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "    !echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a > /dev/null 2>&1\n",
        "    \n",
        "    import subprocess\n",
        "    cmd='echo `mecab-config --dicdir`\\\"/mecab-ipadic-neologd\\\"'\n",
        "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                                     shell=True).communicate()[0]).decode('utf-8')\n",
        "\n",
        "    !python -m unidic download > /dev/null 2>&1\n",
        "    !pip install ipadic > /dev/null 2>&1\n",
        "    !pip install 'konoha[mecab]' > /dev/null 2>&1 \n",
        "\n",
        "    !pip install Levenshtein > /dev/null 2>&1\n",
        "    !pip install jaconv > /dev/null 2>&1 \n",
        "    !pip install japanize_matplotlib > /dev/null 2>&1 \n",
        "\n",
        "import Levenshtein    \n",
        "import jaconv    \n",
        "\n",
        "# ライブラリの依存関係がスパゲティになってる。整理しなければ。\n",
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc \n",
        "import MeCab\n",
        "yomi = MeCab.Tagger('-Oyomi').parse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9X9Cp5ySugI"
      },
      "source": [
        "# 1 データセットの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ccap import ccap_w2v\n",
        "w2v = ccap_w2v().w2v"
      ],
      "metadata": {
        "id": "mP1S6_AM5Dzx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BVHzuA0QSugI",
        "outputId": "7cfee99a-3106-48fb-8728-5e697ceb4d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ccap'...\n",
            "remote: Enumerating objects: 236, done.\u001b[K\n",
            "remote: Counting objects: 100% (236/236), done.\u001b[K\n",
            "remote: Compressing objects: 100% (224/224), done.\u001b[K\n",
            "remote: Total 236 (delta 126), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (236/236), 4.57 MiB | 10.36 MiB/s, done.\n",
            "Resolving deltas: 100% (126/126), done.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-11cebca8ca53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mccap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtlpa_o2p\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTLPA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtlpa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTLPA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindata_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'traindata_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolored\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'訓練データサイズ:{len(tlpa.training_data)} 語。NTT 日本語語彙特性の頻度上位 {len(tlpa.training_data)} 語'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ただし，TLPA に用いられる単語は含まない'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ccap/tlpa_o2p.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, yomi, w2v, reload, traindata_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mccap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mccap_w2v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mccap_w2v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis2017\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0myomi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ccap/ccap_w2v.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, is2017)\u001b[0m\n\u001b[1;32m     72\u001b[0m                                                     \u001b[0mdest_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                                                     \u001b[0munzip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                                                     showsize=True)\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0;31m#!wget --no-check-certificate --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1B9HGhLZOja4Xku5c_d-kMhCXn1LBZgDb' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1B9HGhLZOja4Xku5c_d-kMhCXn1LBZgDb\" -O 2021_05jawiki_hid128_win10_neg10_cbow.bin.gz && rm -rf /tmp/cookies.txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;31m#!wget --no-check-certificate --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1OWmFOVRC6amCxsomcRwdA6ILAA5s4y4M' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1OWmFOVRC6amCxsomcRwdA6ILAA5s4y4M\" -O 2021_05jawiki_hid128_win10_neg10_sgns.bin.gz && rm -rf /tmp/cookies.txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google_drive_downloader/google_drive_downloader.py\u001b[0m in \u001b[0;36mdownload_file_from_google_drive\u001b[0;34m(file_id, dest_path, overwrite, unzip, showsize)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdestination_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
          ]
        }
      ],
      "source": [
        "#%load_ext autoreload\n",
        "#%autoreload 2\n",
        "\n",
        "# 自作ライブラリの読み込み\n",
        "if isColab:\n",
        "    ![ -d ccap ] & /bin/rm -rf ccap\n",
        "    !git clone https://github.com/ShinAsakawa/ccap.git\n",
        "\n",
        "# from ccap import ccap_w2v\n",
        "# w2v = ccap_w2v().w2v\n",
        "\n",
        "from ccap.tlpa_o2p import TLPA\n",
        "tlpa = TLPA(traindata_size=params['traindata_size'])\n",
        "print(colored(f'訓練データサイズ:{len(tlpa.training_data)} 語。NTT 日本語語彙特性の頻度上位 {len(tlpa.training_data)} 語','blue',attrs=['bold']))\n",
        "print('ただし，TLPA に用いられる単語は含まない')\n",
        "print(colored(f'検証データサイズ:{len(tlpa.tlpa_data)} TLPA 単語数','blue',attrs=['bold']))\n",
        "print(colored(f'音素数:{len(tlpa.phone_vocab)}','blue',attrs=['bold']))\n",
        "print(colored(f'書記素数:{len(tlpa.ortho_vocab)}','blue',attrs=['bold']))\n",
        "print(colored(f'最長書記素(文字)数:{tlpa.max_ortho_length}, 最長音素数:{tlpa.max_phone_length}', 'blue',attrs=['bold']))\n",
        "\n",
        "tlpa.max_length = tlpa.max_ortho_length + 1 if tlpa.max_ortho_length > tlpa.max_phone_length else tlpa.max_phone_length + 1\n",
        "print(colored(f'tlpa.max_length: {tlpa.max_length}', 'blue', attrs=['bold']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##!wget --no-check-certificate --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1B9HGhLZOja4Xku5c_d-kMhCXn1LBZgDb' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1B9HGhLZOja4Xku5c_d-kMhCXn1LBZgDb\" -O 2021_05jawiki_hid128_win10_neg10_cbow.bin.gz && rm -rf /tmp/cookies.txt\n",
        "!wget --no-check-certificate --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1OWmFOVRC6amCxsomcRwdA6ILAA5s4y4M' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1OWmFOVRC6amCxsomcRwdA6ILAA5s4y4M\" -O 2021_05jawiki_hid128_win10_neg10_sgns.bin.gz && rm -rf /tmp/cookies.txt\n",
        "##!wget --no-check-certificate --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1JTkU5SUBU2GkURCYeHkAWYs_Zlbqob0s' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1JTkU5SUBU2GkURCYeHkAWYs_Zlbqob0s\" -O 2021_05jawiki_hid200_win20_neg20_cbow.bin.gz && rm -rf /tmp/cookies.txt\n",
        "##!wget --no-check-certificate --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1VPL2Mr9JgWHik9HjRmcADoxXIdrQ3ds7' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1VPL2Mr9JgWHik9HjRmcADoxXIdrQ3ds7\" -O 2021_05jawiki_hid200_win20_neg20_sgns.bin.gz && rm -rf /tmp/cookies.txt\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec\n",
        "w2v_file = '2021_05jawiki_hid128_win10_neg10_sgns.bin.gz'\n",
        "w2v = KeyedVectors.load_word2vec_format(w2v_file, \n",
        "                                         encoding='utf-8', \n",
        "                                        unicode_errors='replace',\n",
        "                                        binary=True)\n",
        "# from ccap.tlpa_o2p import TLPA\n",
        "# tlpa = TLPA(traindata_size=params['traindata_size'])\n",
        "# print(colored(f'訓練データサイズ:{len(tlpa.training_data)} 語。NTT 日本語語彙特性の頻度上位 {len(tlpa.training_data)} 語','blue',attrs=['bold']))\n",
        "# print('ただし，TLPA に用いられる単語は含まない')\n",
        "# print(colored(f'検証データサイズ:{len(tlpa.tlpa_data)} TLPA 単語数','blue',attrs=['bold']))\n",
        "# print(colored(f'音素数:{len(tlpa.phone_vocab)}','blue',attrs=['bold']))\n",
        "# print(colored(f'書記素数:{len(tlpa.ortho_vocab)}','blue',attrs=['bold']))\n",
        "# print(colored(f'最長書記素(文字)数:{tlpa.max_ortho_length}, 最長音素数:{tlpa.max_phone_length}', 'blue',attrs=['bold']))\n",
        "\n",
        "# tlpa.max_length = tlpa.max_ortho_length + 1 if tlpa.max_ortho_length > tlpa.max_phone_length else tlpa.max_phone_length + 1\n",
        "# print(colored(f'tlpa.max_length: {tlpa.max_length}', 'blue', attrs=['bold']))"
      ],
      "metadata": {
        "id": "ZtXVQMMnKEST",
        "outputId": "5457c3db-2642-4c30-c010-a7b26db3813c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-22 03:31:25--  https://docs.google.com/uc?export=download&confirm=t&id=1OWmFOVRC6amCxsomcRwdA6ILAA5s4y4M\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.81.206, 2607:f8b0:4004:82f::200e\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.81.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-0k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l7a3s32so8k9lhsl1tscssjcgghnhq4r/1645500675000/10804218431552746902/*/1OWmFOVRC6amCxsomcRwdA6ILAA5s4y4M?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-02-22 03:31:25--  https://doc-0s-0k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l7a3s32so8k9lhsl1tscssjcgghnhq4r/1645500675000/10804218431552746902/*/1OWmFOVRC6amCxsomcRwdA6ILAA5s4y4M?e=download\n",
            "Resolving doc-0s-0k-docs.googleusercontent.com (doc-0s-0k-docs.googleusercontent.com)... 142.250.81.193, 2607:f8b0:4004:82f::2001\n",
            "Connecting to doc-0s-0k-docs.googleusercontent.com (doc-0s-0k-docs.googleusercontent.com)|142.250.81.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 467794825 (446M) [application/x-gzip]\n",
            "Saving to: ‘2021_05jawiki_hid128_win10_neg10_sgns.bin.gz’\n",
            "\n",
            "2021_05jawiki_hid12 100%[===================>] 446.12M  92.2MB/s    in 7.5s    \n",
            "\n",
            "2022-02-22 03:31:33 (59.8 MB/s) - ‘2021_05jawiki_hid128_win10_neg10_sgns.bin.gz’ saved [467794825/467794825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ccap.tlpa_o2p import TLPA\n",
        "tlpa = TLPA(traindata_size=params['traindata_size'])\n"
      ],
      "metadata": {
        "id": "cEgMYIH-fHaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZdab5FKSugJ"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.1 補足説明\n",
        "\n",
        "### 著作権に関する注意事項\n",
        "\n",
        "上で読み込んだ `tlpa` には，tlpa の絵画命名検査に用いられる 200 図版の単語情報と，NTT 日本語語彙特性の頻度情報が含まれています。\n",
        "本コードを再配布する際には，これら 2 つ，TLPA と NTT 日本語語彙特性データの著作権にご留意ください。\n",
        "\n",
        "### ライブラリの使い方\n",
        "\n",
        "今回作成したライブラリには以下のものが含まれる\n",
        "\n",
        "- `tlpa.vocab`: 登録されている全単語 159061 語のリスト\n",
        "- `tlpa.phone_vocab`: julius 表記した音表現のリスト。音であるから vocab ではなく phoneme などとすべきだったかも知れないが，特殊トークンを含む 47 音表現\n",
        "- `tlpa.ortho_vocab`: 書記素表現のリスト，上の phone_vocab 同様に vocab という表記は適切ではなく，grapheme などと表記すべきだが，自然言語の文脈で考えていたので，このよな表記となった。\n",
        "- `ntt_orth2hira`: NTT 日本語語彙特性に現れる表記を，ひらがなに変換する辞書 `tlpa.ntt_orth2hira[\"神経\"]` などとすれば，`シンケイ` という読みを得る\n",
        "- `tlpa.tlpa`: tlpa で用いられる単語 237 語の，音表現，カタカナ，意味表現(word2vec) \n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFGnWVV5SugJ"
      },
      "outputs": [],
      "source": [
        "# 再現性確保のため，乱数の種を設定する\n",
        "import random\n",
        "import torch\n",
        "\n",
        "# リソースの選択（CPU/GPU）\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 乱数シード固定（再現性の担保）\n",
        "def fix_seed(seed):\n",
        "    random.seed(seed)  # for random\n",
        "    np.random.seed(seed) # for numpy\n",
        "\n",
        "    # for pytorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed = 42\n",
        "fix_seed(seed)\n",
        "\n",
        "# データローダーのサブプロセスの乱数の seed が固定\n",
        "def worker_init_fn(worker_id):\n",
        "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
        "\n",
        "print(worker_init_fn(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv8Rc5fFSugK"
      },
      "outputs": [],
      "source": [
        "source_vocab = tlpa.ortho_vocab if  params['source'] == 'orthography' else tlpa.phone_vocab\n",
        "target_vocab = tlpa.ortho_vocab if params['target'] == 'orthography' else tlpa.phone_vocab\n",
        "source_ids = 'ortho_ids' if params['source'] == 'orthography' else 'phone_ids'\n",
        "target_ids = 'ortho_ids' if params['target'] == 'orthography' else 'phone_ids'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT10MAQwSugK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "class _train_dataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    上で読み込んだ自作データ管理ライブラリ TLPA でも良いので冗長なのだが，訓練データセットと検証データセットとを明示的に定義しておく\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, tlpa=tlpa)->None:\n",
        "        self.tlpa = tlpa\n",
        "        self.data = tlpa.training_data\n",
        "        self.order = {i:self.data[x] for i, x in enumerate(self.data)}\n",
        "        \n",
        "    def __len__(self)->int:\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, x:int):\n",
        "        return self.order[x][source_ids] + [source_vocab.index('<EOW>')], self.order[x][target_ids] + [target_vocab.index('<EOW>')]\n",
        "    \n",
        "    def convert_source_ids_to_tokens(self, ids:list):\n",
        "        return [source_vocab[idx] for idx in ids]\n",
        "    \n",
        "    def convert_target_ids_to_tokens(self, ids:list):\n",
        "        return [target_vocab[idx] for idx in ids]\n",
        "\n",
        "\n",
        "class _val_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tlpa=tlpa)->None:\n",
        "        self.tlpa = tlpa\n",
        "        self.data = tlpa.tlpa_data\n",
        "        self.order = {i:self.data[x] for i, x in enumerate(self.data)}\n",
        "        \n",
        "    def __len__(self)->int:\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, x:int):\n",
        "        return self.order[x][source_ids] + [source_vocab.index('<EOW>')], self.order[x][target_ids] + [target_vocab.index('<EOW>')]\n",
        "    \n",
        "    def convert_source_ids_to_tokens(self, ids:list):\n",
        "        return [source_vocab[idx] for idx in ids]\n",
        "    \n",
        "    def convert_target_ids_to_tokens(self, ids:list):\n",
        "        return [target_vocab[idx] for idx in ids]\n",
        "\n",
        "\n",
        "train_dataset = _train_dataset()\n",
        "val_dataset = _val_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWvow18nSugL"
      },
      "source": [
        "# 2 学習に用いる，符号化器 (エンコーダ)，復号化器 (デコーダ) の定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ivlR1AXSugL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    '''RNNによる符号化器'''\n",
        "    def __init__(self, n_inp, n_hid):\n",
        "        super().__init__()\n",
        "        self.n_hid = n_hid\n",
        "\n",
        "        self.embedding = nn.Embedding(n_inp, n_hid)\n",
        "        self.gru = nn.GRU(n_hid, n_hid)\n",
        "\n",
        "    def forward(self, inp, hid):\n",
        "        #print(f'inp:{inp}. inp.size(): {inp.size()}')\n",
        "        embedded = self.embedding(inp).view(1, 1, -1)\n",
        "        out = embedded\n",
        "        out, hid = self.gru(out, hid)\n",
        "        return out, hid\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.n_hid, device=device)\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    '''注意付き復号化器の定義'''\n",
        "    def __init__(self, n_hid, n_out, dropout_p=0.1, max_length=tlpa.max_length):\n",
        "        super().__init__()\n",
        "        self.n_hid = n_hid\n",
        "        self.n_out = n_out\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_out, self.n_hid)\n",
        "        self.attn = nn.Linear(self.n_hid * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.n_hid * 2, self.n_hid)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.n_hid, self.n_hid)\n",
        "        self.out = nn.Linear(self.n_hid, self.n_out)\n",
        "\n",
        "    def forward(self, inp, hid, encoder_outputs):\n",
        "        embedded = self.embedding(inp).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hid[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        out = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        out = self.attn_combine(out).unsqueeze(0)\n",
        "\n",
        "        out = F.relu(out)\n",
        "        out, hid = self.gru(out, hid)\n",
        "\n",
        "        out = F.log_softmax(self.out(out[0]), dim=1)\n",
        "        return out, hid, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.n_hid, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0ubNNEYSugL"
      },
      "source": [
        "# 3. 訓練関数 `train()` の定義 教師強制付き"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkonJGjrSugM"
      },
      "outputs": [],
      "source": [
        "def convert_ids2tensor(sentence_ids):\n",
        "    return torch.tensor(sentence_ids, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "teacher_forcing_ratio = 0.5  # 教師強制率。文献によっては，訓練中にこの値を徐々に減衰させることも行われます\n",
        "\n",
        "def train(input_tensor, \n",
        "          target_tensor, \n",
        "          encoder, decoder, \n",
        "          encoder_optimizer, decoder_optimizer, \n",
        "          criterion, max_length=tlpa.max_length):\n",
        "\n",
        "    \n",
        "    encoder_hidden = encoder.initHidden() # 符号化器の中間層を初期化\n",
        "    encoder_optimizer.zero_grad()         # 符号化器の最適化関数の初期化\n",
        "    decoder_optimizer.zero_grad()         # 復号化器の最適化関数の初期化\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.n_hid, device=device)\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[target_vocab.index('<SOW>')]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    \n",
        "    # 教師強制をするか否かを確率的に決める\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    if use_teacher_forcing: # 教師強制する場合 Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else: # 教師強制しない場合 Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == target_vocab.index('<EOW>'):\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri3aU_7PSugM"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    \"\"\"時間変数を見やすいように，分と秒に変換して返す\"\"\"\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return f'{int(m):2d}分 {int(s):2d}秒'\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    \"\"\"開始時刻 since と，現在の処理が全処理中に示す割合 percent を与えて，経過時間と残り時間を計算して表示する\"\"\"\n",
        "    now = time.time()  #現在時刻を取得\n",
        "    s = now - since    # 開始時刻から現在までの経過時間を計算\n",
        "    #s = since - now    \n",
        "    es = s / (percent) # 経過時間を現在までの処理割合で割って終了予想時間を計算\n",
        "    rs = es - s        # 終了予想時刻から経過した時間を引いて残り時間を計算\n",
        "\n",
        "    return f'経過時間:{asMinutes(s)} (残り時間 {asMinutes(rs)})'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRn_-6xuSugM"
      },
      "source": [
        "# 4. `fit()` 関数の定義 エポックを反復して `train()` を呼び出す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_jrB1XOSugM"
      },
      "outputs": [],
      "source": [
        "def fit(encoder:nn.Module, \n",
        "        decoder:nn.Module, \n",
        "        epochs:int=params['epochs'],\n",
        "        lr:float=params['lr'],\n",
        "        n_sample:int=3)->list:\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    encoder_optimizer = params['optim_func'](encoder.parameters(), lr=lr)\n",
        "    decoder_optimizer = params['optim_func'](decoder.parameters(), lr=lr)\n",
        "    criterion = params['loss_func']\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        #エポックごとに学習順をシャッフルする\n",
        "        learning_order = np.random.permutation(train_dataset.__len__())\n",
        "        for i in range(train_dataset.__len__()):\n",
        "            x = learning_order[i]   # ランダムにデータを取り出す \n",
        "            inputs, targets = train_dataset.__getitem__(x)\n",
        "            input_tensor = convert_ids2tensor(inputs)\n",
        "            target_tensor = convert_ids2tensor(targets)\n",
        "            \n",
        "            #訓練の実施\n",
        "            loss = train(input_tensor, target_tensor, \n",
        "                         encoder, decoder, \n",
        "                         encoder_optimizer, decoder_optimizer, \n",
        "                         criterion)\n",
        "            epoch_loss += loss\n",
        "        \n",
        "        losses.append(epoch_loss/train_dataset.__len__())\n",
        "        print(colored(f'エポック:{epoch:2d} 損失:{epoch_loss/train_dataset.__len__():.2f}', 'cyan', attrs=['bold']),\n",
        "              f'{timeSince(start_time, (epoch+1) * train_dataset.__len__()/(epochs * train_dataset.__len__()))}')\n",
        "        \n",
        "        evaluateRandomly(encoder, decoder, n=n_sample)\n",
        "        \n",
        "    return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aSpuoBbSugN"
      },
      "source": [
        "# 5 評価関数 `evaluate()` の定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZgex7oNSugN"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder:nn.Module, \n",
        "             decoder:nn.Module, \n",
        "             input_ids:list, \n",
        "             max_length:int=tlpa.max_length,\n",
        "            )->(list,torch.LongTensor):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = convert_ids2tensor(input_ids)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.n_hid, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[source_vocab.index('<SOW>')]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words, decoded_ids = [], []  # decoded_ids を追加\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            decoded_ids.append(int(topi.squeeze().detach())) # decoded_ids に追加\n",
        "            if topi.item() == target_vocab.index('<EOW>'):\n",
        "                decoded_words.append('<EOW>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(target_vocab[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoded_ids, decoder_attentions[:di + 1]  # decoded_ids を返すように変更\n",
        "        #return decoded_words, decoder_attentions[:di + 1]    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhYAG-x2SugN"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder:nn.Module, \n",
        "                     decoder:nn.Module, \n",
        "                     n:int=5)->float:\n",
        "    \n",
        "    srcs, preds = [], []\n",
        "    for x in np.random.randint(val_dataset.__len__(), size=n):\n",
        "        input_ids, target_ids = val_dataset.__getitem__(x)\n",
        "        input_words = val_dataset.convert_source_ids_to_tokens(input_ids)\n",
        "        print(f'入力: {target_ids}<-{input_ids}:{input_words}')\n",
        "        output_words, output_ids, attentions = evaluate(encoder, decoder, input_ids)\n",
        "\n",
        "        srcs.append(input_words)\n",
        "        preds.append(output_words)\n",
        "        print(f'出力: {output_ids}',f':{output_words}')\n",
        "        print('---')\n",
        "    return srcs, preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i__RlINySugN"
      },
      "source": [
        "# 以前に保存したモデルがある場合には，そのパラメータを読み込む"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkiXW872SugN"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(params['path_saved']):\n",
        "    path_saved = params['path_saved'] \n",
        "    checkpoint = torch.load(path_saved)\n",
        "    encoder = EncoderRNN(len(target_vocab), params['hidden_size']).to(device)\n",
        "    decoder = AttnDecoderRNN(n_hid=params['hidden_size'], n_out=len(target_vocab), dropout_p=params['dropout_p']).to(device)\n",
        "    encoder.load_state_dict(checkpoint['encoder'])\n",
        "    decoder.load_state_dict(checkpoint['decoder'])\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    losses = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkGAT3dNSugO"
      },
      "source": [
        "# 6. 学習の実施"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5-RUJQuSugO"
      },
      "outputs": [],
      "source": [
        "encoder = EncoderRNN(len(source_vocab), params['hidden_size']).to(device)\n",
        "decoder = AttnDecoderRNN(n_hid=params['hidden_size'], n_out=len(target_vocab), dropout_p=params['dropout_p']).to(device)\n",
        "decoder\n",
        "#print(tlpa.max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt3INALlSugO"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "hidden_size = params['hidden_size']\n",
        "encoder = EncoderRNN(len(source_vocab), hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(n_hid=hidden_size, n_out=len(target_vocab), dropout_p=params['dropout_p']).to(device)\n",
        "\n",
        "losses = []\n",
        "losses = losses + fit(encoder, decoder, epochs=params['epochs'], n_sample=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJA6RFXjSugO"
      },
      "outputs": [],
      "source": [
        "#losses = losses + fit(encoder, decoder, epochs=params['epochs'], n_sample=5)\n",
        "losses = losses + fit(encoder, decoder, epochs=3, n_sample=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLwi6RReSugO"
      },
      "source": [
        "# 7 学習経過の描画"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwJiwAHfSugO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points:list)->None:\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    loc = ticker.MultipleLocator(base=0.2) # this locator puts ticks at regular intervals\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    \n",
        "showPlot(losses)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPmRv7-uSugO"
      },
      "outputs": [],
      "source": [
        "_ = evaluateRandomly(encoder, decoder, n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COwAzNeTSugO"
      },
      "source": [
        "# 8 自由入力による評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i93SgBX-SugP"
      },
      "outputs": [],
      "source": [
        "def tokenize(inp_word:str, pad:bool=False)->list:\n",
        "    if pad:\n",
        "        ret = [source_vocab.index('<PAD>') for _ in range(tlpa.max_length - len(inp_word))]\n",
        "    else:\n",
        "        ret = []\n",
        "    return ret + [source_vocab.index(ch) if ch in source_vocab else source_vocab[source_vocab.index('<UNK>')] for ch in inp_word]\n",
        "\n",
        "\n",
        "print(train_dataset.convert_source_ids_to_tokens(tokenize('watashi',pad=True)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMOJWyjSSugP"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WfsqAjhSugP"
      },
      "outputs": [],
      "source": [
        "def evaluate_free_input(encoder:nn.Module, \n",
        "                        decoder:nn.Module,\n",
        "                        inp=None,\n",
        "                       )->None:\n",
        "    if inp == None:\n",
        "        inp = input()\n",
        "    inp = jaconv.normalize(inp)\n",
        "    inputs = tokenize(inp, pad=False)\n",
        "    input_ids = inputs # ['input_ids']\n",
        "    output_tokens, output_words, attentions = evaluate(encoder, decoder, input_ids)\n",
        "    #output_ids = val_dataset.convert_phone_ids_to_tokens(output_tokens)\n",
        "    output_ids = output_tokens\n",
        "    return input_ids, output_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeA2wzj3SugP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIzylQSLSugP"
      },
      "outputs": [],
      "source": [
        "#inp, out = evaluate_free_input(encoder,decoder, inp='お前は虎だ。虎になるのだ。')\n",
        "inp, out = evaluate_free_input(encoder,decoder)\n",
        "print(val_dataset.convert_source_ids_to_tokens(inp))\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpmNMX0zSugP"
      },
      "source": [
        "# 9 エンコーダの内部表現の取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPzVjnMLSugP"
      },
      "outputs": [],
      "source": [
        "def get_an_encoder_representation(encoder:nn.Module,\n",
        "                                  input_ids:list,\n",
        "                                  max_length:int=tlpa.max_length)->(list,torch.LongTensor):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_tensor = convert_ids2tensor(input_ids)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.n_hid, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        return encoder_hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3Q8tziYSugP"
      },
      "outputs": [],
      "source": [
        "X = np.zeros((val_dataset.__len__(),hidden_size))\n",
        "for i in range(val_dataset.__len__()):\n",
        "    x = get_an_encoder_representation(encoder, input_ids = val_dataset.__getitem__(1)[0])\n",
        "    X[i] = x.squeeze(0).clone().detach().numpy()[0]\n",
        "\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijmDsCWzSugQ"
      },
      "outputs": [],
      "source": [
        "import ccap.tsne as tsne\n",
        "\n",
        "tsne_result = tsne.tsne(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeZCrix-SugQ"
      },
      "source": [
        "# 10 内部表現の描画"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0ZPDYq9SugQ"
      },
      "outputs": [],
      "source": [
        "#tsne_result.shape\n",
        "len(tlpa.tlpa_vocab)\n",
        "X.shape\n",
        "#plt.figure(figsize=(10,10))\n",
        "#plt.scatter(tsne_result[:,0],tsne_result[:,1])\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt8xocCVSugQ"
      },
      "outputs": [],
      "source": [
        "def plot_tsne(ax, R, wordlist, title=\"\"):\n",
        "    #tsne = TSNE()\n",
        "    tsne_result = TSNE(n_components=2).fit_transform(R)\n",
        "    print(tsne_result.shape)\n",
        "    tsne1, tsne2 = tsne_result[:,0], tsne_result[:,1]\n",
        "    ax_scatter_gram(ax, tsne1, tsne2, wordlist, title=title, x_label=\"tSNE 1\", y_label=\"tSNE 2\")\n",
        "    ###plot_pca(ax, Onmtp_w2v, onmtp_list, title='オノマトペ附置 (PCA)')\n",
        "    fig, ax = plt.subplots(figsize=(12,13))         # Sample figsize in inches\n",
        "    plot_pca(ax, R_sala, sala.labels, title='(PCA)')\n",
        "    plt.show()\n",
        "    fig, ax = plt.subplots(figsize=(12,13))         # Sample figsize in inches\\n\",\n",
        "    plot_tsne(ax, R_sala, sala.labels, title='onomatopea (tSNE)')\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "fig, ax = plt.subplots(figsize=(18,18))         # Sample figsize in inches\n",
        "tsne1, tsne2 = tsne_result[:,0], tsne_result[:,1]\n",
        "ax.scatter(tsne1, tsne2, s=20, color='cyan')\n",
        "\n",
        "tlpa_romaji = [\"\".join(tlpa.tlpa_data[x]['phone']) for x in tlpa.tlpa_data]\n",
        "#for i, label in enumerate(tlpa_romaji):\n",
        "for i, label in enumerate(tlpa.tlpa_vocab):\n",
        "    ax.annotate(label, (tsne1[i], tsne2[i]),fontsize=10)\n",
        "ax.set_xlabel('tSNE1')\n",
        "ax.set_ylabel('tSNE2')\n",
        "ax.set_title('p2p リカレントニューラルネットワーク自己符号化モデルのエンコーダによる tlpa 書記素の布置 tSNE') \n",
        "#plt.savefig('2022_0218tlpa_o2p_rnn_autoencoder_tSNE.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNFI_VnRSugQ"
      },
      "outputs": [],
      "source": [
        "params['path_saved']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xcau6HKFSugQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSkvE-xESugQ"
      },
      "outputs": [],
      "source": [
        "path_saved = params['path_saved']\n",
        "torch.save({'encoder':encoder.state_dict(),\n",
        "            'decoder':decoder.state_dict()}, path_saved)\n",
        "checkpoint = torch.load(path_saved)\n",
        "encoder2 = EncoderRNN(len(source_vocab), hidden_size).to(device)\n",
        "decoder2 = AttnDecoderRNN(n_hid=hidden_size, n_out=len(target_vocab), dropout_p=params['dropout_p']).to(device)\n",
        "encoder2.load_state_dict(checkpoint['encoder'])\n",
        "decoder2.load_state_dict(checkpoint['decoder'])\n",
        "encoder2.eval()\n",
        "decoder2.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07QCNP3bSugQ"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "def eval_corpus(encoder:nn.Module, \n",
        "                     decoder:nn.Module, \n",
        "                     corpus_Xy:List[Tuple])->float:\n",
        "    \n",
        "    results = {}\n",
        "    n_hit = 0\n",
        "    for input_ids, target_ids in corpus_Xy:\n",
        "        input_words = val_dataset.convert_source_ids_to_tokens(input_ids)\n",
        "        #print(f'入力: {target_ids}<-{input_ids}:{input_words}')\n",
        "        input_str = \"\".join(input_words[:-1])\n",
        "        output_words, output_ids, attentions = evaluate(encoder, decoder, input_ids)\n",
        "        output_str = \"\".join(output_words[:-1])\n",
        "        dist = Levenshtein.distance(input_str,output_str)\n",
        "        n_hit += 1 if dist == 0 else 0\n",
        "        results[input_str] = {'output':output_str, 'dist':dist}\n",
        "        \n",
        "    return results\n",
        "\n",
        "Xy = [(val_dataset.__getitem__(x)) for x in range(val_dataset.__len__())]\n",
        "results = eval_corpus(encoder, decoder, Xy)\n",
        "print(f\"平均レーベンシュタイン距離: {np.array([results[x]['dist'] for x in results]).mean():.3f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "2022_0219tlpa_enc_dec.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}