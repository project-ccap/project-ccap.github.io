{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLPA 絵画命名検査での単語発話モデル CCAP バージョン\n",
    "\n",
    "- date: 2022-0206\n",
    "- filename: 2022_0206tlpa_o2p.ipynb\n",
    "- file required: 呼称(TLPA)語彙属性.xlsx\n",
    "- author: 浅川伸一\n",
    "- license: MIT\n",
    "\n",
    "## 手続き\n",
    "\n",
    "\n",
    "### 材料\n",
    "\n",
    "TLPA の絵画命名課題に用いられる図版 200 枚を単語にして刺激語として用いた。\n",
    "TLPA の本課題は，入力が白黒画像であるが，これを単語として表現した場合，複数の表記が考えられる。\n",
    "たとえば，「海苔巻き」と「ノリ巻き」とであれば，どちらでも刺激図版を表す単語として用いることができる。\n",
    "今回は，日本語ウィキペディア 2021 年 5 月版に記載のあった単語を用いることとした。\n",
    "このため，200 枚の図版に対して，237 語の刺激語を用いた。\n",
    "一方，NTT 日本語語彙特性 (天野，近藤, 1999) の頻度に基づいて，最頻語を 10,000 語用いて訓練データとした。\n",
    "NTT 日本語語彙特性に基づく単語頻度のうち，記号やアルファベットを除き，かつ，上述の TLPA 単語 237 語を除外した\n",
    "上位 10,000 語を訓練データとした。\n",
    "\n",
    "訓練においては，各入力単語は，文字別に付番した番号を文字トークン ID とした。\n",
    "この文字トークン ID を ワンホットベクトルとみなし，入力層に与えた。\n",
    "出力の音素表現については，単語の読みを mecab (Kudo, 2007) によって取得した。\n",
    "得られた単語の読みを julius 表記によってアルファベット文字列に変換した情報をワンホットベクトルとみなして出力表現とした。\n",
    "このようにして得られた，単語の音表現は 41 種 と特殊トークン 4 種の計 45 種 であった。\n",
    "特殊トークン 4 種とは，入力表現でも用いられた，次の 4 種である。\n",
    "\n",
    "* '\\<EOW\\>': 単語の終端を表す特殊トークン\n",
    "* '\\<SOW\\>': 単語の先端を表す特殊トークン\n",
    "* '\\<UNK\\>': 未知記号を表す特殊トークン\n",
    "* '\\<PAD\\>': 埋め草用特殊トークン，ミニバッチ学習の際に入出力表現ベクトルの次元を揃えるために用いられる\n",
    "    \n",
    "このようにして得られた入力用文字トークンは，1903 種，音トークンは先述のとおり 45 種とした。\n",
    "入力文字トークン系列の最大系列長は 12, 出力音トークン系列の最大系列長は，28 であった。\n",
    "\n",
    "たとえば，単語「バス」についての入出力表現は，以下の通りである:\n",
    "\n",
    "```\n",
    "{0: {'orig': 'バス', 'ortho': ['バ', 'ス'], 'phone': ['b', 'a', 's', 'u'], 'ortho_ids': [696, 519], 'phone_ids': [25, 7, 19, 12]\n",
    "```\n",
    "\n",
    "### 訓練手続き\n",
    "\n",
    "材料の項で述べた単語の書記素表現系列から音表現系列への変換を，符号化器=復号化器モデル，あるいは seq2seq モデル，(Sutskever et.al, 2014, arxiv:1409.3215) \n",
    "を用いて学習させた。\n",
    "中間層のニューロン数は 64 または 256 とした。\n",
    "符号化器=復号化器モデルに用いた理関連とニューラルネットワークとして GRU (Cho, et.al, 2014) を用いた。\n",
    "学習の評価には，負の対数尤度を用いた。\n",
    "学習は Adam (Kingma and Welling, 2015, arxiv:1412.6980) を用いて訓練した。\n",
    "このとき学習係数は 0.01，alpha, beta の値は PyTorch の既定値を用いた。\n",
    "各学習エポックでは，10,000 語のデータが用いれ，TLPA 語は学習には用いず，検証データして用いた。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 補足\n",
    "- 用いた TLPA 単語 (237 語)\n",
    "```\n",
    "'バス', '緑', '桜', 'のり巻き', '海苔巻', '五重塔', 'コップ', 'ごぼう', '土踏まず', '風呂', 'ヒトデ', 'ハム', 'うさぎ', '兎', 'ウサギ', 'ロープウェイ', '学校', 'ちりとり', '縁側', '歯', 'ねぎ', 'あじさい', '紫陽花', '灰色', '天井', '鍵', '肌色', 'ワニ', '鰐', '電車', '顔', '松', 'ガードレール', '柿', 'ちまき', '信号', 'ススキ', '薄', 'じょうろ', 'ジョウロ', 'コンセント', '天ぷら', 'てんぷら', '中指', 'ヨット', 'ピンク', 'フクロウ', 'ふくろう', 'みかん', '蜜柑', 'ミカン', '柱', '角砂糖', '犬', 'かご', '駕籠', 'バラ', '薔薇', '鍋', 'まぶた', 'くるみ', '黒', 'デパート', 'カーネーション', '城', 'アリ', '豆腐', 'ドライバー', '紺', '階段', '戦車', '人参', '背中', '鏡餅', 'スプーン', '朝顔', '金色', '足', 'ふすま', 'へび', '蛇', 'ヘビ', 'レモン', '公園', '乳母車', '床', '藤', 'ピンセット', 'トラック', 'いちご', '苺', 'イチゴ', '黄土色', '銭湯', 'ナマズ', 'ソバ', '蕎麦', 'おなか', 'お腹', 'オレンジ', 'バター', '工場', 'ハト', '鳩', '電卓', 'のど仏', '喉仏', 'チューリップ', '白菜', 'トラクター', '廊下', 'パトカー', '押し入れ', '鉛筆', '目尻', '芋', '吊橋', '赤', 'かき氷', '豹', 'サボテン', 'ピラミッド', 'サイ', '目', 'ひまわり', 'はたき', 'さしみ', '刺身', '玄関', 'トマト', '黄緑', '三輪車', 'にわとり', '鶏', 'つむじ', 'アスパラガス', 'ドア', '銀色', 'ウイスキー', '梅', 'タクシー', '動物園', '床の間', 'こげ茶', 'ぶどう', '葡萄', 'ブドウ', '飴', '毛虫', 'アイロン', '寺', 'そり', 'ひょうたん', '首', '消しゴム', '頬', 'イチョウ', 'いちょう', '駅', '餃子', '牛', 'びわ', '枇杷', '飛行機', '畳', '白', '竹', 'ペリカン', '紫', '手すり', '口', '大根', '風車', '鋏', 'ハサミ', '潜水艦', 'ステーキ', 'マッチ', '二階', '落花生', 'ごはん', 'ご飯', '自転車', '歩道橋', 'クジラ', '鯨', '茶色', 'あやめ', 'ふくらはぎ', 'もも', '桃', '鯛焼き', '道路', '靴べら', '水色', '壁', 'タンポポ', 'たんぽぽ', 'いかだ', 'ヤギ', '山羊', '鼻', 'エビ', '海老', '台所', 'オートバイ', 'かぶ', '蕪', '柳', 'しゃもじ', 'まんじゅう', '饅頭', 'かかと', '薄紫', '家', 'おせち料理', '青', '傘', 'つくし', 'リンゴ', '林檎', '馬車', '線路', 'タツノオトシゴ', '耳', '便所', 'レンコン', '蓮根', '猫', '黄色', 'へそ', '街灯', '障子', '酒', '船', '安全ピン', 'もみじ'\n",
    "```\n",
    "\n",
    "- 用いた音表現\n",
    "```\n",
    "'<EOW>', '<SOW>', '<UNK>', '<PAD>', 'n', 'o', 'h', 'a', 'i', 't', 'g', 'r', 'u', 'd', 'e', 'sh', 'q', 'm', 'k', 's', 'y', 'ch', 'p', 'N', '', 'b', 'ts', 'o:', 'ky', 'f', 'w', 'ry', 'gy', 'u:', 'z', 'j', 'py', 'hy', 'i:', 'e:', 'a:', 'by', 'ny', 'my', 'dy'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. ハイパーパラメータの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'traindata_size': 10000, 'epochs': 20, 'hidden_size': 256, 'loss_func': CrossEntropyLoss(), 'optim_func': <class 'torch.optim.adam.Adam'>, 'lr': 0.001, 'dropout_p': 0.1}\n"
     ]
    }
   ],
   "source": [
    "import torch.nn\n",
    "import torch.optim\n",
    "\n",
    "params = {\n",
    "    'traindata_size': 10000,\n",
    "    'epochs': 20,\n",
    "    'hidden_size': 256, \n",
    "    'loss_func' :torch.nn.CrossEntropyLoss(), # 交差エントロピー損失\n",
    "    #'loss_func': torch.nn.NLLLoss(),         # 負の対数尤度損失   \n",
    "    #'optim_func': torch.optim.SGD\n",
    "    #'optim_func': torch.optim.AdamW\n",
    "    'optim_func': torch.optim.Adam, \n",
    "    'lr': 0.001,                              # 学習率\n",
    "    'dropout_p': 0.1,                         # ドロップアウト率\n",
    "}\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 準備作業"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import typing\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from termcolor import colored\n",
    "\n",
    "# from tqdm import tqdm         #commandline で実行時\n",
    "from tqdm.notebook import tqdm  #jupyter で実行時\n",
    "\n",
    "import platform\n",
    "isColab = 'google.colab' in str(get_ipython())\n",
    "if isColab:\n",
    "    !pip install Levenshtein\n",
    "    !pip install jaconv\n",
    "    !pip install japanize_matplotlib\n",
    "\n",
    "import Levenshtein    \n",
    "import jaconv    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 上間先生からいただいた TLPA 文字データをエクセルファイルから読み込む\n",
      "#NTT日本語語彙特性より頻度情報を取得\n",
      "# 訓練に用いる単語の選定 10000 語\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c440dc1863bd4951b1e174ed65cad6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/158824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m訓練データサイズ:10000 語。NTT 日本語語彙特性の頻度上位 10000 語\u001b[0m\n",
      "ただし，TLPA に用いられる単語は含まない\n",
      "\u001b[1m\u001b[34m検証データサイズ:237 TLPA 単語数\u001b[0m\n",
      "\u001b[1m\u001b[34m音素数:47\u001b[0m\n",
      "\u001b[1m\u001b[34m書記素数:1887\u001b[0m\n",
      "\u001b[1m\u001b[34m最長書記素(文字)数:11, 最長音素数:28\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 自作ライブラリの読み込み\n",
    "if isColab:\n",
    "    ![ -d ccap ] & /bin/rm -rf ccap\n",
    "    !git clone https://github.com/ShinAsakawa/ccap.git\n",
    "\n",
    "from ccap import ccap_w2v\n",
    "w2v = ccap_w2v(is2017=False, isColab=False).w2v\n",
    "\n",
    "import MeCab\n",
    "from ccap.mecab_settings import yomi\n",
    "\n",
    "from ccap.tlpa_o2p import TLPA\n",
    "tlpa = TLPA(traindata_size=params['traindata_size'])\n",
    "print(colored(f'訓練データサイズ:{len(tlpa.training_data)} 語。NTT 日本語語彙特性の頻度上位 {len(tlpa.training_data)} 語','blue',attrs=['bold']))\n",
    "print('ただし，TLPA に用いられる単語は含まない')\n",
    "print(colored(f'検証データサイズ:{len(tlpa.tlpa_data)} TLPA 単語数','blue',attrs=['bold']))\n",
    "print(colored(f'音素数:{len(tlpa.phone_vocab)}','blue',attrs=['bold']))\n",
    "print(colored(f'書記素数:{len(tlpa.ortho_vocab)}','blue',attrs=['bold']))\n",
    "print(colored(f'最長書記素(文字)数:{tlpa.max_ortho_length}, 最長音素数:{tlpa.max_phone_length}', 'blue',attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.1 補足説明\n",
    "\n",
    "### 著作権に関する注意事項\n",
    "\n",
    "上で読み込んだ `tlpa` には，tlpa の絵画命名検査に用いられる 200 図版の単語情報と，NTT 日本語語彙特性の頻度情報が含まれています。\n",
    "本コードを再配布する際には，これら 2 つ，TLPA と NTT 日本語語彙特性データの著作権にご留意ください。\n",
    "\n",
    "### ライブラリの使い方\n",
    "\n",
    "今回作成したライブラリには以下のものが含まれる\n",
    "\n",
    "- `tlpa.vocab`: 登録されている全単語 159061 語のリスト\n",
    "- `tlpa.phone_vocab`: julius 表記した音表現のリスト。音であるから vocab ではなく phoneme などとすべきだったかも知れないが，特殊トークンを含む 47 音表現\n",
    "- `tlpa.ortho_vocab`: 書記素表現のリスト，上の phone_vocab 同様に vocab という表記は適切ではなく，grapheme などと表記すべきだが，自然言語の文脈で考えていたので，このよな表記となった。\n",
    "- `ntt_orth2hira`: NTT 日本語語彙特性に現れる表記を，ひらがなに変換する辞書 `tlpa.ntt_orth2hira[\"神経\"]` などとすれば，`シンケイ` という読みを得る\n",
    "- `tlpa.tlpa`: tlpa で用いられる単語 237 語の，音表現，カタカナ，意味表現(word2vec) \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# 再現性確保のため，乱数の種を設定する\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# リソースの選択（CPU/GPU）\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 乱数シード固定（再現性の担保）\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)  # for random\n",
    "    np.random.seed(seed) # for numpy\n",
    "\n",
    "    # for pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 42\n",
    "fix_seed(seed)\n",
    "\n",
    "# データローダーのサブプロセスの乱数の seed が固定\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "print(worker_init_fn(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "class _train_dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    上で読み込んだ自作データ管理ライブラリ TLPA でも良いので冗長なのだが，訓練データセットと検証データセットとを明示的に定義しておく\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, tlpa=tlpa)->None:\n",
    "        self.tlpa = tlpa\n",
    "        self.data = tlpa.training_data\n",
    "        self.order = {i:self.data[x] for i, x in enumerate(self.data)}\n",
    "        \n",
    "    def __len__(self)->int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, x:int):\n",
    "        return self.order[x]['ortho_ids'] + [self.tlpa.ortho_vocab.index('<EOW>')], self.order[x]['phone_ids'] + [self.tlpa.phone_vocab.index('<EOW>')]\n",
    "    \n",
    "    def convert_ortho_ids_to_tokens(self, ids:list):\n",
    "        return [self.tlpa.ortho_vocab[idx] for idx in ids]\n",
    "    \n",
    "    def convert_phone_ids_to_tokens(self, ids:list):\n",
    "        return [self.tlpa.phone_vocab[idx] for idx in ids]\n",
    "\n",
    "\n",
    "class _val_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tlpa=tlpa)->None:\n",
    "        self.tlpa = tlpa\n",
    "        self.data = tlpa.tlpa_data\n",
    "        self.order = {i:self.data[x] for i, x in enumerate(self.data)}\n",
    "        \n",
    "    def __len__(self)->int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, x:int):\n",
    "        return self.order[x]['ortho_ids'] + [self.tlpa.ortho_vocab.index('<EOW>')], self.order[x]['phone_ids'] + [self.tlpa.phone_vocab.index('<EOW>')]\n",
    "        #return self.order[x]['ortho_ids'], self.order[x]['phone_ids']\n",
    "    \n",
    "    def convert_ortho_ids_to_tokens(self, ids:list):\n",
    "        return [self.tlpa.ortho_vocab[idx] for idx in ids]\n",
    "    \n",
    "    def convert_phone_ids_to_tokens(self, ids:list):\n",
    "        return [self.tlpa.phone_vocab[idx] for idx in ids]\n",
    "    \n",
    "train_dataset = _train_dataset()\n",
    "val_dataset = _val_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 学習に用いる，符号化器 (エンコーダ)，復号化器 (デコーダ) の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    '''RNNによる符号化器'''\n",
    "    def __init__(self, n_inp, n_hid):\n",
    "        super().__init__()\n",
    "        self.n_hid = n_hid\n",
    "\n",
    "        self.embedding = nn.Embedding(n_inp, n_hid)\n",
    "        self.gru = nn.GRU(n_hid, n_hid)\n",
    "\n",
    "    def forward(self, inp, hid):\n",
    "        embedded = self.embedding(inp).view(1, 1, -1)\n",
    "        out = embedded\n",
    "        out, hid = self.gru(out, hid)\n",
    "        return out, hid\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.n_hid, device=device)\n",
    "\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    '''注意付き復号化器の定義'''\n",
    "    def __init__(self, n_hid, n_out, dropout_p=0.1, max_length=tlpa.max_phone_length+1):\n",
    "        super().__init__()\n",
    "        self.n_hid = n_hid\n",
    "        self.n_out = n_out\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_out, self.n_hid)\n",
    "        self.attn = nn.Linear(self.n_hid * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.n_hid * 2, self.n_hid)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.n_hid, self.n_hid)\n",
    "        self.out = nn.Linear(self.n_hid, self.n_out)\n",
    "\n",
    "    def forward(self, inp, hid, encoder_outputs):\n",
    "        embedded = self.embedding(inp).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hid[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        out = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        out = self.attn_combine(out).unsqueeze(0)\n",
    "\n",
    "        out = F.relu(out)\n",
    "        out, hid = self.gru(out, hid)\n",
    "\n",
    "        out = F.log_softmax(self.out(out[0]), dim=1)\n",
    "        return out, hid, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.n_hid, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 訓練関数 `train()` の定義 教師強制付き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ids2tensor(sentence_ids):\n",
    "    return torch.tensor(sentence_ids, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "teacher_forcing_ratio = 0.5  # 教師強制率。文献によっては，訓練中にこの値を徐々に減衰させることも行われます\n",
    "\n",
    "def train(input_tensor, \n",
    "          target_tensor, \n",
    "          encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, \n",
    "          criterion, max_length=tlpa.max_phone_length+1):\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden() # 符号化器の中間層を初期化\n",
    "    encoder_optimizer.zero_grad()         # 符号化器の最適化関数の初期化\n",
    "    decoder_optimizer.zero_grad()         # 復号化器の最適化関数の初期化\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.n_hid, device=device)\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[train_dataset.tlpa.ortho_vocab.index('<SOW>')]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    # 教師強制をするか否かを確率的に決める\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing: # 教師強制する場合 Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else: # 教師強制しない場合 Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == train_dataset.tlpa.ortho_vocab.index('<EOW>'):\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    \"\"\"時間変数を見やすいように，分と秒に変換して返す\"\"\"\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f'{int(m):2d}分 {int(s):2d}秒'\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    \"\"\"開始時刻 since と，現在の処理が全処理中に示す割合 percent を与えて，経過時間と残り時間を計算して表示する\"\"\"\n",
    "    now = time.time()  #現在時刻を取得\n",
    "    s = now - since    # 開始時刻から現在までの経過時間を計算\n",
    "    #s = since - now    \n",
    "    es = s / (percent) # 経過時間を現在までの処理割合で割って終了予想時間を計算\n",
    "    rs = es - s        # 終了予想時刻から経過した時間を引いて残り時間を計算\n",
    "\n",
    "    return f'経過時間:{asMinutes(s)} (残り時間 {asMinutes(rs)})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. `fit()` 関数の定義 エポックを反復して `train()` を呼び出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(encoder:nn.Module, \n",
    "        decoder:nn.Module, \n",
    "        epochs:int=params['epochs'],\n",
    "        lr:float=params['lr'],\n",
    "        n_sample:int=3)->list:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    #encoder_optimizer = optim.SGD(encoder.parameters(), lr=lr)\n",
    "    #decoder_optimizer = optim.SGD(decoder.parameters(), lr=lr)\n",
    "    #encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    #decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "    encoder_optimizer = params['optim_func'](encoder.parameters(), lr=lr)\n",
    "    decoder_optimizer = params['optim_func'](decoder.parameters(), lr=lr)\n",
    "    criterion = params['loss_func']\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        #エポックごとに学習順をシャッフルする\n",
    "        learning_order = np.random.permutation(train_dataset.__len__())\n",
    "        for i in range(train_dataset.__len__()):\n",
    "            x = learning_order[i]   # ランダムにデータを取り出す \n",
    "            inputs, targets = train_dataset.__getitem__(x)\n",
    "            input_tensor = convert_ids2tensor(inputs)\n",
    "            target_tensor = convert_ids2tensor(targets)\n",
    "            \n",
    "            #訓練の実施\n",
    "            loss = train(input_tensor, target_tensor, \n",
    "                         encoder, decoder, \n",
    "                         encoder_optimizer, decoder_optimizer, \n",
    "                         criterion)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        losses.append(epoch_loss/train_dataset.__len__())\n",
    "        print(colored(f'エポック:{epoch:2d} 損失:{epoch_loss/train_dataset.__len__():.2f}', 'cyan', attrs=['bold']),\n",
    "              f'{timeSince(start_time, (epoch+1) * train_dataset.__len__()/(epochs * train_dataset.__len__()))}')\n",
    "        \n",
    "        evaluateRandomly(encoder, decoder, n=n_sample)\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 評価関数 `evaluate()` の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder:nn.Module, \n",
    "             decoder:nn.Module, \n",
    "             input_ids:list, \n",
    "             max_length:int=tlpa.max_phone_length+1)->(list,torch.LongTensor):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = convert_ids2tensor(input_ids)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.n_hid, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[val_dataset.tlpa.phone_vocab.index('<SOW>')]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words, decoded_ids = [], []  # decoded_ids を追加\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            decoded_ids.append(int(topi.squeeze().detach())) # decoded_ids に追加\n",
    "            if topi.item() == val_dataset.tlpa.phone_vocab.index('<EOW>'):\n",
    "                decoded_words.append('<EOW>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(val_dataset.tlpa.phone_vocab[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoded_ids, decoder_attentions[:di + 1]  # decoded_ids を返すように変更\n",
    "        #return decoded_words, decoder_attentions[:di + 1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder:nn.Module, \n",
    "                     decoder:nn.Module, \n",
    "                     n:int=5)->float:\n",
    "    \n",
    "    srcs, preds = [], []\n",
    "    for x in np.random.randint(val_dataset.__len__(), size=n):\n",
    "        input_ids, target_ids = val_dataset.__getitem__(x)\n",
    "        input_words = val_dataset.convert_ortho_ids_to_tokens(input_ids)\n",
    "        print(f'入力: {target_ids}<-{input_ids}:{input_words}')\n",
    "        output_words, output_ids, attentions = evaluate(encoder, decoder, input_ids)\n",
    "        #output_ids, attentions = evaluate(encoder, decoder, input_ids)\n",
    "        #output_word = val_dataset.convert_phone_ids_to_tokens(output_ids)\n",
    "        #output_word = output_ids\n",
    "\n",
    "        srcs.append(input_words)\n",
    "        preds.append(output_words)\n",
    "        print(f'出力: {output_ids}',f':{output_words}')\n",
    "        print('---')\n",
    "    return srcs, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluateRandomly(encoder,decoder,n=5)\n",
    "#val_dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 学習の実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hidden_size = params['hidden_size']\n",
    "encoder = EncoderRNN(len(tlpa.ortho_vocab), hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(n_hid=hidden_size, n_out=len(tlpa.phone_vocab), dropout_p=params['dropout_p']).to(device)\n",
    "\n",
    "losses = []\n",
    "losses = losses + fit(encoder, decoder, epochs=params['epochs'], n_sample=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = losses + fit(encoder, decoder, epochs=params['epochs'], n_sample=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 学習経過の描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points:list)->None:\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # this locator puts ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "showPlot(losses)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluateRandomly(encoder, decoder, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 自由入力による評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(inp_word:str, pad:bool=False)->list:\n",
    "    if pad:\n",
    "        ret = [tlpa.ortho_vocab.index('<PAD>') for _ in range(tlpa.max_ortho_length - len(inp_word))]\n",
    "    else:\n",
    "        ret = []\n",
    "    return ret + [tlpa.ortho_vocab.index(ch) if ch in tlpa.ortho_vocab else tlpa.ortho_vocab[tlpa.ortho_vocab.index('<UNK>')] for ch in inp_word]\n",
    "\n",
    "\n",
    "#print(train_dataset.convert_ortho_ids_to_tokens(tokenize('わたし',pad=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_free_input(encoder:nn.Module, \n",
    "                        decoder:nn.Module,\n",
    "                        inp=None,\n",
    "                       )->None:\n",
    "    if inp == None:\n",
    "        inp = input()\n",
    "    inp = jaconv.normalize(inp)\n",
    "    inputs = tokenize(inp, pad=False)\n",
    "    input_ids = inputs # ['input_ids']\n",
    "    output_tokens, output_words, attentions = evaluate(encoder, decoder, input_ids)\n",
    "    #output_ids = val_dataset.convert_phone_ids_to_tokens(output_tokens)\n",
    "    output_ids = output_tokens\n",
    "    return input_ids, output_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inp, out = evaluate_free_input(encoder,decoder, inp='お前は虎だ。虎になるのだ。')\n",
    "inp, out = evaluate_free_input(encoder,decoder)\n",
    "print(val_dataset.convert_ortho_ids_to_tokens(inp))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 エンコーダの内部表現の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_an_encoder_representation(encoder:nn.Module,\n",
    "                                  input_ids:list,\n",
    "                                  max_length:int=tlpa.max_phone_length+1)->(list,torch.LongTensor):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = convert_ids2tensor(input_ids)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.n_hid, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        return encoder_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((val_dataset.__len__(),hidden_size))\n",
    "for i in range(val_dataset.__len__()):\n",
    "    x = get_an_encoder_representation(encoder, input_ids = val_dataset.__getitem__(1)[0])\n",
    "    X[i] = x.squeeze(0).clone().detach().numpy()[0]\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccap.tsne as tsne\n",
    "\n",
    "tsne_result = tsne.tsne(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 内部表現の描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_result.shape\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(tsne_result[:,0],tsne_result[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = tsne_result[:,0].argmin()\n",
    "x_max = tsne_result[:,0].argmax()\n",
    "y_min = tsne_result[:,1].argmin()\n",
    "y_max = tsne_result[:,1].argmax()\n",
    "\n",
    "print(f'X 座標最小値:{tlpa.vocab[x_min]}')\n",
    "print(f'X 座標最大値:{tlpa.vocab[x_max]}')\n",
    "print(f'Y 座標最小値:{tlpa.vocab[y_min]}')\n",
    "print(f'Y 座標最大値:{tlpa.vocab[y_max]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlpa.vocab[x_min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved = '2022_0212tlpa_o2p.pt'\n",
    "torch.save({'encoder':encoder.state_dict(),\n",
    "            'decoder':decoder.state_dict()}, path_saved)\n",
    "checkpoint = torch.load(path_saved)\n",
    "encoder2 = EncoderRNN(len(tlpa.ortho_vocab), hidden_size).to(device)\n",
    "decoder2 = AttnDecoderRNN(n_hid=hidden_size, n_out=len(tlpa.phone_vocab), dropout_p=params['dropout_p']).to(device)\n",
    "encoder2.load_state_dict(checkpoint['encoder'])\n",
    "decoder2.load_state_dict(checkpoint['decoder'])\n",
    "encoder2.eval()\n",
    "decoder2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluateRandomly(encoder2, decoder2, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "050bfdc7fc2f48c08d4e2dd3597147e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "11805a0d6eb44866af894698fcf69ffa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b5eb182f9b7241d989e459d085c86eab",
       "style": "IPY_MODEL_050bfdc7fc2f48c08d4e2dd3597147e5",
       "value": " 10069/158824 [00:00&lt;00:09, 16185.36it/s]"
      }
     },
     "3bf171308fa8441bb49ea706a0c84c7b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8940b0e9c32a4586b01ed0efa53fb348": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_eccfc75531e546c781ddc724d8352eb6",
       "max": 158824,
       "style": "IPY_MODEL_ee74b369c1134ce5a6d1b96b9ae234c9",
       "value": 10069
      }
     },
     "b5eb182f9b7241d989e459d085c86eab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c17766ab429e4522a384590d7e58e93a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c440dc1863bd4951b1e174ed65cad6ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c7f75829706644a8afe7f6ce91ab7017",
        "IPY_MODEL_8940b0e9c32a4586b01ed0efa53fb348",
        "IPY_MODEL_11805a0d6eb44866af894698fcf69ffa"
       ],
       "layout": "IPY_MODEL_c17766ab429e4522a384590d7e58e93a"
      }
     },
     "c7f75829706644a8afe7f6ce91ab7017": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3bf171308fa8441bb49ea706a0c84c7b",
       "style": "IPY_MODEL_ebf8e2ee47f24ddb8e9f05d234f29441",
       "value": "  6%"
      }
     },
     "ebf8e2ee47f24ddb8e9f05d234f29441": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "eccfc75531e546c781ddc724d8352eb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ee74b369c1134ce5a6d1b96b9ae234c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
