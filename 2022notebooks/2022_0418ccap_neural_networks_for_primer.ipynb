{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022_0418ccap_neural_networks_for_primer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO1VKldLgwj0EETeB9jYskO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/2022notebooks/2022_0418ccap_neural_networks_for_primer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2022_0418ccap 資料\n",
        "\n",
        "- filename: `2022_0418ccap_neural_networks_for_primer.ipynb`"
      ],
      "metadata": {
        "id": "HDVJ_F4_vgMk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtN7zhpzu_pV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python\n",
        "\n",
        "[Python](https://www.python.org/) は\n",
        "    \n",
        "- 基本データ型: リスト，辞書，集合，タプル\n",
        "- 関数，クラス\n",
        "- [Numpy](https://numpy.org/)\n",
        "- [Scipy](https://scipy.org/)\n",
        "- [Matplotlib](https://matplotlib.org/)\n",
        "- [PyTorch](https://pytorch.org/)\n",
        "\n",
        "Python は高級言語である。\n",
        "スクリプト言語 (awk, Perl, シェルスクリプト, R) と比較される疑似言語と捉える場合もある。\n",
        "このことは，数行のコードで高度な算法を行えることを意味する。\n",
        "例えば古典的なクイックソートアルゴリズムは Python では以下のようになる:\n",
        "\n",
        "```python\n",
        "def quicksort(arr):\n",
        "    if len(arr) <= 1:\n",
        "        return arr\n",
        "    pivot = arr[int(len(arr) / 2)]\n",
        "    left = [x for x in arr if x < pivot]\n",
        "    middle = [x for x in arr if x == pivot]\n",
        "    right = [x for x in arr if x > pivot]\n",
        "    return quicksort(left) + middle + quicksort(right)\n",
        "print(quicksort([3,6,8,10,1,2,1]))\n",
        "# 印字( \"[1, 1, 2, 3, 6, 8, 10]\")\n",
        "```\n",
        "\n",
        "上記コードを理解するためには，`def`, `if`, `len`, `return`, `arr`, `for`, `print` と言った Python の予約語を覚える必要があるだろう。\n",
        "\n",
        "駄菓子菓子，Python の大きな特徴は，インデント (indent 字下げ) に敏感 (indent sensitive) であることだろう。\n",
        "\n",
        "    ちなみに，字下げ幅は，一貫していれば良い。\n",
        "    上例の字下げ幅は 4 である。\n",
        "    コード作成者の好みだが，字下げ幅 2 を好むプログラマもいる。\n",
        "    字下げ幅 8 を好むプログラマもいる。\n",
        "\n",
        "上記の例では，関数を定義するために `def` が用いられている。\n",
        "この `def` の範囲が字下げで示されている。\n",
        "字下げの有効範囲からは，上例下部の `print` 文は `def quicksort():` で始まる関数定義の範囲外である。\n",
        "\n",
        "<!-- その他は，[Python の基礎](https://komazawa-deep-learning.github.io//python_numpy_intro_ja/) 参照のこと -->\n"
      ],
      "metadata": {
        "id": "87dMtpNPvcLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep learning\n",
        "- Yann LeCun, Yoshua Bengio & Geoffrey Hinton\n",
        "- doi:10.1038/nature14539\n",
        "- 436, NATURE, Vol. 521, 28 MAY 2015\n",
        "-\n",
        "REVIEW doi:10.1038/nature14539\n",
        "\n",
        "<center>\n",
        "<div style=\"align:center\">    \n",
        "<img src=\"https://project-ccap.github.io/figures/2015LeCun_Bengio_Hinton_fig1ab.png\" width=\"77%\">\n",
        "</center>\n",
        "    \n",
        "<div sytle=\"text-align:left; width:77%; background-color:cornsilk\">\n",
        "\n",
        "図 1.  多層ニューラルネットとバックプロパゲーション。\n",
        "* a)  多層ニューラルネット (つながった点で示す) は入力空間を歪め，データのクラス (赤と青の線上の例) を線形分離可能にすることができる。\n",
        "入力空間の規則正しい格子 (左図) が，隠れ層ユニットによってどのように変換されるか (中央図) にも注目。\n",
        "この図 は 2 入力層ユニット，2 隠れ層ユニット，1 出力層ユニットを持つ例である。\n",
        "物体認識や自然言語処理に用いられるネットワークは数万から数十万のユニットを持つ。\n",
        "C. Olah (http://colah.github.io/) の許可を得て複製。\n",
        "\n",
        "<!-- Figure 1. Multilayer neural networks and backpropagation.\n",
        "* a, A multilayer neural network (shown by the connected dots) can distort the input space to make the classes of data (examples of which are on the red and blue lines) linearly separable.\n",
        "Note how a regular grid (shown on the left) in input space is also transformed (shown in the middle panel) by hidden units.\n",
        "This is an illustrative example with only two input units, two hidden units and one output unit, but the networks used for object recognition or natural language processing contain tens or hundreds of thousands of units. Reproduced with permission from C. Olah (http://colah.github.io/).  -->\n",
        "\n",
        "* b)  微分の連鎖則 (合成関数の微分公式のこと) は，2 つの小さな影響 ($x$ の小さな変化が $y$ に及ぼす影響と $y$ の小さな変化が $z$ に及ぼす影響) がどのように構成されるかを教えてくれる。\n",
        "$x$ の小さな変化 $\\Delta x$ は $\\partial y/\\partial x$ を掛けられることによって，まず $y$ の小さな変化 $\\Delta y$ に変換される (偏微分の定義)。\n",
        "同様に，変化量 $\\Delta y$ は $z$ の変化量 $\\Delta z$ を生み出す。\n",
        "一方の式を他方の式に代入すると，微分の連鎖法則，つまり $\\partial y/\\partial x$ と $\\partial x$ の積の掛け算で $\\Delta x$ が $\\Delta z$ になることがわかる。\n",
        "$x, y, z$ がベクトル (そして導関数がヤコビアン Jacobian) であるときにも有効。\n",
        "\n",
        "<!-- * b, The chain rule of derivatives tells us how two small effects (that of a small change of x on y, and that of y on z) are composed.\n",
        "A small change Δx in x gets transformed first into a small change Δy in y by getting multiplied by ∂y/∂x (that is, the definition of partial derivative).\n",
        "Similarly, the change Δy creates a change Δz in z.\n",
        "Substituting one equation into the other gives the chain rule of derivatives — how Δx gets turned into Δz through multiplication by the product of ∂y/∂x and ∂z/∂x.\n",
        "It also works when x, y and z are vectors (and the derivatives are Jacobian matrices).  -->\n",
        "</div>\n",
        "</div>    "
      ],
      "metadata": {
        "id": "dRblnukbvIh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np                                # 必要となるライブラリの輸入\n",
        "lr, N_hid = 4, 8                                  # lr: 学習率, N_hid: 中間層数\n",
        "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ]) # 入力とバイアスの定義\n",
        "y = np.array([[0,1,1,0]]).T                       # ターゲットの定義\n",
        "Wh = np.random.random((X.shape[1], N_hid)) - 1/2  # 入力層から中間層への結合係数の初期化\n",
        "Wo = np.random.random((N_hid, y.shape[1])) - 1/2  # 中間層から出力層への結合係数の初期化\n",
        "for t in range(100):                              # 繰り返し\n",
        "    H  = np.tanh(np.dot(X, Wh))                   # 入力層から中間層への計算。ハイパータンジェント関数\n",
        "    _y = 1/(1. + np.exp(-(np.dot(H, Wo))))        # 中間層から出力層への計算。シグモイド関数\n",
        "    Dy = (y - _y) * (_y * (1. - _y))              # 誤差の微分\n",
        "    DH = Dy.dot(Wo.T) * (1. - H ** 2)             # 誤差逆伝播\n",
        "    Wo += lr * _y.T.dot(Dy)                       # 中間層から出力層への重み更新\n",
        "    Wh += lr * X.T.dot(DH)                        # 中間層から入力層への重み更新\n",
        "print(_y.T)                                       # 結果の出力"
      ],
      "metadata": {
        "id": "4u8AzZfnvCNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "X = np.array([ [0,0],[0,1],[1,0],[1,1] ])\n",
        "y = np.array([[0,1,1,0]]).T\n",
        "X = torch.Tensor(X)\n",
        "y = torch.Tensor(y)\n",
        "lr = 0.01\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, n_inp, n_hid, n_out=1):\n",
        "        super().__init__()\n",
        "        self.n_inp = n_inp\n",
        "        self.n_hid = n_hid\n",
        "        self.n_out = n_out\n",
        "        self.hid_layer = nn.Linear(in_features =self.n_inp,\n",
        "                                   out_features=self.n_hid,\n",
        "                                   bias=True)\n",
        "        self.out_layer = nn.Linear(self.n_hid, self.n_out)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hid_layer(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.out_layer(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "mlp = MLP(n_inp=2, n_hid=16, n_out=1)\n",
        "#loss_f = nn.MSELoss()\n",
        "#loss_f = nn.CrossEntropyLoss()\n",
        "loss_f = nn.BCELoss()\n",
        "#optim_f = torch.optim.SGD(mlp.parameters(),lr=lr)\n",
        "optim_f = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
        "\n",
        "mlp.eval()\n",
        "_y = mlp(X)\n",
        "_pre_loss = loss_f(_y, y)\n",
        "print(f'訓練開始前の損失値:    {_pre_loss.item():.3f}')\n",
        "\n",
        "mlp.train()\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    optim_f.zero_grad()\n",
        "\n",
        "    _y = mlp(X)                  # モデルに処理させて出力を得る\n",
        "    loss = loss_f(_y, y)         # 損失値の計算\n",
        "\n",
        "    if epoch % (epochs>>2) == 0: # 途中結果の出力\n",
        "        print(f'エポック:{epoch:4d}: 損失値:{loss.item():.3f}')\n",
        "\n",
        "    loss.backward()              # 誤差逆伝播\n",
        "    optim_f.step()               # 重み更新。すなわち学習\n",
        "\n",
        "mlp.eval()\n",
        "_y = mlp(X)\n",
        "print(f'最終結果:{_y.squeeze().detach().numpy()}')\n"
      ],
      "metadata": {
        "id": "ZJRfZGKjvBiw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}