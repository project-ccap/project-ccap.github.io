{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "2021_1010facial_keypoints_detection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/2021notebooks/2021_1010facial_keypoints_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COWiJRs4PJNy"
      },
      "source": [
        "# -*- coding: utf-8 -*-"
      ],
      "id": "COWiJRs4PJNy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "704ddafb-4063-4265-8cc0-f8bff781ddf1"
      },
      "source": [
        "---\n",
        "title: Getting Started with Facial Keypoint Detection using Deep Learning and PyTorch\n",
        "author: Sovit Ranjan \n",
        "date: 2020October\n",
        "source: https://debuggercafe.com/getting-started-with-facial-keypoint-detection-using-pytorch/\n",
        "\n",
        "---\n",
        "\n",
        "- [Kaggle](https://www.kaggle.com/c/facial-keypoints-detection/data)\n",
        "- Kaggle からデータ入手\n",
        "\n",
        "```bash\n",
        "kaggle competitions download -c facial-keypoints-detection\n",
        "```"
      ],
      "id": "704ddafb-4063-4265-8cc0-f8bff781ddf1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5mtWAnCKfte"
      },
      "source": [
        "%%shell\n",
        "curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1r1XxfBOQMzfhaohgg6aq-KbsYACkiWNa\" > /dev/null\n",
        "CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"  \n",
        "curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1r1XxfBOQMzfhaohgg6aq-KbsYACkiWNa\" -o kaggle_facial_keypoints_detection.tar.gz\n"
      ],
      "id": "w5mtWAnCKfte",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP-_lzWyMg2x"
      },
      "source": [
        "!gunzip kaggle_facial_keypoints_detection.tar.gz\n",
        "!tar -xf kaggle_facial_keypoints_detection.tar"
      ],
      "id": "uP-_lzWyMg2x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d5561a8-dd37-4620-9eec-da286caf40e2"
      },
      "source": [
        "# 深層学習とPyTorchを使った顔のキーポイント検出\n",
        "\n",
        "PyTorch を使った顔のキーポイント検出デモ\n",
        "\n",
        "<center>\n",
        "<img src=\"https://debuggercafe.com/wp-content/uploads/2020/10/intro_exmp.png\" width=\"33%\"><br/>\n",
        "<p style=\"text-align:left;width:88%; background-color:cornsilk\">\n",
        "Figure 1. An example of facial keypoint detection using deep learning and PyTorch. We will try to achieve similar results after going through this tutorial.\n",
        "</p>\n",
        "</center>\n",
        "\n",
        "図 1 は濃淡画像上での顔のキーポイント検出の例です。\n",
        "このチュートリアルの終わりまでに， 同様の結果を得ることを目標としています。\n",
        "\n",
        "* 顔のキーポイント検出の必要性を簡単に紹介します。\n",
        "* 深層学習と PyTorch を使って、顔のキーポイント検出を始めるための簡単なデータセットを使用しています。\n",
        "* シンプルな畳み込みニューラルネットワークモデルを使って、データセット上で訓練を行います。\n",
        "* 次に、訓練されたモデルを使って、テストデータセットの未見の画像の顔のキーポイントを検出します。\n",
        "* 最後に、メリット、デメリット、さらなる実験と改善のために取るべきステップにたどり着きます。\n",
        "\n",
        "# 1. なぜ顔のキーポイント検出が必要なのか？\n",
        "\n",
        "\n",
        "先に進む前に、素朴な疑問に答えてみましょう。\n",
        "なぜ、顔のキーポイント検出のような技術が必要なのか？たくさんありますが、いくつかをご紹介します。\n",
        "\n",
        "スマートフォンのアプリで， フィルターを見たことがある人も多いのではないでしょうか。\n",
        "このようなフィルターを顔に正確に適用するためには，人の顔のキーポイント (注目点) を正しく判断する必要があります。\n",
        "そのためには， 顔のキーポイントを検出する必要があります。\n",
        "また、顔のキーポイント検出は、人の年齢を判定するのにも利用できます。\n",
        "実際、多くの産業や企業がこの技術を利用しています。\n",
        "顔認証によるスマートフォンのロック解除にも、顔のキーポイント検出が使われています。\n",
        "上記は、実際の使用例の一部に過ぎません。\n",
        "他にもたくさんありますが、それらの詳細については今は触れません。\n",
        "もっと詳しく知りたい方は、ユースケースについてより多くのポイントを説明した[こちらの記事をお読みください](https://www.facefirst.com/blog/amazing-uses-for-face-recognition-facial-recognition-use-cases/)。\n",
        "\n",
        "上述したように、このチュートリアルでは、顔のキーポイント検出にディープラーニングを使用します。\n",
        "深層学習と畳み込みニューラルネットワークは、現在、顔認識とキーポイント検出の分野で大きな役割を果たしています。"
      ],
      "id": "4d5561a8-dd37-4620-9eec-da286caf40e2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "9c118bc7-ee19-4e29-8aa9-8e0ec6765721"
      },
      "source": [
        "## 1.1 データセット\n",
        "<!-- ## 1.1 The Dataset-->\n",
        "\n",
        "過去に開催された Kaggle のコンペティションのデータセットを使用します。\n",
        "競技名は Facial Keypoints Detection です。\n",
        "競技のルールに同意した後、データセットをダウンロードするように言われたら、ダウンロードしてください。\n",
        "\n",
        "データセットは大きくありません。約 80 MBしかありません。\n",
        "訓練データセットとテストデータセットを含む CSV ファイルで構成されています。\n",
        "画像も CSV ファイルの中にピクセル値で入っています。\n",
        "画像はすべて 96×96 次元の濃淡画像です。\n",
        "濃淡画像で次元が小さいため、深層学習による顔のキーポイント検出を始めるのに適した、簡単なデータセットです。\n",
        "\n",
        "このデータセットには (x, y) 形式の 15 個の座標特徴のキーポイントが含まれています。\n",
        "つまり、各顔画像には合計 30 個のポイント特徴があるということです。\n",
        "すべてのデータポイントは CSV ファイルの異なる列に入っており、最後の列には画像のピクセル値が入っています。\n",
        "\n",
        "<!--\n",
        "We will use a dataset from one of the past Kaggle competitions. \n",
        "The competition is Facial Keypoints Detection. Go ahead and download the dataset after accepting the competition rules if it asks you to do so.\n",
        "\n",
        "The dataset is not big. It is only around 80 MB. \n",
        "It consists of CSV files containing the training and test dataset. \n",
        "The images are also within the CSV files with the pixel values. \n",
        "All the images are 96×96 dimensional grayscale images. \n",
        "As the images are grayscale and small in dimension, that is why it is a good and easy dataset to start with facial keypoint detection using deep learning.\n",
        "\n",
        "The dataset contains the keypoints for 15 coordinate features in the form of (x, y). \n",
        "So, there are a total of 30 point features for each face image. \n",
        "All the data points are in different columns of the CSV file with the final column holding the image pixel values.-->\n",
        "\n",
        "次のコードスニペットは CSV ファイルのデータフォーマットを示しています。\n",
        "<!-- The following code snippet shows the data format in the CSV files. -->\n",
        "\n",
        "\n",
        "<pre style=\"background-color:powderblue\">\n",
        "left_eye_center_x  left_eye_center_y  right_eye_center_x  ...  mouth_center_bottom_lip_x  mouth_center_bottom\n",
        "_lip_y                                              Image\n",
        "0             66.033564          39.002274           30.227008  ...                  43.130707                \n",
        "  84.485774  238 236 237 238 240 240 239 241 241 243 240 23...\n",
        "1             64.332936          34.970077           29.949277  ...                  45.467915                \n",
        "  85.480170  219 215 204 196 204 211 212 200 180 168 178 19...\n",
        "2             65.057053          34.909642           30.903789  ...                  47.274947                \n",
        "  78.659368  144 142 159 180 188 188 184 180 167 132 84 59 ...\n",
        "3             65.225739          37.261774           32.023096  ...                  51.561183                \n",
        "  78.268383  193 192 193 194 194 194 193 192 168 111 50 12 ...\n",
        "4             66.725301          39.621261           32.244810  ...                  44.227141                \n",
        "  86.871166  147 148 160 196 215 214 216 217 219 220 206 18...\n",
        "...                 ...                ...                 ...  ...                        ...                \n",
        "        ...                                                ...\n",
        "7044          67.402546          31.842551           29.746749  ...                  50.426637                \n",
        "  79.683921  71 74 85 105 116 128 139 150 170 187 201 209 2...\n",
        "7045          66.134400          38.365501           30.478626  ...                  50.287397                \n",
        "  77.983023  60 60 62 57 55 51 49 48 50 53 56 56 106 89 77 ...\n",
        "7046          66.690732          36.845221           31.666420  ...                  49.462572                \n",
        "  78.117120  74 74 74 78 79 79 79 81 77 78 80 73 72 81 77 1...\n",
        "7047          70.965082          39.853666           30.543285  ...                  50.065186                \n",
        "  79.586447  254 254 254 254 254 238 193 145 121 118 119 10...\n",
        "7048          66.938311          43.424510           31.096059  ...                  45.900480                \n",
        "  82.773096  53 62 67 76 86 91 97 105 105 106 107 108 112 1...</pre>\n",
        "\n",
        "\n",
        "キーポイントとなる特徴的な列を見ることができます。\n",
        "このような列は、顔の左側と右側で30個あります。\n",
        "最後の列は、ピクセル値を示す「画像」の列です。\n",
        "これは文字列形式です。\n",
        "このように、データセットに深層学習技術を適用する前に、少しだけ前処理を行う必要があります。\n",
        "<!-- You can see the keypoint feature columns. \n",
        "There are 30 such columns for the left and right sides of the face. \n",
        "The last column is the Image column with the pixel values. \n",
        "They are in string format. \n",
        "So, we will have to do a bit of preprocessing before we can apply our deep learning techniques to the dataset. -->\n",
        "\n",
        "\n",
        "以下は、顔にキーポイントを設定した `training.csv` ファイルのサンプル画像です。\n",
        "<!-- The following are some sample images from the training.csv file with the keypoints on the faces. -->\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://debuggercafe.com/wp-content/uploads/2020/10/training_samples.png\" width=\"66%\"><br/>\n",
        "<p style=\"text-align:left;width:77%;background-color:cornsilk\">\n",
        "Figure 2. Some samples from the training set with their facial keypoints. \n",
        "We will use this dataset to train our deep neural network using PyTorch.</p>\n",
        "</center>\n",
        "\n",
        "また、このデータセットには多くの欠損値が含まれています。\n",
        "7048 個のインスタンス (行) のうち 4909 行は 1 つ以上の列で少なくとも 1 つの NULL 値を含んでいます。\n",
        "また、全てのキーポイントが揃っている完全なデータは 2140 行のみです。\n",
        "このような状況は、データセットを作成する際に対処しなければなりません。\n"
      ],
      "id": "9c118bc7-ee19-4e29-8aa9-8e0ec6765721"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb758281-2a4a-47bc-83bc-9fcff0123348"
      },
      "source": [
        "# 2. 深層学習とPyTorchによる顔のキーポイント検出\n",
        "<!-- # 2. Facial Keypoint Detection using Deep Learning and PyTorch -->\n",
        "\n",
        "PyTorch フレームワークを使った顔のキーポイント検出のためのコーディング作業に入っていきます。"
      ],
      "id": "bb758281-2a4a-47bc-83bc-9fcff0123348"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f0e99f2-f3c2-4ec8-ada4-01140e911480"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "!pip install japanize_matplotlib\n",
        "import japanize_matplotlib"
      ],
      "id": "1f0e99f2-f3c2-4ec8-ada4-01140e911480",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83133ecc-ab17-4be2-9054-7887dadd2787"
      },
      "source": [
        "import torch\n",
        "\n",
        "ROOT_PATH = 'kaggle_facial_keypoints_detection'\n",
        "!mkdir outputs\n",
        "OUTPUT_PATH = 'outputs'\n",
        "\n",
        "# learning parameters\n",
        "BATCH_SIZE = 256\n",
        "LR = 0.0001\n",
        "EPOCHS = 300\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# train/test split\n",
        "TEST_SPLIT = 0.2\n",
        "TEST_SPLIT = 0.1\n",
        "\n",
        "# show dataset keypoint plot\n",
        "SHOW_DATASET_PLOT = True"
      ],
      "id": "83133ecc-ab17-4be2-9054-7887dadd2787",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d330d13-aca8-4dd8-9c9e-375b5e3cebdf"
      },
      "source": [
        "トレーニングと検証のための学習パラメータは以下の通りです。\n",
        "\n",
        "* バッチサイズは 256 としています。\n",
        "画像のサイズが 96×96 と小さく、また濃淡画像であるため，ミニバッチサイズを大きくしてもメモリの問題は発生しません。\n",
        "ただし GPU のメモリに応じて、バッチサイズを自由に増減させてください。\n",
        "* 学習率は 0.0001 です。\n",
        "様々な学習率を試した結果、今回使用するモデルとデータセットでは、この学習率が最も安定していると思われます。\n",
        "* 顔のキーポイントのデータセットに対して 300 エポックでモデルを学習します。\n",
        "多いと思われるかもしれませんが、実際には， このように多くのエポックを行うことで， モデルは恩恵を受けます。\n",
        "* 0.2 のテスト分割を使用しています。\n",
        " データの 80 %をトレーニングに，20 %を検証に使用します。\n",
        "* SHOW_DATASET_PLOT が True の場合， 訓練直前に，いくつかの顔と，それに対応する顔のキーポイントのプロットが表示されます。\n",
        "必要であれば，これを False にしておくこともできます。\n",
        "\n",
        "## 2.1 深層学習と PyTorch による顔のキーポイント検出のためのユーティリティー関数の作成\n",
        "\n",
        "この節では、作業を容易にするためのユーティリティー関数をいくつか書きます。\n",
        "ユーティリティー関数は全部で 3 つあります。\n",
        "この 3 つのユーティリティー関数はすべて、顔の画像上に顔のキーポイントをプロットするのに役立ちます。\n",
        "しかし 3 つとも異なるシナリオに対応しています。\n",
        "1 つずつ取り組んでいきましょう。\n",
        "\n",
        "### 2.1.1 顔に検証キーポイントをプロットする関数\n",
        "\n",
        "まず、検証用のキーポイントをプロットする関数を紹介します。\n",
        "この関数を `valid_keypoints_plot()` と呼ぶことにします。\n",
        "この関数は基本的に，与えられた一定のエポック数の後に，画像の顔上に検証（回帰したキーポイント）をプロットします．\n",
        "\n",
        "まずはコードを書いてみましょう。その後、重要な部分の説明に入ります。\n"
      ],
      "id": "3d330d13-aca8-4dd8-9c9e-375b5e3cebdf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17e36b19-f532-4869-bd85-9f902a7f2ba4"
      },
      "source": [
        "def valid_keypoints_plot(image, outputs, orig_keypoints, epoch):\n",
        "    \"\"\"\n",
        "    This function plots the regressed (predicted) keypoints and the actual keypoints after each validation epoch for one image in the batch.\n",
        "    \"\"\"\n",
        "    # detach the image, keypoints, and output tensors from GPU to CPU\n",
        "    image = image.detach().cpu()\n",
        "    outputs = outputs.detach().cpu().numpy()\n",
        "    orig_keypoints = orig_keypoints.detach().cpu().numpy()\n",
        "\n",
        "    # just get a single datapoint from each batch\n",
        "    img = image[0]\n",
        "    output_keypoint = outputs[0]\n",
        "    orig_keypoint = orig_keypoints[0]\n",
        "    img = np.array(img, dtype=float)\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = img.reshape(96, 96)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "\n",
        "    output_keypoint = output_keypoint.reshape(-1, 2)\n",
        "    orig_keypoint = orig_keypoint.reshape(-1, 2)\n",
        "    for p in range(output_keypoint.shape[0]):\n",
        "        plt.plot(output_keypoint[p, 0], output_keypoint[p, 1], 'r.')\n",
        "        plt.text(output_keypoint[p, 0], output_keypoint[p, 1], f\"{p}\")\n",
        "        plt.plot(orig_keypoint[p, 0], orig_keypoint[p, 1], 'g.')\n",
        "        plt.text(orig_keypoint[p, 0], orig_keypoint[p, 1], f\"{p}\")\n",
        "    plt.savefig(f\"{OUTPUT_PATH}/val_epoch_{epoch}.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def test_keypoints_plot(images_list, outputs_list, figsize=(10,10)):\n",
        "    \"\"\"\n",
        "    This function plots the keypoints for the outputs and images\n",
        "    in `test.csv` file.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(len(images_list)):\n",
        "        outputs = outputs_list[i]\n",
        "        image = images_list[i]\n",
        "        outputs = outputs.cpu().detach().numpy()\n",
        "        outputs = outputs.reshape(-1, 2)\n",
        "        plt.subplot(3, 3, i+1)\n",
        "        #plt.imshow(image, cmap='gray')\n",
        "        plt.imshow(image.reshape(96,96), cmap='gray')\n",
        "\n",
        "        for p in range(outputs.shape[0]):\n",
        "                plt.plot(outputs[p, 0], outputs[p, 1], 'r.')\n",
        "                plt.text(outputs[p, 0], outputs[p, 1], f\"{p}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.savefig(f\"{OUTPUT_PATH}/test_output.pdf\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def dataset_keypoints_plot(data, figsize=(22,20), n_samples=30):\n",
        "    \"\"\"\n",
        "    This function shows the image faces and keypoint plots that the model will actually see. \n",
        "    This is a good way to validate that our dataset is in fact corrent and the faces align wiht the keypoint features. \n",
        "    The plot will be show just before training starts. Press `q` to quit the plot and start training.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(n_samples):\n",
        "        sample = data[i]\n",
        "        img = sample['image']\n",
        "        img = np.array(img, dtype=float)\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "        img = img.reshape(96, 96)\n",
        "        plt.subplot(5, 6, i+1)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        keypoints = sample['keypoints']\n",
        "        for j in range(len(keypoints)):\n",
        "            plt.plot(keypoints[j, 0], keypoints[j, 1], 'r.')\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "id": "17e36b19-f532-4869-bd85-9f902a7f2ba4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "317dde55-0d50-487c-beeb-d4a7e11934f5"
      },
      "source": [
        "最初の 2 行のコメントを読めば，この関数の要点が容易に理解できると思います。\n",
        "画像テンソル（`image`），出力テンソル（`outputs`），データセットからのオリジナルキーポイント（`orig_keypoints`）を，エポック番号とともに関数に渡します．\n",
        "<!-- If you read the comment in the first two lines then you will easily get the gist of the function. \n",
        "We provide the image tensors (`image`), the output tensors (`outputs`), and the original keypoints from the dataset (`orig_keypoints`) along with the epoch number to the function.-->\n",
        "\n",
        "* 7, 8, 9行目では GPU からデータを切り離し CPU にロードしています。\n",
        "* テンソルは，画像，予測されたキーポイント，オリジナルのキーポイントそれぞれについて 256 個のデータポイントを含むバッチの形をしています．\n",
        "12 行目から 14 行目で、それぞれの最初のデータポイントを取得します。\n",
        "* 次に，画像を NumPy の配列形式に変換し，チャンネルを最後にして転置し，元の 96×96 のサイズに整形します。\n",
        "そして，Matplotlib を使って画像をプロットします。\n",
        "* 21 行目と 22 行目では，予測されたキーポイントと元のキーポイントの形状を変更します．\n",
        "21 行目と 22 行目で，予測キーポイントとオリジナルキーポイントの形状を変更します．\n",
        "* 23 行目から 27 行目まで，顔の画像上に予測キーポイントとオリジナルキーポイントをプロットします。\n",
        "予測されたキーポイントは赤のドットで、オリジナルのキーポイントは緑のドットになります。\n",
        "また，`plt.text()`を用いて，対応するキーポイントの番号をプロットします。\n",
        "* 最後に，画像を `outputs` フォルダに保存します。\n",
        "\n",
        "<!--\n",
        "* At lines 7, 8, and 9 we detach the data from the GPU and load them onto the CPU.\n",
        "* The tensors are in the form of a batch containing 256 datapoints each for the image, the predicted keypoints, and the original keypoints. \n",
        "We get just the first datapoint from each from lines 12 to 14.\n",
        "* Then we convert the image to NumPy array format, transpose it make channels last, and reshape it into the original 96×96 dimensions. \n",
        "Then we plot the image using Matplotlib.\n",
        "* At lines 21 and 22, we reshape the predicted and original keypoints. \n",
        "This will make them have 2 columns along with the respective number of rows.\n",
        "* Starting from lines 23 till 27, we plot the predicted and original keypoints on the image of the face. \n",
        "The predicted keypoints will be red dots while the original keypoints will be green dots. \n",
        "We also plot the corresponding keypoint numbers using `plt.text()`.\n",
        "* Finally, we save the image in the `outputs` folder.\n",
        "-->\n",
        "\n",
        "Now, we will move onto the next function for the `utils.py` file.\n",
        "\n",
        "### 2.1.2 テスト用キーポイントを顔にプロットする関数\n",
        "<!-- ### 2.1.2 Function to Plot the Test Keypoints on the Faces -->\n",
        "\n",
        "ここでは、テスト時に予測するキーポイントをプロットするためのコードを書きます。\n",
        "具体的には `test.csv` ファイルにピクセル値が入っている画像が対象となります。\n",
        "<!--\n",
        "Here, we will write the code for plotting the keypoints that we will predict during testing. \n",
        "Specifically, this is for those images whose pixel values are in the test.csv file. -->"
      ],
      "id": "317dde55-0d50-487c-beeb-d4a7e11934f5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94913ebc-f48c-4229-a7ef-186578429b7b"
      },
      "source": [
        "import torch\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "resize = 96\n",
        "\n",
        "def train_test_split(csv_path, split):\n",
        "    df_data = pd.read_csv(csv_path)\n",
        "\n",
        "    # drop all the rows with missing values\n",
        "    df_data = df_data.dropna()\n",
        "    len_data = len(df_data)\n",
        "\n",
        "    # calculate the validation data sample length\n",
        "    valid_split = int(len_data * split)\n",
        "\n",
        "    # calculate the training data samples length\n",
        "    train_split = int(len_data - valid_split)\n",
        "    training_samples = df_data.iloc[:train_split][:]\n",
        "    valid_samples = df_data.iloc[-valid_split:][:]\n",
        "    print(f\"訓練データサンプル数: {len(training_samples)}\")\n",
        "    print(f\"検証データサンプル数: {len(valid_samples)}\")\n",
        "    return training_samples, valid_samples\n",
        "\n",
        "\n",
        "class FaceKeypointDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.data = samples\n",
        "\n",
        "        # get the image pixel column only\n",
        "        self.pixel_col = self.data.Image\n",
        "        self.image_pixels = []\n",
        "        #for i in tqdm(range(len(self.data))):\n",
        "        for i in range(len(self.data)):\n",
        "            img = self.pixel_col.iloc[i].split(' ')\n",
        "            self.image_pixels.append(img)\n",
        "        self.images = np.array(self.image_pixels, dtype=float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # reshape the images into their original 96x96 dimensions\n",
        "        image = self.images[index].reshape(96, 96)\n",
        "        orig_w, orig_h = image.shape\n",
        "\n",
        "        # resize the image into `resize` defined above\n",
        "        image = cv2.resize(image, (resize, resize))\n",
        "\n",
        "        # again reshape to add grayscale channel format\n",
        "        image = image.reshape(resize, resize, 1)\n",
        "        image = image / 255.0\n",
        "\n",
        "        # transpose for getting the channel size to index 0\n",
        "        image = np.transpose(image, (2, 0, 1))\n",
        "\n",
        "        # get the keypoints\n",
        "        keypoints = self.data.iloc[index][:30]\n",
        "        keypoints = np.array(keypoints, dtype=float)\n",
        "\n",
        "        # reshape the keypoints\n",
        "        keypoints = keypoints.reshape(-1, 2)\n",
        "        # rescale keypoints according to image resize\n",
        "        keypoints = keypoints * [resize / orig_w, resize / orig_h]\n",
        "        return {\n",
        "            'image': torch.tensor(image, dtype=torch.float),\n",
        "            'keypoints': torch.tensor(keypoints, dtype=torch.float),\n",
        "        }\n",
        "\n",
        "\n",
        "# get the training and validation data samples\n",
        "training_samples, valid_samples = train_test_split(f\"{ROOT_PATH}/training.csv\", TEST_SPLIT)\n",
        "\n",
        "# initialize the dataset - `FaceKeypointDataset()`\n",
        "print('--- PREPARING DATA ---')\n",
        "train_data = FaceKeypointDataset(training_samples)\n",
        "valid_data = FaceKeypointDataset(valid_samples)\n",
        "print('--- DATA PREPRATION DONE ---')\n",
        "# prepare data loaders\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True)\n",
        "valid_loader = DataLoader(valid_data,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=False)\n",
        "\n",
        "\n",
        "# whether to show dataset keypoint plots\n",
        "if SHOW_DATASET_PLOT:\n",
        "    dataset_keypoints_plot(valid_data)\n"
      ],
      "id": "94913ebc-f48c-4229-a7ef-186578429b7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b800849-97f5-44ba-a350-106d12d3d46f"
      },
      "source": [
        "#dataset_keypoints_plot(valid_data,figsize=(24,20))\n",
        "#dataset_keypoints_plot(train_data,figsize=(24,20))\n",
        "#df_data = pd.read_csv(f\"{ROOT_PATH}/training/training.csv\")\n",
        "#df_data"
      ],
      "id": "0b800849-97f5-44ba-a350-106d12d3d46f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "848f8cd3-80bd-420e-b7fc-2a0358bfca01"
      },
      "source": [
        "#from model import FaceKeypointModel\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FaceKeypointModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FaceKeypointModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(128, 30) \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout2d(p=0.2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        bs, _, _, _ = x.shape\n",
        "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc1(x) \n",
        "        return out"
      ],
      "id": "848f8cd3-80bd-420e-b7fc-2a0358bfca01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "67366011-50db-4b60-9610-c5cf5dd4c741"
      },
      "source": [
        "`test_keypoints_plot()` 関数の入力パラメータは， `images_list` と `outputs_list` です。\n",
        "これらは， 指定された数の入力画像と， プロットしたい予測キーポイントを含む 2 つのリストです。\n",
        "この関数は非常にシンプルです。\n",
        "\n",
        "* 7 行目からは，単純な for ループを実行し，2 つのリストに含まれる画像と予測されるキーポイントをループします。\n",
        "* `valid_keypoints_plot()` 関数と同じ経路をたどります。\n",
        "* しかし，今回は，すべての画像を 1 つのプロットにしたいので，Matplotlib の `subplot()` 関数を利用します。\n",
        "9 枚の画像をプロットするので，`plt.subplot(3, 3, i+1)` を使用します．\n",
        "* 最後に， プロットされた画像と予測されたキーポイントを，出力フォルダに保存します。\n",
        "\n",
        "以上でこの関数の説明は終わりです。\n",
        "\n",
        "### 2.1.3 入力データセットの顔画像とキーポイントをプロットする関数\n",
        "\n",
        "ニューラルネットワークモデルにデータを入力する前に，データが正しいかどうかを確認します。\n",
        "すべてのキーポイントが顔に正しく対応しているかどうかは，わからないかもしれません。\n",
        "そのため，学習開始前に，顔画像とそれに対応するキーポイントを表示する関数を書きます。\n",
        "`SHOW_DATASET_PLOT = True` になっている場合のみ行われます。\n"
      ],
      "id": "67366011-50db-4b60-9610-c5cf5dd4c741"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32a909f1-b71d-42fd-8ef8-733e0f069105"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import matplotlib\n",
        "\n",
        "# model\n",
        "model = FaceKeypointModel().to(DEVICE)\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# we need a loss function which is good for regression like MSELoss\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# training function\n",
        "def train(model, dataloader, data):\n",
        "    #print('Training')\n",
        "    model.train()\n",
        "    train_running_loss = 0.0\n",
        "    counter = 0\n",
        "    # calculate the number of batches\n",
        "    num_batches = int(len(data)/dataloader.batch_size)\n",
        "\n",
        "    for i, data in enumerate(dataloader): #, total=num_batches):\n",
        "        counter += 1\n",
        "        image, keypoints = data['image'].to(DEVICE), data['keypoints'].to(DEVICE)\n",
        "        # flatten the keypoints\n",
        "        keypoints = keypoints.view(keypoints.size(0), -1)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(image)\n",
        "        loss = criterion(outputs, keypoints)\n",
        "        train_running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss = train_running_loss/counter\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "# validatioon function\n",
        "def validate(model, dataloader, data, epoch, print_interval=3):\n",
        "    #print('Validating')\n",
        "    model.eval()\n",
        "    valid_running_loss = 0.0\n",
        "    counter = 0\n",
        "    # calculate the number of batches\n",
        "    num_batches = int(len(data)/dataloader.batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader): #, total=num_batches):\n",
        "            counter += 1\n",
        "            image, keypoints = data['image'].to(DEVICE), data['keypoints'].to(DEVICE)\n",
        "            # flatten the keypoints\n",
        "            keypoints = keypoints.view(keypoints.size(0), -1)\n",
        "            outputs = model(image)\n",
        "            loss = criterion(outputs, keypoints)\n",
        "            valid_running_loss += loss.item()\n",
        "            # plot the predicted validation keypoints after every...\n",
        "            # ... print_interval epochs and from the first batch\n",
        "            if (epoch+1) % print_interval == 0 and i == 0:\n",
        "                valid_keypoints_plot(image, outputs, keypoints, epoch)\n",
        "\n",
        "    valid_loss = valid_running_loss/counter\n",
        "    return valid_loss"
      ],
      "id": "32a909f1-b71d-42fd-8ef8-733e0f069105",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfC5RvqvQke9"
      },
      "source": [
        "EPOCHS"
      ],
      "id": "YfC5RvqvQke9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c164a07-3337-4921-807e-a5f38181a362"
      },
      "source": [
        "EPOCHS = 64 # 時間の都合上 EPOCHS を少なくしています\n",
        "interval = EPOCHS >> 3\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "for epoch in range(EPOCHS):\n",
        "    train_epoch_loss = train(model, train_loader, train_data)\n",
        "    val_epoch_loss = validate(model, valid_loader, valid_data, epoch, print_interval=2)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    val_loss.append(val_epoch_loss)\n",
        "\n",
        "    if ((epoch) % interval) == 0:\n",
        "        print(f\"エポック {epoch+1:<4d}/{EPOCHS:<4d}\", end=\"\")\n",
        "        print(f\"訓練損失: {train_epoch_loss:.3f}\", \n",
        "              f'検証損失: {val_epoch_loss:.3f}')"
      ],
      "id": "4c164a07-3337-4921-807e-a5f38181a362",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b555c23c-4093-4112-9836-a23a7a889181"
      },
      "source": [
        "# loss plots\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_loss, color='blue', label='訓練損失')\n",
        "plt.plot(val_loss, color='red', label='検証損失')\n",
        "plt.xlabel('エポック')\n",
        "plt.ylabel('損失値')\n",
        "plt.legend()\n",
        "plt.savefig(f\"{OUTPUT_PATH}/loss.pdf\")\n",
        "plt.show()"
      ],
      "id": "b555c23c-4093-4112-9836-a23a7a889181",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "071ffa2b-c93d-47bb-ba36-07944c8ba461"
      },
      "source": [
        "# 結果の保存\n",
        "torch.save({\n",
        "            'epoch': EPOCHS,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': criterion,\n",
        "            }, f\"{OUTPUT_PATH}/model.pth\")\n"
      ],
      "id": "071ffa2b-c93d-47bb-ba36-07944c8ba461",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1135d1fd-0455-47bd-9df7-03334e65bc6d"
      },
      "source": [
        "## 結果の再読み込みと視覚化\n",
        "\n",
        "保存した結果を再度読み込んで表示します\n"
      ],
      "id": "1135d1fd-0455-47bd-9df7-03334e65bc6d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0047d82-f2fb-4290-af94-ee24ca04bbcd"
      },
      "source": [
        "model = FaceKeypointModel().to(DEVICE)\n",
        "\n",
        "# load the model checkpoint\n",
        "checkpoint = torch.load(f\"{OUTPUT_PATH}/model.pth\")\n",
        "\n",
        "# load model weights state_dict\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "id": "e0047d82-f2fb-4290-af94-ee24ca04bbcd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4863b90b-6ce7-46c0-9055-a10a00d82280"
      },
      "source": [
        "## テストデータによる結果の視覚化\n",
        "\n",
        "`test.csv` を読み込んで，結果を表示します"
      ],
      "id": "4863b90b-6ce7-46c0-9055-a10a00d82280"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ebcb9e5-2871-4f76-a9d5-1ef52e390af6"
      },
      "source": [
        "model.eval()"
      ],
      "id": "2ebcb9e5-2871-4f76-a9d5-1ef52e390af6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9074b67-37b2-4f47-998f-473e0d29d8e0"
      },
      "source": [
        "# `test.csv` ファイルの読み込み\n",
        "\n",
        "csv_file = f\"{ROOT_PATH}/test.csv\"\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "pixel_col = data.Image\n",
        "image_pixels = []\n",
        "for i in range(len(pixel_col)):\n",
        "    img = pixel_col[i].split(' ')\n",
        "    image_pixels.append(img)\n",
        "\n",
        "# NumPy 配列へ変換\n",
        "images = np.array(image_pixels, dtype=float)"
      ],
      "id": "f9074b67-37b2-4f47-998f-473e0d29d8e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdc053d3-d2f6-4fd3-94b2-6d853c9713c9"
      },
      "source": [
        "## キーポイント予測結果の視覚化\n",
        "\n",
        "9 つのデータを視覚化します。\n",
        "\n",
        "予測されたキーポイントを取得し， `outputs` に格納します。\n",
        "各前向き処理後に， 画像と出力をそれぞれ  `images_list` と `outputs_list` に追加します。\n",
        "\n",
        "最後に，`test_keypoints_plot()` を呼び出し，予測キーポイントを顔の画像上にプロットします。\n",
        "\n",
        "検証結果と比較すると、テスト結果は良好に見えます。\n"
      ],
      "id": "bdc053d3-d2f6-4fd3-94b2-6d853c9713c9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0195f2c2-d1e9-47c8-8cc6-1d2c4d1b10c7"
      },
      "source": [
        "images_list, outputs_list = [], []\n",
        "for i in range(9):\n",
        "    with torch.no_grad():\n",
        "        image = images[i]\n",
        "        image = image.reshape(96, 96, 1)\n",
        "        image = cv2.resize(image, (resize, resize))\n",
        "        image = image.reshape(resize, resize, 1)\n",
        "        orig_image = image.copy()\n",
        "        image = image / 255.0\n",
        "        image = np.transpose(image, (2, 0, 1))\n",
        "        image = torch.tensor(image, dtype=torch.float)\n",
        "        image = image.unsqueeze(0).to(DEVICE)\n",
        "        \n",
        "        # forward pass through the model\n",
        "        outputs = model(image)\n",
        "        # append the current original image\n",
        "        images_list.append(orig_image)\n",
        "        # append the current outputs\n",
        "        outputs_list.append(outputs)\n",
        "        \n",
        "        \n",
        "test_keypoints_plot(images_list, outputs_list, figsize=(14,14))"
      ],
      "id": "0195f2c2-d1e9-47c8-8cc6-1d2c4d1b10c7",
      "execution_count": null,
      "outputs": []
    }
  ]
}