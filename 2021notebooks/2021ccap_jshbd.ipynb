{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021ccap_jshbd.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNib8Eo+VgG1HuZ6jQPLQfT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/2021notebooks/2021ccap_jshbd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovLqDB6cJPbP"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import platform\n",
        "# ローカルマシンで実行しているか，Google Colab で実行しているかを判定\n",
        "# ローカルマシンで実行しているなら isLocal = True, isColab = False\n",
        "isLocal = True if platform.system() == 'Darwin' else False\n",
        "isColab = not isLocal\n",
        "\n",
        "if isColab:\n",
        "    !git clone https://github.com/project-ccap/ccap.git\n",
        "    !pip install japanize_matplotlib\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import scipy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0PUZxItJXML"
      },
      "source": [
        "# TLPA と SALA のデータ定義\n",
        "tlpa_labels = ['バス', '緑', '桜', 'のり巻き', '五重塔', 'コップ', 'ごぼう', '土踏まず', '風呂', 'ヒトデ', 'ハム', '兎', 'ロープウエイ', '学校', 'ちりとり', '縁側', '歯', 'ネギ', 'あじさい', '灰色', '天井', '鍵', '肌色', 'ワニ', '電車', '顔', '松', 'ガードレール', '柿', 'ちまき', '信号', 'すすき', 'じょうろ', 'コンセント', '天ぷら', '中指', 'ヨット', 'ピンク', 'ふくろう', 'みかん', '柱', '角砂糖', '犬', 'かご', 'バラ', '鍋', 'まぶた', 'くるみ', '黒', 'デパート', 'カーネーション', '城', '蟻', '豆腐', 'ドライバー', '紺', '階段', '戦車', '人参', '背中', '鏡餅', 'スプーン', '朝顔', '金', '足', 'ふすま', '蛇', 'レモン', '公園', '乳母車', '床', '藤', 'ピンセット', 'トラック', '苺', '黄土色', '銭湯', 'ナマズ', 'そば', 'お腹', 'オレンジ', 'バター', '工場', '鳩', '電卓', '喉仏', 'チューリップ', '白菜', 'トラクター', '廊下', 'パトカー', '押入れ', '鉛筆', '目尻', '芋', '吊り橋', '赤', 'かき氷', '豹', 'サボテン', 'ピラミッド', 'サイ', '目', 'ひまわり', 'はたき', '刺し身', '玄関', 'トマト', '黄緑', '三輪車', '鶏', 'つむじ', 'アスパラガス', 'ドア', '銀色', 'すりこ木', 'ウイスキー', '梅', 'タクシー', '動物園', '床の間', '焦げ茶', 'ぶどう', '飴', '毛虫', 'アイロン', '寺', 'そり', 'ひょうたん', '首', '消しゴム', '頬', 'いちょう', '駅', 'ギョウザ', '牛', 'びわ', '飛行機', '畳', '白', '竹', 'ペリカン', '紫', '手すり', '口', '大根', '風車', '鋏', '潜水艦', 'ステーキ', 'マッチ', '二階', '落花生', '御飯', '自転車', '歩道橋', '鯨', '茶色', '菖蒲', 'ふくらはぎ', '桃', 'タイヤキ', '道路', '靴べら', '水色', '壁', 'たんぽぽ', 'いかだ', '山羊', '鼻', '海老', '台所', 'オートバイ', 'かぶ', '柳', 'しゃもじ', 'まんじゅう', 'かかと', '薄紫', '家', 'おせち料理', '青', '傘', 'つくし', 'りんご', '馬車', '線路', 'タツノオトシゴ', '耳', '便所', '蓮根', '猫', '黄色', 'へそ', '街灯', '障子', '酒', '船', '安全ピン', 'もみじ']\n",
        "tlpa_fam = ['高', '高', '高', '低', '低', '高', '低', '低', '高', '低', '高', '高', '低', '高', '低', '低', '高', '高', '低', '低', '高', '高', '低', '低', '高', '高', '高', '低', '低', '低', '高', '低', '低', '低', '高', '低', '高', '高', '低', '高', '低', '低', '高', '低', '高', '高', '低', '低', '高', '高', '低', '低', '高', '高', '低', '低', '高', '低', '高', '高', '低', '高', '高', '低', '高', '低', '高', '低', '高', '低', '高', '低', '低', '高', '高', '低', '低', '低', '高', '高', '高', '高', '高', '高', '低', '低', '高', '低', '低', '低', '高', '高', '高', '低', '高', '低', '高', '低', '低', '低', '低', '低', '高', '高', '低', '高', '高', '高', '低', '低', '高', '低', '低', '高', '低', '低', '低', '高', '高', '高', '低', '低', '高', '高', '低', '高', '高', '低', '低', '高', '高', '低', '低', '高', '低', '高', '低', '高', '低', '高', '高', '低', '高', '低', '高', '高', '低', '高', '低', '低', '高', '低', '低', '高', '高', '低', '高', '高', '低', '低', '高', '低', '高', '低', '低', '高', '高', '低', '低', '高', '高', '高', '高', '低', '低', '低', '高', '低', '低', '高', '低', '高', '高', '低', '高', '低', '低', '低', '高', '高', '低', '高', '高', '低', '低', '低', '高', '高', '低', '高']\n",
        "tlpa_cat = ['乗り物', '色', '植物', '加工食品', '建造物', '道具', '野菜果物', '身体部位', '屋内部位', '動物', '加工食品', '動物', '乗り物', '建造物', '道具', '屋内部位', '身体部位', '野菜果物', '植物', '色', '屋内部位', '道具', '色', '動物', '乗り物', '身体部位', '植物', '建造物', '野菜果物', '加工食品', '建造物', '植物', '道具', '屋内部位', '加工食品', '身体部位', '乗り物', '色', '動物', '野菜果物', '屋内部位', '加工食品', '動物', '乗り物', '植物', '道具', '身体部位', '野菜果物', '色', '建造物', '植物', '建造物', '動物', '加工食品', '道具', '色', '屋内部位', '乗り物', '野菜果物', '身体部位', '加工食品', '道具', '植物', '色', '身体部位', '屋内部位', '動物', '野菜果物', '建造物', '乗り物', '屋内部位', '植物', '道具', '乗り物', '野菜果物', '色', '建造物', '動物', '加工食品', '身体部位', '色', '加工食品', '建造物', '動物', '道具', '身体部位', '植物', '野菜果物', '乗り物', '屋内部位', '乗り物', '屋内部位', '道具', '身体部位', '野菜果物', '建造物', '色', '加工食品', '動物', '植物', '建造物', '動物', '身体部位', '植物', '道具', '加工食品', '屋内部位', '野菜果物', '色', '乗り物', '動物', '身体部位', '野菜果物', '屋内部位', '色', '道具', '加工食品', '植物', '乗り物', '建造物', '屋内部位', '色', '野菜果物', '加工食品', '動物', '道具', '建造物', '乗り物', '植物', '身体部位', '道具', '身体部位', '植物', '建造物', '加工食品', '動物', '野菜果物', '乗り物', '屋内部位', '色', '植物', '動物', '色', '屋内部位', '身体部位', '野菜果物', '建造物', '道具', '乗り物', '加工食品', '道具', '屋内部位', '野菜果物', '加工食品', '乗り物', '建造物', '動物', '色', '植物', '身体部位', '野菜果物', '加工食品', '建造物', '道具', '色', '屋内部位', '植物', '乗り物', '動物', '身体部位', '動物', '屋内部位', '乗り物', '野菜果物', '植物', '道具', '加工食品', '身体部位', '色', '建造物', '加工食品', '色', '道具', '植物', '野菜果物', '乗り物', '建造物', '動物', '身体部位', '屋内部位', '野菜果物', '動物', '色', '身体部位', '建造物', '屋内部位', '加工食品', '乗り物', '道具', '植物']\n",
        "\n",
        "sala_labels = ['秤', '十手', '靴', '船', '急須', '鉢巻', '蓮根', '枕', '灯籠', '犀', '城', '茶碗', '轆轤首', '毬藻', 'タラップ', '野球', '電車', '雨', '鈴蘭', '糸', '桃', '自転車', '新幹線', '梯子', '寿司', '庇', '台所', '金槌', '兜', 'お茶', '錨', '時計', '手', '医者', 'スリコギ', '舞妓', '犬', '火の見櫓', '花束', '鞘', '暖簾', '鏡', '指輪', '池', '線路', '天井', '釣針', 'ケーキ', '鼻緒', '吊橋', '茶托', '自動車', '菊人形', '蔦', '机', '果物', '灸', '門', '風呂', '蚊帳', 'ワイン', '擂鉢', '行司', '錐', '升', '酒', '砂糖', '教会', '薪割り', '雲', '手袋', '鼠', '駱駝', '黒子', '綱引き', 'コーヒー', '筏', '口紅', '煉瓦', '踵', 'ストロー', '河馬', '簀子', '御神輿', '綱渡り', '刷毛', '竹', '硯', '裾野', '茶筒', '人参', '新聞', '箕', '花火', '箱', '切符', 'たこ', '紅茶', 'バラ', 'えび', 'たわし', 'トンネル', 'バナナ', '太陽', '耳', 'ライオン', '水着', 'うさぎ', 'マスク', 'あじさい', 'ダム', '馬', 'ちりとり', 'タオル', '毛虫', '学校', 'ピザ', 'やかん', 'にわとり', 'パン', '切手', 'テレビ', '指', 'カーテン', 'りんご', '目薬', '空', 'こま', 'おにぎり', 'ポスト', '虹', '信号', 'エプロン', '電話', 'あご', '牛乳', 'バス', 'ねぎ', 'アルバム', 'はさみ', 'じゃがいも', 'バット', 'いか', '滝', '病院', 'ネクタイ', 'デモ', 'ろうそく', '口', '交番', 'しゃもじ', '背中', 'ハム', 'いちご', '手紙', 'ガム', 'こけし', 'アイロン', 'へそ', 'ペンギン', '火山', '朝顔', 'のこぎり', 'コップ', 'やぎ', 'ミシン', '牛', 'うちわ', 'ペン', 'ひまわり', 'ピアノ', 'かまぼこ', '窓', 'そろばん', '金魚', '灰皿', 'ドア', 'ふぐ', 'キャベツ', '写真', 'なす', 'アンテナ', '弁当', 'こたつ', '骨', 'ドーナツ']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ8Ey33HJyMK"
      },
      "source": [
        "from ccap import ccap_w2v\n",
        "_ccap_w2v = ccap_w2v()\n",
        "w2v = _ccap_w2v.w2v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYvZ3nCXLNvu"
      },
      "source": [
        "# 0. 基本統計量の産出\n",
        "\n",
        "## 0.1 TLPA と SALA の語彙的意味と視覚的意味はどの程度散らばっているか。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fgaywNGJ5kg"
      },
      "source": [
        "import typing\n",
        "import pandas as pd\n",
        "#np.printoptions(precision=3)\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "# 以前作成した ResNet の最終直下層の活性化を読み込む\n",
        "if isColab:\n",
        "    import requests\n",
        "    resnet_url = 'https://project-ccap.github.io/2020-0720ccap_ResNet.xlsx'\n",
        "    resnet_excel_fname = resnet_url.split('/')[-1]\n",
        "    response = requests.get(resnet_url)\n",
        "    with open(resnet_excel_fname, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    resnet_base = '.'\n",
        "else:\n",
        "    resnet_base = '/Users/asakawa/study/2020ccap/notebooks/'\n",
        "    resnet_excel_fname = '2020-0720ccap_ResNet.xlsx'\n",
        "\n",
        "Vis_tlpa = pd.read_excel(os.path.join(resnet_base, resnet_excel_fname), sheet_name='TLPA')\n",
        "print(f'ResNet のエントリは {len(Vis_tlpa)} である。これは TLPA の色名検査項目を使っていないためである ')\n",
        "Vis_tlpa = Vis_tlpa.to_numpy()[:,1:].astype(np.float64)\n",
        "\n",
        "Vis_sala = pd.read_excel(os.path.join(resnet_base, resnet_excel_fname), sheet_name='SALA')\n",
        "Vis_sala = Vis_sala.to_numpy()[:,1:].astype(np.float64)\n",
        "\n",
        "def calc_det(X, print_on=False) -> dict:\n",
        "    C = np.cov(X)                          # 共分散行列\n",
        "    sum_eigs = np.linalg.eig(C)[0].sum()   # 固有値の和\n",
        "    trace = np.trace(C)                    # トレース\n",
        "    det = np.linalg.det(C)                 # 行列式\n",
        "    if print_on:\n",
        "        print(f'固有値の和: {sum_eigs:.3f}',\n",
        "              f'トレース: {trace:.3f}',\n",
        "              f'行列式: {det:.3f}')\n",
        "    return {'固有値の和':sum_eigs, 'trace':trace, 'det':det}\n",
        "\n",
        "\n",
        "Lex_sala = np.array([w2v[w] for w in sala_labels],  dtype=float)\n",
        "print(f'Lex_sala {[{k:v} for k, v in calc_det(Lex_sala).items()]}')\n",
        "\n",
        "print('従って，分析前に色名呼称項目だけを抜き出す必要がある')\n",
        "tlpa_labels_no_colors = []\n",
        "for i, x in enumerate(tlpa_cat):\n",
        "    if x == '色':  #色 データだったら無視して先に進む\n",
        "        continue\n",
        "    tlpa_labels_no_colors.append(tlpa_labels[i])\n",
        "\n",
        "print(f'TLPA 項目数: {len(tlpa_labels_no_colors)} 項目: {tlpa_labels_no_colors}')\n",
        "Lex_tlpa = np.array([w2v[w] for w in tlpa_labels_no_colors],  dtype=float)\n",
        "print(f'Lex_tlpa.shape={Lex_tlpa.shape}')\n",
        "\n",
        "tlpa_cat_no_colors = []\n",
        "for x in tlpa_cat:\n",
        "    if x != '色':\n",
        "        tlpa_cat_no_colors.append(x)\n",
        "\n",
        "print(f'TLPA カテゴリー項目数(色抜き):{len(tlpa_cat_no_colors)}',\n",
        "      f'カテゴリー項目:{tlpa_cat_no_colors}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2Id5IZAJ8ec"
      },
      "source": [
        "print(f'Lex_tlpa {[{k:v} for k, v in calc_det(Lex_tlpa).items()]}')\n",
        "print(f'Vis_tlpa {[{k:v} for k, v in calc_det(Vis_tlpa).items()]}')\n",
        "print('-' * 88)\n",
        "print(f'Lex_sala {[{k:v} for k, v in calc_det(Lex_sala).items()]}')\n",
        "print(f'Vis_sala {[{k:v} for k, v in calc_det(Vis_sala).items()]}')\n",
        "\n",
        "#whole_words = np.array([w2v[w] for w in list(w2v.key_to_index)[:10000]])\n",
        "whole_words = np.array([w2v[w] for w in list(w2v.vocab)[:10000]])\n",
        "print(whole_words.shape)\n",
        "whole_trace = np.trace(np.cov(whole_words))\n",
        "print(f'whole_trace: {whole_trace:9.2f}')\n",
        "\n",
        "print(f'TLPA の分散と全単語の分散比 {np.trace(np.cov(Lex_tlpa)):.2f}', '/', f'{whole_trace:.2f}', '=', f'{np.trace(np.cov(Lex_tlpa))/whole_trace:.4f} 占有率')\n",
        "print(f'SALA の分散と全単語の分散比 {np.trace(np.cov(Lex_sala)):.2f}', '/', f'{whole_trace:.2f}', '=', f'{np.trace(np.cov(Lex_sala))/whole_trace:.4f} 占有率')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TukdCwHpPxtf"
      },
      "source": [
        "- 上の結果は，語彙的意味表現の散らばりと視覚的意味表現の散らばりを比較したものである。\n",
        "- 異なるデータなので直接比較することはできないが，word2vec の 200 次元ベクトルを用いた TLPA 各図版の散らばりと，ResNet の最終直下層を用いた 512 次元のベクトルを比較すると大きく異ることが分かる。\n",
        "- また，\n",
        "各図版を多次元ベクトルと捉え，意味空間が多次元正規分布に従うと仮定すると TLPA 図版で構成される意味空間の分散は，各図版で構成される行列の行列式で表される。\n",
        "TLPA 各図版間の分散共分散行列を $C$ とすれば，$\\det\\left|C\\right|$ である。\n",
        "ちなみに，多次元正規分布の確率密度関数は，次式で与えられる:\n",
        "$$\n",
        "p(x;\\mu,\\Sigma)=\\frac{1}{(2\\pi)^m\\left|\\Sigma\\right|^{1/2}} \\exp\\left(\\frac{1}{2}(x-\\mu)^\\top\\Sigma^{-1}(x-\\mu)\\right),\\tag{1}\n",
        "$$\n",
        "ここで $\\mu$ は母平均，$\\Sigma$ は母分散共分散行列を, $m$ は次元数を表す。\n",
        "\n",
        "上のセルで求めた値は，(1) 式の $\\det\\left|\\Sigma\\right|$ の推定値である。\n",
        "\n",
        "TLPA の項目で定義される意味範囲より，SALA のそれで定義される意味範囲の方が若干大きいことが分かる(SALA=975.7, TLPA=877.1)\n",
        "視覚的意味についてもどうようで，SALA=104.1, TLPA=91.3 程度である。\n",
        "\n",
        "次に，TLPA は 10 カテゴリに分かれているので，色を除く 9 カテゴリについて，カテゴリ内の群内平均，分散を求めてみよう。\n",
        "\n",
        "## 0.2 TLPA のカテゴリ毎の散らばり\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsOoj-CzNghF"
      },
      "source": [
        "def make_tlpa_cat_data(X,                             # X は入力データ\n",
        "                       cats=tlpa_cat_no_colors,       # カテゴリー名のリスト X の行数と等しい要素数を持つ\n",
        "                       labels=tlpa_labels_no_colors,  # ラベル名のリスト    X の行数と等しい要素数を持つ\n",
        "                       w2v=w2v) -> (dict, dict):\n",
        "    ret, ret_stats = {}, {}\n",
        "    for i, (cat, word) in enumerate(zip(cats, labels)):\n",
        "        if not cat in ret:\n",
        "            ret[cat] = []              # 初出のカテゴリなら配列を宣言する\n",
        "        ret[cat].append(X[i])          # それ以外は，追加する\n",
        "    for key in ret.keys():\n",
        "        ret[key] = np.array(ret[key])  # 上で作成したリストを numpy の配列に変換する\n",
        "        \n",
        "    for cat in ret.keys():\n",
        "        ret_stats[cat] = {}\n",
        "        C = np.cov(ret[cat].T)           # 分散共分散行列の計算\n",
        "        ret_stats[cat]['trace'] = np.trace(C)              #トレース\n",
        "        ret_stats[cat]['avg'] = np.mean(ret[cat], axis=0)  #平均\n",
        "        ret_stats[cat]['cov'] = C                          #分散共分散行列\n",
        "        ret_stats[cat]['inv_C'] = np.linalg.inv(C)         #分散共分散行列の逆行列\n",
        "    return ret, ret_stats\n",
        "\n",
        "Lex_tlpa_cat, Lex_tlpa_cat_stats = make_tlpa_cat_data(Lex_tlpa)\n",
        "Vis_tlpa_cat, Vis_tlpa_cat_stats = make_tlpa_cat_data(Vis_tlpa)   \n",
        "\n",
        "\n",
        "print('語彙的意味のカテゴリ毎の分散')\n",
        "for x in Lex_tlpa_cat_stats.keys():\n",
        "    print(f'{x} {Lex_tlpa_cat_stats[x][\"trace\"]:.3f}')\n",
        "\n",
        "print('\\n視覚的意味のカテゴリ毎の分散')\n",
        "for x in Vis_tlpa_cat_stats.keys():\n",
        "    print(f'{x} {Vis_tlpa_cat_stats[x][\"trace\"]:.3f}')\n",
        "\n",
        "print('\\n# 単位揃っていないので，全体の分散の推定値である 全体のトレースで割った値を表示してみる')\n",
        "Lex_trace, Vis_trace = calc_det(Lex_tlpa)['trace'], calc_det(Vis_tlpa)['trace']\n",
        "\n",
        "print('語彙的意味のカテゴリ毎の分散の比')\n",
        "for x in Lex_tlpa_cat_stats.keys():\n",
        "    print(f'{x} {Lex_tlpa_cat_stats[x][\"trace\"]/Lex_trace:.3f}')\n",
        "\n",
        "print('\\n視覚的意味のカテゴリ毎の分散の比')\n",
        "for x in Vis_tlpa_cat_stats.keys():\n",
        "    print(f'{x} {Vis_tlpa_cat_stats[x][\"trace\"]/Vis_trace:.3f}')\n",
        "\n",
        "\n",
        "size=24\n",
        "params = {'legend.fontsize': 'large',\n",
        "          'figure.figsize': (10,6),\n",
        "          'axes.labelsize': size,\n",
        "          'axes.titlesize': size * 0.8,\n",
        "          'xtick.labelsize': size*0.75,\n",
        "          'ytick.labelsize': size*0.75,\n",
        "          'axes.titlepad': 25}\n",
        "#plt.rcParams.update(params)\n",
        "\n",
        "x_ticks = list(Lex_tlpa_cat_stats.keys())\n",
        "\n",
        "lex_cat_traces = [Lex_tlpa_cat_stats[x]['trace']/Lex_trace for x in Lex_tlpa_cat_stats.keys()]\n",
        "vis_cat_traces = [Vis_tlpa_cat_stats[x]['trace']/Vis_trace for x in Vis_tlpa_cat_stats.keys()]\n",
        "max_traces = max(lex_cat_traces) + 0.1 if max(lex_cat_traces) > max(vis_cat_traces) else max(vis_cat_traces) + 0.1\n",
        "\n",
        "tickvalues = range(0,len(Lex_tlpa_cat_stats))\n",
        "x_ticks = list(Vis_tlpa_cat_stats.keys())\n",
        "\n",
        "plt.rcParams.update(params)\n",
        "plt.xticks(ticks = tickvalues ,labels = x_ticks, rotation = 'vertical')\n",
        "plt.ylim(0, max_traces)\n",
        "#plt.bar(range(0,len(Lex_tlpa_cat_stats)), cat_traces)\n",
        "plt.bar(range(0,len(Lex_tlpa_cat_stats)), [Lex_tlpa_cat_stats[x]['trace']/Lex_trace for x in Lex_tlpa_cat_stats.keys()])\n",
        "plt.title('TLPA語彙的意味カテゴリー毎の群内分散/全体分散')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.rcParams.update(params)\n",
        "tickvalues = range(0,len(Vis_tlpa_cat_stats))\n",
        "x_ticks = list(Vis_tlpa_cat_stats.keys())\n",
        "plt.xticks(ticks = tickvalues, labels = x_ticks, rotation = 'vertical')\n",
        "plt.ylim(0, max_traces)\n",
        "plt.bar(range(0,len(Vis_tlpa_cat_stats)), [Vis_tlpa_cat_stats[x]['trace']/Vis_trace for x in Vis_tlpa_cat_stats.keys()])\n",
        "plt.title('TLPA視覚的意味カテゴリー毎の群内分散/全体分散')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMFs2YRPP9K9"
      },
      "source": [
        "\n",
        "語彙的意味カテゴリの散らばりの方が，各カテゴリごとの比較で変動が大きいように見受けられる。\n",
        "一方，視覚的意味カテゴリの変動は，カテゴリ間の変動が少ない。\n",
        "このことは，視覚的意味カテゴリは，各図版間の変動が小さいことに起因するように思われる。\n",
        "すなわち，TLPA 図版は，すべて線画で，かつ，白黒濃淡画像である。\n",
        "ところが ResNet は ImageNet コンテストで 100 万枚以上のカラー画像から，1000 カテゴリ分類を行うために設計された一般画像認識器である。\n",
        "すなわち，一般画像認識全般に比して，TLPA 全図版 180 枚は，その部分集合に過ぎない。\n",
        "TLPA が測定しようしている概念の範囲から見れば，各カテゴリに割り当てられた図版の変動は小さいという解釈が可能であると考える。\n",
        "\n",
        "一方，TLPA の辞書的意味は，単語埋め込みモデル (ここでは word2vec) で定義されたベクトルであり，訓練データとして用いた日本語ウィキペディア全文から抽出された意味空間内で，各カテゴリの占める範囲を考えると，視覚的意味カテゴリーに比べて，変動が大きくなることは納得できる結果である。\n",
        "\n",
        "全分散で除した値で見ても，相対的に値が小さい，すなわち棒グラフの高さが，辞書的意味カテゴリーの方が低いのは，上記の TLPA 図版全体で構成される意味空間（の散らばり）に対して，どの程度散らばっているのかという図であることから，辞書的意味の棒グラフの相対的な値の小ささが説明可能であると思われる。\n",
        "\n",
        "各カテゴリ毎に，脳内で群化されていると考えて，各カテゴリの中心から，それぞれの検査項目がどの程度離れているのかを計算してみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4Wl6hyeOBVv"
      },
      "source": [
        "import typing\n",
        "def mahalanobis(x=None, data=None, cov=None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    source: https://www.statology.org/mahalanobis-distance-python/\n",
        "    \"\"\"\n",
        "\n",
        "    x_mu = x - np.mean(data)\n",
        "    if not cov:\n",
        "        cov = np.cov(data.T)\n",
        "        #cov = np.cov(data.values.T)\n",
        "    inv_covmat = np.linalg.inv(cov)\n",
        "    left = np.dot(x_mu, inv_covmat)\n",
        "    mahal = np.dot(left, x_mu.T)\n",
        "    if isinstance(mahal, np.float):\n",
        "        return mahal\n",
        "    else:\n",
        "        return mahal.diagonal()\n",
        "\n",
        "\n",
        "def tlpa_mahalanobis(x=None, data=None) -> np.ndarray:\n",
        "    avg = np.mean(data)\n",
        "    inv_cov = np.linalg.inv(np.cov(data.T))\n",
        "    x_diff = x - avg\n",
        "    mahal = x_diff.T @ inv_cov\n",
        "    mahal = mahal @ x_diff\n",
        "    if isinstance(mahal, np.float):\n",
        "        return mahal\n",
        "    else:\n",
        "        return mahal.diagnonal()\n",
        "    \n",
        "    \n",
        "\n",
        "for i, (cat, word) in enumerate(zip(tlpa_cat_no_colors, tlpa_labels_no_colors)):\n",
        "    print(f'{i+1:3d} 項目(正解): {word}({cat})', end='\\t')\n",
        "    x = w2v[word]\n",
        "    x_mu = x - Lex_tlpa_cat_stats[cat]['avg']\n",
        "    _x_ = x_mu.T @ Lex_tlpa_cat_stats[cat]['inv_C']\n",
        "    _x_ = _x_ @ x_mu\n",
        "    dist = x_mu.T @ x_mu\n",
        "    _dist = tlpa_mahalanobis(x, data=Lex_tlpa_cat[cat])\n",
        "    #m_dist = mahalanobis(x=w2v[word] #, data=Lex_tlpa\n",
        "    print(f'カテゴリ中心からの距離:{dist:.2f} {_dist:.3f}') # , {np.trace(np.cov(Lex_tlpa_cat[cat])):.3f}')\n",
        "    #      #f'{mahalanobis(x=x, data=Lex_tlpa_cat[cat]):.3f}')\n",
        "    #     avg = Lex_cat_stats[cat]['avg']\n",
        "    #      print(i, w2v[word] - Lex_cat_stats[cat]['avg'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fWilYymQd-D"
      },
      "source": [
        "# 1. Linear regression models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh9GtF-lQdn8"
      },
      "source": [
        "#print(f'{np.linalg.det(np.cov(lex_tlpa)):.3f}')\n",
        "Cov_Lex_tlpa = np.cov(Lex_tlpa)\n",
        "print(f'固有値の和: {np.linalg.eig(Cov_Lex_tlpa)[0].sum():.3f}')\n",
        "print(f'トレース: {np.trace(Cov_Lex_tlpa):.3f}')\n",
        "print(f'行列式: {np.sqrt(np.linalg.det(Cov_Lex_tlpa)):.3f}')\n",
        "\n",
        "Cov_Vis_tlpa = np.cov(Vis_tlpa)\n",
        "print(np.linalg.eig(Cov_Vis_tlpa)[0].sum())\n",
        "print(np.trace(Cov_Vis_tlpa))\n",
        "print(f'行列式: {np.sqrt(np.linalg.det(Cov_Vis_tlpa)):.3f}')\n",
        "\n",
        "def mahalanobis(x=None, data=None, cov=None):\n",
        "    \"\"\"\n",
        "    source: https://www.statology.org/mahalanobis-distance-python/\n",
        "    \"\"\"\n",
        "\n",
        "    x_mu = x - np.mean(data)\n",
        "    if not cov:\n",
        "        cov = np.cov(data.values.T)\n",
        "    inv_covmat = np.linalg.inv(cov)\n",
        "    left = np.dot(x_mu, inv_covmat)\n",
        "    mahal = np.dot(left, x_mu.T)\n",
        "    return mahal.diagonal()\n",
        "\n",
        "# TLPA word2vec のカテゴリごとの分散\n",
        "Lex_tlpa = {}\n",
        "for cat, word in zip(tlpa_cat_no_colors, tlpa_labels_no_colors):\n",
        "    if not cat in Lex_tlpa:\n",
        "        Lex_tlpa[cat] = []\n",
        "    Lex_tlpa[cat].append(w2v[word])\n",
        "\n",
        "for key in Lex_tlpa.keys():\n",
        "    Lex_tlpa[key] = np.array(Lex_tlpa[key])\n",
        "    \n",
        "for cat in Lex_tlpa.keys():\n",
        "    C = np.cov(Lex_tlpa[cat])\n",
        "    print(f'{cat:<4s}: ',\n",
        "#          f'{np.linalg.eig(C)[0].sum():.3f}',\n",
        "#          f'{np.trace(C):.3f}',\n",
        "#          f'{np.diagonal(C).sum():.3f}',\n",
        "          f'{np.sqrt(np.linalg.det(C)):.3f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxioE5WdOCqD"
      },
      "source": [
        "#print(lex_tlpa.shape, img_tlpa.shape)\n",
        "X, Y = np.copy(Lex_tlpa), np.copy (Vis_tlpa)\n",
        "XXinv = np.linalg.inv(X.T @ X)\n",
        "XY = X.T @ Y\n",
        "w0 = XXinv @ XY\n",
        "\n",
        "Y_hat = X @ w0\n",
        "Err = (Y - Y_hat)\n",
        "\n",
        "MSE = 0\n",
        "for err in Err:\n",
        "    MSE += np.sum(err ** 2) / err.shape[0] / Err.shape[0]\n",
        "#MSE = (Err ** 2).mean()\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(f'MSE: {MSE:.3f}',\n",
        "      f'RMSE: {RMSE:.3f}')\n",
        "#print((Y - Y_hat) ** 2).sum(axis=1).mean())\n",
        "\n",
        "#test_v = w2v['戸棚']\n",
        "#test_v @ w0\n",
        "#print(w0.shape)\n",
        "#print(w0)\n",
        "print(Y[0][:10])\n",
        "print(Y_hat[0][:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJKmJjfiQuAK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}